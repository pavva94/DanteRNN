{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_text_generator_00.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m8xB0z_hYbG"
      },
      "source": [
        "# TensorFlow 2 Text generator on Dante Alighieri's Divine Comedy\n",
        "\n",
        "Author: **Ivan Bongiorni**, [LinkedIn profile](https://www.linkedin.com/in/ivan-bongiorni-b8a583164/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncDUryHdqckI"
      },
      "source": [
        "This Notebook contains a **text generator RNN** that was trained on the **Divina Commedia** (the *Divine Comedy*) by **Dante Alighieri**. This is a poem written at the beginning of the XII century. It's hard to explain what it represents for Italian culture: it's without any doubt the main pillar of our national literature, one of the building blocks of modern Italian language, and arguably the gratest poem ever. All modern representations of Hell, Purgatory and Heaven derive from this opera.\n",
        "\n",
        "It's structure is extremely interesting: each verse is composed of 11 syllables, and its rhymes follow an **A-B-A-B** structure. Lot of pattern to be learned! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8FnhxiWY_Y1",
        "outputId": "283336ea-96db-4abc-cec3-d1ec08c0c8d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Read file from Colab Notebook\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5YiWU0daYkT"
      },
      "source": [
        "current_path = \" [...] /TF_2.0/NLP/text_generator/\"\n",
        "\n",
        "# Read the Divina Commedia\n",
        "with open( \"DivinaCommedia.txt\", 'r', encoding=\"utf8\") as file:\n",
        "    divina_commedia = file.read()\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "#divina_commedia = divina_commedia.replace(\"[\", \"\")\n",
        "#divina_commedia = divina_commedia.replace(\"]\", \"\")\n",
        "\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'\\[.*\\r?\\n', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'.*Canto.*\\r?\\n', '', divina_commedia)\n",
        "\n",
        "# divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")  # with this i lose the \"terzina\": results are not so exciting\n",
        "#divina_commedia = divina_commedia.replace(\" \\n\", \"<eot>\")  # end of terzina\n",
        "#divina_commedia = divina_commedia.replace(\"\\n\", \"<eor>\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynI7x9Rp7yQe",
        "outputId": "2c9216cd-cc24-47c3-c7aa-201725fd76c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "print(divina_commedia[1:1000])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NFERNO\n",
            "\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura\n",
            "esta selva selvaggia e aspra e forte\n",
            "che nel pensier rinova la paura! \n",
            "\n",
            "Tant'è amara che poco è più morte;\n",
            "ma per trattar del ben ch'i' vi trovai,\n",
            "dirò de l'altre cose ch'i' v' ho scorte. \n",
            "\n",
            "Io non so ben ridir com'i' v'intrai,\n",
            "tant'era pien di sonno a quel punto\n",
            "che la verace via abbandonai. \n",
            "\n",
            "Ma poi ch'i' fui al piè d'un colle giunto,\n",
            "là dove terminava quella valle\n",
            "che m'avea di paura il cor compunto, \n",
            "\n",
            "guardai in alto e vidi le sue spalle\n",
            "vestite già de' raggi del pianeta\n",
            "che mena dritto altrui per ogne calle. \n",
            "\n",
            "Allor fu la paura un poco queta,\n",
            "che nel lago del cor m'era durata\n",
            "la notte ch'i' passai con tanta pieta. \n",
            "\n",
            "E come quei che con lena affannata,\n",
            "uscito fuor del pelago a la riva,\n",
            "si volge a l'acqua perigliosa e guata, \n",
            "\n",
            "così l'animo mio, ch'ancor fuggiva,\n",
            "si volse a retro a rimirar lo passo\n",
            "che non lasciò già mai persona viva.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILphRXIXaYrP",
        "outputId": "59449b5a-20d3-408e-facf-7a428c7425e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check lenght of text\n",
        "print(len(divina_commedia))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "534048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZw3-Joyhg5l"
      },
      "source": [
        "I will now extract the set of unique characters, and create a dictionary for vectorization of text. In order to feed the text into a Neural Network, I must turn each character into a number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwvjeLGWAVE4"
      },
      "source": [
        "# Store unique characters into a dict with numerical encoding\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "\n",
        "# Store them in a dict, associated with a numerical index\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkXOJ3LGAVCF",
        "outputId": "99010e4c-e287-41d1-a04c-d47e5c3c9a98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(char2idx))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sieJxDhAU_V",
        "outputId": "c024635d-96f8-4d5c-e1e9-2af65c7f1179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "char2idx"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " ',': 5,\n",
              " '-': 6,\n",
              " '.': 7,\n",
              " ':': 8,\n",
              " ';': 9,\n",
              " '?': 10,\n",
              " 'A': 11,\n",
              " 'B': 12,\n",
              " 'C': 13,\n",
              " 'D': 14,\n",
              " 'E': 15,\n",
              " 'F': 16,\n",
              " 'G': 17,\n",
              " 'H': 18,\n",
              " 'I': 19,\n",
              " 'L': 20,\n",
              " 'M': 21,\n",
              " 'N': 22,\n",
              " 'O': 23,\n",
              " 'P': 24,\n",
              " 'Q': 25,\n",
              " 'R': 26,\n",
              " 'S': 27,\n",
              " 'T': 28,\n",
              " 'U': 29,\n",
              " 'V': 30,\n",
              " 'Z': 31,\n",
              " 'a': 32,\n",
              " 'b': 33,\n",
              " 'c': 34,\n",
              " 'd': 35,\n",
              " 'e': 36,\n",
              " 'f': 37,\n",
              " 'g': 38,\n",
              " 'h': 39,\n",
              " 'i': 40,\n",
              " 'j': 41,\n",
              " 'l': 42,\n",
              " 'm': 43,\n",
              " 'n': 44,\n",
              " 'o': 45,\n",
              " 'p': 46,\n",
              " 'q': 47,\n",
              " 'r': 48,\n",
              " 's': 49,\n",
              " 't': 50,\n",
              " 'u': 51,\n",
              " 'v': 52,\n",
              " 'x': 53,\n",
              " 'y': 54,\n",
              " 'z': 55,\n",
              " 'È': 56,\n",
              " 'à': 57,\n",
              " 'è': 58,\n",
              " 'ì': 59,\n",
              " 'ò': 60,\n",
              " 'ù': 61}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2H5G86HhyvE"
      },
      "source": [
        "Once I have a dictionary that maps each characted with its respective numerical index, I can process the whole corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk_BqFK4AU9H"
      },
      "source": [
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7JpYN9LAU7U",
        "outputId": "5139ffbd-871b-4e96-d6ff-3dc069be169e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Let's see what the first line will look like\n",
        "print(\"{}\".format(divina_commedia[276:511]))\n",
        "print(\"\\nbecomes:\")\n",
        "print(numerical_encoding(divina_commedia[276:511], char2idx))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "el ben ch'i' vi trovai,\n",
            "dirò de l'altre cose ch'i' v' ho scorte. \n",
            "\n",
            "Io non so ben ridir com'i' v'intrai,\n",
            "tant'era pien di sonno a quel punto\n",
            "che la verace via abbandonai. \n",
            "\n",
            "Ma poi ch'i' fui al piè d'un colle giunto,\n",
            "là dove terminava qu\n",
            "\n",
            "becomes:\n",
            "[36 42  1 33 36 44  1 34 39  4 40  4  1 52 40  1 50 48 45 52 32 40  5  0\n",
            " 35 40 48 60  1 35 36  1 42  4 32 42 50 48 36  1 34 45 49 36  1 34 39  4\n",
            " 40  4  1 52  4  1 39 45  1 49 34 45 48 50 36  7  1  0  0 19 45  1 44 45\n",
            " 44  1 49 45  1 33 36 44  1 48 40 35 40 48  1 34 45 43  4 40  4  1 52  4\n",
            " 40 44 50 48 32 40  5  0 50 32 44 50  4 36 48 32  1 46 40 36 44  1 35 40\n",
            "  1 49 45 44 44 45  1 32  1 47 51 36 42  1 46 51 44 50 45  0 34 39 36  1\n",
            " 42 32  1 52 36 48 32 34 36  1 52 40 32  1 32 33 33 32 44 35 45 44 32 40\n",
            "  7  1  0  0 21 32  1 46 45 40  1 34 39  4 40  4  1 37 51 40  1 32 42  1\n",
            " 46 40 58  1 35  4 51 44  1 34 45 42 42 36  1 38 40 51 44 50 45  5  0 42\n",
            " 57  1 35 45 52 36  1 50 36 48 43 40 44 32 52 32  1 47 51]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqALeTUmnl0X"
      },
      "source": [
        "## RNN dataprep\n",
        "\n",
        "I need to generate a set of stacked input sequences. My goal is to train a Neural Network to find a mapping between an input sequence and an output sequence of equal length, in which each character is shifted left of one position.\n",
        "\n",
        "For example, the first verse:\n",
        "\n",
        "> Nel mezzo del cammin di nostra vita\n",
        "\n",
        "would be translated in a train sequence as:\n",
        "\n",
        "`Nel mezzo del cammin di nostra vit`\n",
        "\n",
        "be associated with the target sequence:\n",
        "\n",
        "`el mezzo del cammin di nostra vita`\n",
        "\n",
        "The following function is a preparatory step for that. More generally, given a sequence:\n",
        "\n",
        "```\n",
        "A B C D E F G H I\n",
        "```\n",
        "\n",
        "and assuming input sequences of length 5, it will generate a matrix like:\n",
        "\n",
        "```\n",
        "A B C D E\n",
        "B C D E F\n",
        "C D E F G\n",
        "D E F G H\n",
        "E F G H I\n",
        "```\n",
        "\n",
        "I will save that matrix as it is in .csv format, to use it to train the Language Generator later.\n",
        "The split between train and target sets will be as:\n",
        "\n",
        "```\n",
        " Train:           Target:\n",
        "                 \n",
        "A B C D E        B C D E F\n",
        "B C D E F        C D E F G\n",
        "C D E F G        D E F G H\n",
        "D E F G H        E F G H I\n",
        "                 \n",
        "```\n",
        "\n",
        "Train and target sets are fundamentally the same matrix, with the train having the last row removed, and the target set having the first removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWcZzOJdG6X9"
      },
      "source": [
        "# Apply it on the whole Comedy\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foyC3ZnGgKLN",
        "outputId": "c2bdee7b-6585-4d79-a008-db5bf8f2a8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(encoded_text[311:600])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[42 50 48 36  1 34 45 49 36  1 34 39  4 40  4  1 52  4  1 39 45  1 49 34\n",
            " 45 48 50 36  7  1  0  0 19 45  1 44 45 44  1 49 45  1 33 36 44  1 48 40\n",
            " 35 40 48  1 34 45 43  4 40  4  1 52  4 40 44 50 48 32 40  5  0 50 32 44\n",
            " 50  4 36 48 32  1 46 40 36 44  1 35 40  1 49 45 44 44 45  1 32  1 47 51\n",
            " 36 42  1 46 51 44 50 45  0 34 39 36  1 42 32  1 52 36 48 32 34 36  1 52\n",
            " 40 32  1 32 33 33 32 44 35 45 44 32 40  7  1  0  0 21 32  1 46 45 40  1\n",
            " 34 39  4 40  4  1 37 51 40  1 32 42  1 46 40 58  1 35  4 51 44  1 34 45\n",
            " 42 42 36  1 38 40 51 44 50 45  5  0 42 57  1 35 45 52 36  1 50 36 48 43\n",
            " 40 44 32 52 32  1 47 51 36 42 42 32  1 52 32 42 42 36  0 34 39 36  1 43\n",
            "  4 32 52 36 32  1 35 40  1 46 32 51 48 32  1 40 42  1 34 45 48  1 34 45\n",
            " 43 46 51 44 50 45  5  1  0  0 38 51 32 48 35 32 40  1 40 44  1 32 42 50\n",
            " 45  1 36  1 52 40 35 40  1 42 36  1 49 51 36  1 49 46 32 42 42 36  0 52\n",
            " 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t41gYByxAU4B"
      },
      "source": [
        "def get_text_matrix(sequence, len_input):\n",
        "    \n",
        "    # create empty matrix\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    \n",
        "    # fill each row/time window from input sequence\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "        \n",
        "    return X"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eazAAQiAk0i"
      },
      "source": [
        "text_matrix = get_text_matrix(encoded_text, 100)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KonviQjQAk40",
        "outputId": "762eef35-9503-49cc-ab6f-46e21c44ddf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(text_matrix.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(533948, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaVngDG7AkyU",
        "outputId": "e293ff6b-924e-4d87-feca-86872976c7e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "print(\"100th train sequence:\\n\")\n",
        "print(text_matrix[ 100, : ])\n",
        "print(\"\\n\\n100th target sequence:\\n\")\n",
        "print(text_matrix[ 101, : ])\n",
        "print(\"\\n\\n102th target sequence:\\n\")\n",
        "print(text_matrix[ 102, : ])\n",
        "print(\"\\n\\n115th target sequence:\\n\")\n",
        "print(text_matrix[ 180, : ])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100th train sequence:\n",
            "\n",
            "[36. 48. 32.  1. 49. 43. 32. 48. 48. 40. 50. 32.  7.  0.  0. 11. 39. 40.\n",
            "  1. 47. 51. 32. 44. 50. 45.  1. 32.  1. 35. 40. 48.  1. 47. 51. 32. 42.\n",
            "  1. 36. 48. 32.  1. 58.  1. 34. 45. 49. 32.  1. 35. 51. 48. 32.  0. 36.\n",
            " 49. 50. 32.  1. 49. 36. 42. 52. 32.  1. 49. 36. 42. 52. 32. 38. 38. 40.\n",
            " 32.  1. 36.  1. 32. 49. 46. 48. 32.  1. 36.  1. 37. 45. 48. 50. 36.  0.\n",
            " 34. 39. 36.  1. 44. 36. 42.  1. 46. 36.]\n",
            "\n",
            "\n",
            "100th target sequence:\n",
            "\n",
            "[48. 32.  1. 49. 43. 32. 48. 48. 40. 50. 32.  7.  0.  0. 11. 39. 40.  1.\n",
            " 47. 51. 32. 44. 50. 45.  1. 32.  1. 35. 40. 48.  1. 47. 51. 32. 42.  1.\n",
            " 36. 48. 32.  1. 58.  1. 34. 45. 49. 32.  1. 35. 51. 48. 32.  0. 36. 49.\n",
            " 50. 32.  1. 49. 36. 42. 52. 32.  1. 49. 36. 42. 52. 32. 38. 38. 40. 32.\n",
            "  1. 36.  1. 32. 49. 46. 48. 32.  1. 36.  1. 37. 45. 48. 50. 36.  0. 34.\n",
            " 39. 36.  1. 44. 36. 42.  1. 46. 36. 44.]\n",
            "\n",
            "\n",
            "102th target sequence:\n",
            "\n",
            "[32.  1. 49. 43. 32. 48. 48. 40. 50. 32.  7.  0.  0. 11. 39. 40.  1. 47.\n",
            " 51. 32. 44. 50. 45.  1. 32.  1. 35. 40. 48.  1. 47. 51. 32. 42.  1. 36.\n",
            " 48. 32.  1. 58.  1. 34. 45. 49. 32.  1. 35. 51. 48. 32.  0. 36. 49. 50.\n",
            " 32.  1. 49. 36. 42. 52. 32.  1. 49. 36. 42. 52. 32. 38. 38. 40. 32.  1.\n",
            " 36.  1. 32. 49. 46. 48. 32.  1. 36.  1. 37. 45. 48. 50. 36.  0. 34. 39.\n",
            " 36.  1. 44. 36. 42.  1. 46. 36. 44. 49.]\n",
            "\n",
            "\n",
            "115th target sequence:\n",
            "\n",
            "[32.  1. 36.  1. 37. 45. 48. 50. 36.  0. 34. 39. 36.  1. 44. 36. 42.  1.\n",
            " 46. 36. 44. 49. 40. 36. 48.  1. 48. 40. 44. 45. 52. 32.  1. 42. 32.  1.\n",
            " 46. 32. 51. 48. 32.  2.  1.  0.  0. 28. 32. 44. 50.  4. 58.  1. 32. 43.\n",
            " 32. 48. 32.  1. 34. 39. 36.  1. 46. 45. 34. 45.  1. 58.  1. 46. 40. 61.\n",
            "  1. 43. 45. 48. 50. 36.  9.  0. 43. 32.  1. 46. 36. 48.  1. 50. 48. 32.\n",
            " 50. 50. 32. 48.  1. 35. 36. 42.  1. 33.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPjMSZOzh_60"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "At this point, I can specify the RNN architecture with all its hyperparameters. An `Embedding()` layer will first learn a representation of each character; the sequence of chracters embedding will then be fed into an `LSTM()` layer, that will extract information from their sequence; `Dense()` layers at the end will produce the next character prediction.\n",
        "\n",
        "The Network is structured to be fed with batches of data of fixed size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFZfbimYAkvk"
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "# size of mini batches during training\n",
        "batch_size = 200  # 100\n",
        "\n",
        "\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "\n",
        "# vector size of char embeddings\n",
        "embedding_size = 300  # 250\n",
        "\n",
        "len_input = 1024   # 200\n",
        "\n",
        "hidden_size = 300  # for Dense() layers 250"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1JqO7rhAktC"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.activations import elu, relu, softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK1OwjD1IDD_",
        "outputId": "8d459cca-fdc8-4647-f2d2-55e34b125de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "EXPERIMENT\n",
        "MODEL\n",
        "'''\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Attention, Flatten, Input\n",
        "from tensorflow.keras.activations import elu, relu, softmax\n",
        "from tensorflow.keras.metrics import categorical_accuracy, sparse_categorical_crossentropy, categorical_crossentropy\n",
        "# Define custom training utilities that are widely used for language modelling\n",
        "\n",
        "n_epochs = 200\n",
        "\n",
        "learning_rate = 0.001  # 0.0001\n",
        "optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate)  # Adam\n",
        "\n",
        "def loss(y_true, y_pred):\n",
        "    \"\"\"Calculates categorical crossentropy as loss\"\"\"\n",
        "    return categorical_crossentropy(y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "\n",
        "def perplexity(labels, logits):\n",
        "    \"\"\"Calculates perplexity metric = 2^(entropy) or e^(entropy)\"\"\"\n",
        "    return pow(2, loss(y_true=labels, y_pred=logits))\n",
        "\n",
        "# Input Layer\n",
        "X = Input(shape=(None, ), batch_size=batch_size)  # 100 is the number of features\n",
        "\n",
        "# Word-Embedding Layer\n",
        "embedded = Embedding(vocab_size, embedding_size, batch_input_shape=(batch_size, None))(X)\n",
        "embedded = Dense(embedding_size, relu)(embedded)\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=2048,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True)(embedded)\n",
        "#attention_input = [encoder_output, hidden_state]\n",
        "\n",
        "encoder_output = Dense(embedding_size, activation='relu')(encoder_output)\n",
        "\n",
        "#encoder_output = Attention()(attention_input, training=True)\n",
        "\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=2048,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True)(encoder_output, initial_state=[hidden_state, cell_state])\n",
        "#encoder_output = Flatten()(encoder_output)\n",
        "encoder_output = Dense(hidden_size, activation='relu')(encoder_output)\n",
        "# Prediction Layer\n",
        "Y = Dense(units=vocab_size)(encoder_output)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=X, outputs=Y)\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer='adam', metrics=[perplexity, sparse_categorical_crossentropy])\n",
        "print(model.summary())\n",
        "\n",
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                y, model(x), from_logits = True))\n",
        "    gradients = tape.gradient(current_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "    \n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"{}.  \\t  Loss: {}  \\t  Time: {}sec/epoch\".format(\n",
        "        epoch+1, current_loss.numpy(), round(time.time()-start, 2)))\n",
        "    \n",
        "\n",
        "\n",
        "'''\n",
        "EXPERIMENT\n",
        "GENERATOR\n",
        "'''\n",
        "\n",
        "# Input Layer\n",
        "X = Input(shape=(None, ), batch_size=1)  # 100 is the number of features\n",
        "\n",
        "# Word-Embedding Layer\n",
        "embedded = Embedding(vocab_size, embedding_size, batch_input_shape=(batch_size, None))(X)\n",
        "embedded = Dense(embedding_size, relu)(embedded)\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=2048,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True,\n",
        "                                                stateful=True)(embedded)\n",
        "#attention_input = [encoder_output, hidden_state]\n",
        "\n",
        "encoder_output = Dense(embedding_size, activation='relu')(encoder_output)\n",
        "\n",
        "#encoder_output = Attention()(attention_input, training=True)\n",
        "\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=2048,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True,\n",
        "                                                stateful=True)(encoder_output, initial_state=[hidden_state, cell_state])\n",
        "#encoder_output = Flatten()(encoder_output)\n",
        "encoder_output = Dense(hidden_size, activation='relu')(encoder_output)\n",
        "# Prediction Layer\n",
        "Y = Dense(units=vocab_size)(encoder_output)\n",
        "\n",
        "# Compile model\n",
        "generator = Model(inputs=X, outputs=Y)\n",
        "generator.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer='adam', metrics=[perplexity, sparse_categorical_crossentropy])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "# Import trained weights from model to generator\n",
        "generator.set_weights(model.get_weights())\n",
        "\n",
        "def generate_text(start_string, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    generator.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "\n",
        "# Let's feed the first lines:\n",
        "start_string = \"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for t in [0.1, 0.5, 1.0, 1.5, 2]:\n",
        "    print(\"####### TEXT GENERATION - temperature = {}\\n\".format(t))\n",
        "    print(generate_text(start_string = start_string, num_generate = 1000, temperature = t))\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(200, None)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (200, None, 300)     18600       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (200, None, 300)     90300       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(200, None, 2048),  19243008    dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (200, None, 300)     614700      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(200, None, 2048),  19243008    dense_1[0][0]                    \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (200, None, 300)     614700      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (200, None, 62)      18662       dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 39,842,978\n",
            "Trainable params: 39,842,978\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "1.  \t  Loss: 3.061933755874634  \t  Time: 174.14sec/epoch\n",
            "2.  \t  Loss: 3.03540301322937  \t  Time: 167.51sec/epoch\n",
            "3.  \t  Loss: 2.5158562660217285  \t  Time: 167.74sec/epoch\n",
            "4.  \t  Loss: 2.1996212005615234  \t  Time: 168.0sec/epoch\n",
            "5.  \t  Loss: 2.067413568496704  \t  Time: 168.25sec/epoch\n",
            "6.  \t  Loss: 1.934459924697876  \t  Time: 168.66sec/epoch\n",
            "7.  \t  Loss: 1.8568652868270874  \t  Time: 168.56sec/epoch\n",
            "8.  \t  Loss: 1.7916312217712402  \t  Time: 168.51sec/epoch\n",
            "9.  \t  Loss: 1.7221605777740479  \t  Time: 169.2sec/epoch\n",
            "10.  \t  Loss: 1.6168667078018188  \t  Time: 168.71sec/epoch\n",
            "11.  \t  Loss: 1.5939011573791504  \t  Time: 169.02sec/epoch\n",
            "12.  \t  Loss: 1.5500268936157227  \t  Time: 168.8sec/epoch\n",
            "13.  \t  Loss: 1.5194380283355713  \t  Time: 169.18sec/epoch\n",
            "14.  \t  Loss: 1.4691011905670166  \t  Time: 168.93sec/epoch\n",
            "15.  \t  Loss: 1.4258140325546265  \t  Time: 169.09sec/epoch\n",
            "16.  \t  Loss: 1.42264986038208  \t  Time: 169.15sec/epoch\n",
            "17.  \t  Loss: 1.3909302949905396  \t  Time: 169.02sec/epoch\n",
            "18.  \t  Loss: 1.3690247535705566  \t  Time: 169.06sec/epoch\n",
            "19.  \t  Loss: 1.332645058631897  \t  Time: 169.2sec/epoch\n",
            "20.  \t  Loss: 1.3009734153747559  \t  Time: 169.36sec/epoch\n",
            "21.  \t  Loss: 1.2745040655136108  \t  Time: 169.05sec/epoch\n",
            "22.  \t  Loss: 1.2531933784484863  \t  Time: 169.45sec/epoch\n",
            "23.  \t  Loss: 1.210526466369629  \t  Time: 169.25sec/epoch\n",
            "24.  \t  Loss: 1.1823400259017944  \t  Time: 169.18sec/epoch\n",
            "25.  \t  Loss: 1.1788285970687866  \t  Time: 169.29sec/epoch\n",
            "26.  \t  Loss: 1.1417087316513062  \t  Time: 169.29sec/epoch\n",
            "27.  \t  Loss: 1.13063645362854  \t  Time: 169.25sec/epoch\n",
            "28.  \t  Loss: 1.1030840873718262  \t  Time: 169.31sec/epoch\n",
            "29.  \t  Loss: 1.0640382766723633  \t  Time: 169.36sec/epoch\n",
            "30.  \t  Loss: 1.0284779071807861  \t  Time: 169.37sec/epoch\n",
            "31.  \t  Loss: 0.9773874878883362  \t  Time: 169.57sec/epoch\n",
            "32.  \t  Loss: 0.9832351803779602  \t  Time: 169.66sec/epoch\n",
            "33.  \t  Loss: 0.9072052836418152  \t  Time: 169.66sec/epoch\n",
            "34.  \t  Loss: 0.8687232136726379  \t  Time: 169.6sec/epoch\n",
            "35.  \t  Loss: 0.8526192307472229  \t  Time: 169.67sec/epoch\n",
            "36.  \t  Loss: 0.7961767315864563  \t  Time: 169.77sec/epoch\n",
            "37.  \t  Loss: 0.7686260938644409  \t  Time: 169.83sec/epoch\n",
            "38.  \t  Loss: 0.7320170998573303  \t  Time: 169.61sec/epoch\n",
            "39.  \t  Loss: 0.6826493740081787  \t  Time: 169.63sec/epoch\n",
            "40.  \t  Loss: 0.6464200019836426  \t  Time: 169.8sec/epoch\n",
            "41.  \t  Loss: 0.615192174911499  \t  Time: 169.5sec/epoch\n",
            "42.  \t  Loss: 0.6011738777160645  \t  Time: 169.31sec/epoch\n",
            "43.  \t  Loss: 0.5271753072738647  \t  Time: 169.49sec/epoch\n",
            "44.  \t  Loss: 0.5131085515022278  \t  Time: 169.43sec/epoch\n",
            "45.  \t  Loss: 0.4599429666996002  \t  Time: 169.35sec/epoch\n",
            "46.  \t  Loss: 0.4458775520324707  \t  Time: 169.33sec/epoch\n",
            "47.  \t  Loss: 0.38139134645462036  \t  Time: 169.47sec/epoch\n",
            "48.  \t  Loss: 0.37503090500831604  \t  Time: 169.62sec/epoch\n",
            "49.  \t  Loss: 0.36333775520324707  \t  Time: 169.43sec/epoch\n",
            "50.  \t  Loss: 0.2997463643550873  \t  Time: 169.2sec/epoch\n",
            "51.  \t  Loss: 0.2635157108306885  \t  Time: 169.49sec/epoch\n",
            "52.  \t  Loss: 0.23524345457553864  \t  Time: 169.29sec/epoch\n",
            "53.  \t  Loss: 0.21986757218837738  \t  Time: 169.22sec/epoch\n",
            "54.  \t  Loss: 0.20222535729408264  \t  Time: 169.41sec/epoch\n",
            "55.  \t  Loss: 0.1707538366317749  \t  Time: 169.18sec/epoch\n",
            "56.  \t  Loss: 0.1581844687461853  \t  Time: 169.47sec/epoch\n",
            "57.  \t  Loss: 0.15990564227104187  \t  Time: 169.24sec/epoch\n",
            "58.  \t  Loss: 0.141805499792099  \t  Time: 169.56sec/epoch\n",
            "59.  \t  Loss: 0.13011789321899414  \t  Time: 169.38sec/epoch\n",
            "60.  \t  Loss: 0.11729823052883148  \t  Time: 169.19sec/epoch\n",
            "61.  \t  Loss: 0.10749182105064392  \t  Time: 169.22sec/epoch\n",
            "62.  \t  Loss: 0.09348258376121521  \t  Time: 169.44sec/epoch\n",
            "63.  \t  Loss: 0.09576421231031418  \t  Time: 169.63sec/epoch\n",
            "64.  \t  Loss: 0.09374655783176422  \t  Time: 169.24sec/epoch\n",
            "65.  \t  Loss: 0.09167293459177017  \t  Time: 169.47sec/epoch\n",
            "66.  \t  Loss: 0.07844171673059464  \t  Time: 169.57sec/epoch\n",
            "67.  \t  Loss: 0.06482793390750885  \t  Time: 169.41sec/epoch\n",
            "68.  \t  Loss: 0.07129962742328644  \t  Time: 169.29sec/epoch\n",
            "69.  \t  Loss: 0.05889834091067314  \t  Time: 169.6sec/epoch\n",
            "70.  \t  Loss: 0.06426191329956055  \t  Time: 169.39sec/epoch\n",
            "71.  \t  Loss: 0.04851699247956276  \t  Time: 169.38sec/epoch\n",
            "72.  \t  Loss: 0.08719761669635773  \t  Time: 169.52sec/epoch\n",
            "73.  \t  Loss: 0.044864721596241  \t  Time: 169.42sec/epoch\n",
            "74.  \t  Loss: 0.05594632402062416  \t  Time: 169.3sec/epoch\n",
            "75.  \t  Loss: 0.03967013210058212  \t  Time: 169.46sec/epoch\n",
            "76.  \t  Loss: 0.03993869200348854  \t  Time: 169.59sec/epoch\n",
            "77.  \t  Loss: 0.046932775527238846  \t  Time: 169.33sec/epoch\n",
            "78.  \t  Loss: 0.04034923017024994  \t  Time: 169.16sec/epoch\n",
            "79.  \t  Loss: 0.03334486111998558  \t  Time: 169.41sec/epoch\n",
            "80.  \t  Loss: 0.03178729861974716  \t  Time: 169.35sec/epoch\n",
            "81.  \t  Loss: 0.03443072363734245  \t  Time: 169.47sec/epoch\n",
            "82.  \t  Loss: 0.029870780184864998  \t  Time: 169.45sec/epoch\n",
            "83.  \t  Loss: 0.0349511094391346  \t  Time: 169.51sec/epoch\n",
            "84.  \t  Loss: 0.0337686762213707  \t  Time: 169.62sec/epoch\n",
            "85.  \t  Loss: 0.02878926321864128  \t  Time: 169.59sec/epoch\n",
            "86.  \t  Loss: 0.029942424967885017  \t  Time: 169.69sec/epoch\n",
            "87.  \t  Loss: 0.02234424650669098  \t  Time: 169.62sec/epoch\n",
            "88.  \t  Loss: 0.0267953984439373  \t  Time: 169.52sec/epoch\n",
            "89.  \t  Loss: 0.06116408109664917  \t  Time: 169.29sec/epoch\n",
            "90.  \t  Loss: 0.01372944749891758  \t  Time: 169.41sec/epoch\n",
            "91.  \t  Loss: 0.012465300038456917  \t  Time: 169.37sec/epoch\n",
            "92.  \t  Loss: 0.012114510871469975  \t  Time: 169.32sec/epoch\n",
            "93.  \t  Loss: 0.018572039902210236  \t  Time: 169.53sec/epoch\n",
            "94.  \t  Loss: 0.011489756405353546  \t  Time: 169.72sec/epoch\n",
            "95.  \t  Loss: 0.00915316492319107  \t  Time: 169.46sec/epoch\n",
            "96.  \t  Loss: 0.008081132546067238  \t  Time: 169.47sec/epoch\n",
            "97.  \t  Loss: 0.009579767473042011  \t  Time: 169.21sec/epoch\n",
            "98.  \t  Loss: 0.009156554006040096  \t  Time: 169.6sec/epoch\n",
            "99.  \t  Loss: 0.010634100064635277  \t  Time: 169.32sec/epoch\n",
            "100.  \t  Loss: 0.013675582595169544  \t  Time: 169.46sec/epoch\n",
            "101.  \t  Loss: 0.01866615004837513  \t  Time: 169.62sec/epoch\n",
            "102.  \t  Loss: 0.06055998057126999  \t  Time: 169.48sec/epoch\n",
            "103.  \t  Loss: 0.006550607271492481  \t  Time: 169.33sec/epoch\n",
            "104.  \t  Loss: 0.007481999229639769  \t  Time: 169.25sec/epoch\n",
            "105.  \t  Loss: 0.006961834151297808  \t  Time: 169.2sec/epoch\n",
            "106.  \t  Loss: 0.007988858968019485  \t  Time: 169.51sec/epoch\n",
            "107.  \t  Loss: 0.006916659418493509  \t  Time: 169.16sec/epoch\n",
            "108.  \t  Loss: 0.00788272637873888  \t  Time: 169.51sec/epoch\n",
            "109.  \t  Loss: 0.009922118857502937  \t  Time: 169.39sec/epoch\n",
            "110.  \t  Loss: 0.011320551857352257  \t  Time: 169.29sec/epoch\n",
            "111.  \t  Loss: 0.012586856260895729  \t  Time: 169.39sec/epoch\n",
            "112.  \t  Loss: 0.016409924253821373  \t  Time: 169.46sec/epoch\n",
            "113.  \t  Loss: 0.006300692912191153  \t  Time: 169.64sec/epoch\n",
            "114.  \t  Loss: 0.007832405157387257  \t  Time: 169.67sec/epoch\n",
            "115.  \t  Loss: 0.009215709753334522  \t  Time: 169.64sec/epoch\n",
            "116.  \t  Loss: 0.009588565677404404  \t  Time: 169.76sec/epoch\n",
            "117.  \t  Loss: 0.05656404420733452  \t  Time: 169.61sec/epoch\n",
            "118.  \t  Loss: 0.0050499276258051395  \t  Time: 169.72sec/epoch\n",
            "119.  \t  Loss: 0.0036072630900889635  \t  Time: 169.72sec/epoch\n",
            "120.  \t  Loss: 0.0039981636218726635  \t  Time: 169.7sec/epoch\n",
            "121.  \t  Loss: 0.004248242825269699  \t  Time: 169.44sec/epoch\n",
            "122.  \t  Loss: 0.004089232534170151  \t  Time: 169.52sec/epoch\n",
            "123.  \t  Loss: 0.0048609087243676186  \t  Time: 169.33sec/epoch\n",
            "124.  \t  Loss: 0.00822615996003151  \t  Time: 169.61sec/epoch\n",
            "125.  \t  Loss: 0.014098224230110645  \t  Time: 169.43sec/epoch\n",
            "126.  \t  Loss: 0.007029334083199501  \t  Time: 169.42sec/epoch\n",
            "127.  \t  Loss: 0.008912173099815845  \t  Time: 169.32sec/epoch\n",
            "128.  \t  Loss: 0.01335009466856718  \t  Time: 169.27sec/epoch\n",
            "129.  \t  Loss: 0.013415745459496975  \t  Time: 169.49sec/epoch\n",
            "130.  \t  Loss: 0.012633554637432098  \t  Time: 169.15sec/epoch\n",
            "131.  \t  Loss: 0.012471665628254414  \t  Time: 169.69sec/epoch\n",
            "132.  \t  Loss: 0.009164576418697834  \t  Time: 169.22sec/epoch\n",
            "133.  \t  Loss: 0.013849064707756042  \t  Time: 169.22sec/epoch\n",
            "134.  \t  Loss: 0.0112343430519104  \t  Time: 169.49sec/epoch\n",
            "135.  \t  Loss: 0.004104023799300194  \t  Time: 169.68sec/epoch\n",
            "136.  \t  Loss: 0.004686304368078709  \t  Time: 169.46sec/epoch\n",
            "137.  \t  Loss: 0.005272480193525553  \t  Time: 169.34sec/epoch\n",
            "138.  \t  Loss: 0.0048363409005105495  \t  Time: 169.28sec/epoch\n",
            "139.  \t  Loss: 0.008407287299633026  \t  Time: 169.34sec/epoch\n",
            "140.  \t  Loss: 0.0070220716297626495  \t  Time: 169.25sec/epoch\n",
            "141.  \t  Loss: 0.011684967204928398  \t  Time: 169.28sec/epoch\n",
            "142.  \t  Loss: 0.010305163450539112  \t  Time: 169.29sec/epoch\n",
            "143.  \t  Loss: 0.01393929123878479  \t  Time: 168.94sec/epoch\n",
            "144.  \t  Loss: 0.015463986434042454  \t  Time: 169.24sec/epoch\n",
            "145.  \t  Loss: 0.012822204269468784  \t  Time: 169.24sec/epoch\n",
            "146.  \t  Loss: 0.011556370183825493  \t  Time: 168.93sec/epoch\n",
            "147.  \t  Loss: 0.012833812274038792  \t  Time: 169.17sec/epoch\n",
            "148.  \t  Loss: 0.0072000762447714806  \t  Time: 169.19sec/epoch\n",
            "149.  \t  Loss: 0.0029932945035398006  \t  Time: 168.91sec/epoch\n",
            "150.  \t  Loss: 0.0030338072683662176  \t  Time: 169.14sec/epoch\n",
            "151.  \t  Loss: 0.0023390925489366055  \t  Time: 169.35sec/epoch\n",
            "152.  \t  Loss: 0.00397645914927125  \t  Time: 168.98sec/epoch\n",
            "153.  \t  Loss: 0.00431065121665597  \t  Time: 169.46sec/epoch\n",
            "154.  \t  Loss: 0.0055898642167449  \t  Time: 169.27sec/epoch\n",
            "155.  \t  Loss: 0.011822356842458248  \t  Time: 169.42sec/epoch\n",
            "156.  \t  Loss: 0.006844755727797747  \t  Time: 169.29sec/epoch\n",
            "157.  \t  Loss: 0.007996962405741215  \t  Time: 169.01sec/epoch\n",
            "158.  \t  Loss: 0.008575471118092537  \t  Time: 169.6sec/epoch\n",
            "159.  \t  Loss: 0.012645796872675419  \t  Time: 169.27sec/epoch\n",
            "160.  \t  Loss: 0.012087051756680012  \t  Time: 169.67sec/epoch\n",
            "161.  \t  Loss: 0.012214443646371365  \t  Time: 169.29sec/epoch\n",
            "162.  \t  Loss: 0.01423155702650547  \t  Time: 169.58sec/epoch\n",
            "163.  \t  Loss: 0.013729793950915337  \t  Time: 169.48sec/epoch\n",
            "164.  \t  Loss: 0.010645176284015179  \t  Time: 169.06sec/epoch\n",
            "165.  \t  Loss: 0.010234765708446503  \t  Time: 169.49sec/epoch\n",
            "166.  \t  Loss: 0.012717549689114094  \t  Time: 169.62sec/epoch\n",
            "167.  \t  Loss: 0.009598982520401478  \t  Time: 169.43sec/epoch\n",
            "168.  \t  Loss: 0.01113786082714796  \t  Time: 169.3sec/epoch\n",
            "169.  \t  Loss: 0.011049548164010048  \t  Time: 169.49sec/epoch\n",
            "170.  \t  Loss: 0.012648561038076878  \t  Time: 169.37sec/epoch\n",
            "171.  \t  Loss: 0.010976812802255154  \t  Time: 169.33sec/epoch\n",
            "172.  \t  Loss: 0.014645063318312168  \t  Time: 169.36sec/epoch\n",
            "173.  \t  Loss: 0.011533805169165134  \t  Time: 169.74sec/epoch\n",
            "174.  \t  Loss: 0.010115696117281914  \t  Time: 169.6sec/epoch\n",
            "175.  \t  Loss: 0.011351815424859524  \t  Time: 169.12sec/epoch\n",
            "176.  \t  Loss: 0.011887012980878353  \t  Time: 169.61sec/epoch\n",
            "177.  \t  Loss: 0.013446150347590446  \t  Time: 169.47sec/epoch\n",
            "178.  \t  Loss: 0.007656052242964506  \t  Time: 169.67sec/epoch\n",
            "179.  \t  Loss: 0.008559751324355602  \t  Time: 169.65sec/epoch\n",
            "180.  \t  Loss: 0.01356053352355957  \t  Time: 169.69sec/epoch\n",
            "181.  \t  Loss: 0.010984550230205059  \t  Time: 169.57sec/epoch\n",
            "182.  \t  Loss: 0.00953430775552988  \t  Time: 170.0sec/epoch\n",
            "183.  \t  Loss: 0.010474968701601028  \t  Time: 170.38sec/epoch\n",
            "184.  \t  Loss: 0.0111949872225523  \t  Time: 169.91sec/epoch\n",
            "185.  \t  Loss: 0.010575664229691029  \t  Time: 169.95sec/epoch\n",
            "186.  \t  Loss: 0.009525610134005547  \t  Time: 169.71sec/epoch\n",
            "187.  \t  Loss: 0.01164042018353939  \t  Time: 170.19sec/epoch\n",
            "188.  \t  Loss: 0.00999219249933958  \t  Time: 170.21sec/epoch\n",
            "189.  \t  Loss: 0.010422180406749249  \t  Time: 170.27sec/epoch\n",
            "190.  \t  Loss: 0.008966557681560516  \t  Time: 170.31sec/epoch\n",
            "191.  \t  Loss: 0.00785275548696518  \t  Time: 170.49sec/epoch\n",
            "192.  \t  Loss: 0.009763702750205994  \t  Time: 170.18sec/epoch\n",
            "193.  \t  Loss: 0.009690961800515652  \t  Time: 169.87sec/epoch\n",
            "194.  \t  Loss: 0.01045454666018486  \t  Time: 170.09sec/epoch\n",
            "195.  \t  Loss: 0.007022708188742399  \t  Time: 169.88sec/epoch\n",
            "196.  \t  Loss: 0.007125671487301588  \t  Time: 170.4sec/epoch\n",
            "197.  \t  Loss: 0.009209809824824333  \t  Time: 170.01sec/epoch\n",
            "198.  \t  Loss: 0.009425624273717403  \t  Time: 170.33sec/epoch\n",
            "199.  \t  Loss: 0.008091235533356667  \t  Time: 170.33sec/epoch\n",
            "200.  \t  Loss: 0.009743629023432732  \t  Time: 170.16sec/epoch\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(200, None)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (200, None, 300)     18600       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (200, None, 300)     90300       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(200, None, 2048),  19243008    dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (200, None, 300)     614700      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(200, None, 2048),  19243008    dense_1[0][0]                    \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (200, None, 300)     614700      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (200, None, 62)      18662       dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 39,842,978\n",
            "Trainable params: 39,842,978\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "####### TEXT GENERATION - temperature = 0.1\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura,\n",
            "poi cominciò: \"Tu stesso ti fai grosso\n",
            "col falso imaginar, sì che non vedi\". \n",
            "\n",
            "Poi si rivolse a quella 'nfiata labbia,\n",
            "e disse: \"Taci, maladetto lupo\n",
            "saltò 'l cantar non si può con altro: \n",
            "\n",
            "ch'io veggio certamente, e però il narro,\n",
            "a darne tempo già stelle propinque,\n",
            "secure a l'ormo fatto chinivi. \n",
            "\n",
            "\"Quell'anima là sù c' ha maggior pena\n",
            "di sua matera fosti s'è davante,\n",
            "lasciando sua vita a far sua fattura, \n",
            "\n",
            "con quella parte che sù si rammenta\n",
            "e in difetto verrà dopo 'l fiore\".\n",
            "\n",
            "\n",
            "\n",
            "Poscia che i fiori e l'altre fresche erbette\n",
            "a rimpetto di me fare spezzo\n",
            "e stupor molt' anime de' sue. \n",
            "\n",
            "A quella luce cotal si confessa\n",
            "lo perfido assessin, che, poi ch'è fitto,\n",
            "richiama lui per che la morte avante. \n",
            "\n",
            "L'altra Tronca e bestie fier le spieghi\n",
            "di nome più di bele; onde biadette,\n",
            "quant'ella più verso le pale approccia. \n",
            "\n",
            "E se Dio m' ha in sua grazia rimase\n",
            "e l'altro con l'altro come se brutte,\n",
            "poscia c' ha' il corpo al primo tempo. \n",
            "\n",
            "Da questo ingrassa il \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 0.5\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura,\n",
            "poi cominciai: \"Belacqua, a maggior fretta\n",
            "nè fu fatta la gente di sè torse. \n",
            "\n",
            "\"O dolce lume a cui fidanza i' entro\n",
            "per lo novo cammin, tu se' credete\n",
            "per sozz'arrostarsi qua, per tremare. \n",
            "\n",
            "Dè questo luci, in cui formazion muca,\n",
            "de' quai piange da l'altro esso dopo,\n",
            "come che solo infiamma alcun facelle! \n",
            "\n",
            "Bestemmiavano Dio e lor parenti,\n",
            "l'umana spezie e 'l loco e 'l tempo e 'l seme\n",
            "di lor semenza e di lor nascimenti. \n",
            "\n",
            "Poi si ritrasser tutte quante insieme,\n",
            "forte piangendo, a la riva malvagia\n",
            "ch'attende ciascun uom che Dio non teme. \n",
            "\n",
            "Caron dimonio, con occhi di bragia\n",
            "loro accennando, tutte le raccoglie;\n",
            "batte col remo qualunque s'adagia. \n",
            "\n",
            "Come d'autor più là sù tosto mota,\n",
            "venendo e trapassando ci ammirava\n",
            "l'animo ad avviso, tanto s'amafia; \n",
            "\n",
            "ma misi me per l'alto mare aperto,\n",
            "ma Ben sè gran maraviglia! ch'a questo stagno\n",
            "toglie alcun de' raggi de la terra accefe. \n",
            "\n",
            "Non sarà tutto tempo sanza frutto,\n",
            "e così mi chiedi; vi confina,\n",
            "che a quel m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 1.0\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura,\n",
            "poi che i vederlo attuffare in questa\n",
            "su per lo suol che d'ogne parte a pensata: \n",
            "\n",
            "non altrimenti l'anitra di botto,\n",
            "quando 'l falcon s'appressa, giù s'attuffa,\n",
            "ed ei ritorna sù cred esse basso. \n",
            "\n",
            "\"O dolce segnor mio\", diss'io, \"or mi dì\n",
            "a quel che tu credi là sù si facea\n",
            "la sua parola tra Savio; e chi sieti?\". \n",
            "\n",
            "Quando mi fu colui che si veggi \n",
            "\n",
            "col mio antecr,, se Stigno trapello,\n",
            "e volsimi li occhi a li occhi belli\n",
            "cantare al Mondo fatto a riguardommi. \n",
            "\n",
            "Già montavam su per li scaglion santi,\n",
            "or vedi tu la morte cera e di Guitto;\n",
            "e già da questo 'nferno traegue mosso,\n",
            "e al suo corpo non volle altra bara, \n",
            "\n",
            "quando s'alcuna strenta ancor si brama,\n",
            "per segnor de' molto anguillar cantando\n",
            "che tu se' venir, dopo lui per poco\". \n",
            "\n",
            "L'un puose l'omo e sì alto sconoscanna,\n",
            "per occulto vero amore, a la mia 'ntesa,\n",
            "per veder de la bolgia ogne consorte, \n",
            "\n",
            "fece lui parlava ancor la lingua stretta\".\n",
            "\n",
            "Noi siam qui con inferno il conserto pareo\n",
            "lo suo primo pen\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 1.5\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura,\n",
            "poi cominciai: \"Belacqua, a maggior sazia\n",
            "per patrapare e Tuggi sì rattofolgo. \n",
            "\n",
            "Di ciò esper, de la doppia danna\n",
            "si fece la cogione, io tondo'\n",
            "Già era al mondo e quel mi t'affraggia. \n",
            "\n",
            "Quant' esser puote ch'un fedesi arnoce\n",
            "dove 'l sacro fonte intender crede\n",
            "quand' era nel concetto e 'Ba si spazia. \n",
            "\n",
            "Qual suole il fiammeggiar del folgòr santo,\n",
            "a ch'io mi volsi, conobbi la voglia\n",
            "in lui di ragionarmi ancora alquanto. \n",
            "\n",
            "El mi pari, poi che fu a terra salute,\n",
            "poscia c' ha' il Zondo e d'Agesto brutto\n",
            "e de la propria convien sanza guerra. \n",
            "\n",
            "Pensa, lettor, se io mi sconfortai\n",
            "nel suon de le parole maladette,\n",
            "quando Maria nel figlio nè quindi. \n",
            "\n",
            "La cittadinanza, ch'è or mista\n",
            "mi fè desideroso di sapere,\n",
            "pur che la terra ciascuna s'appunta. \n",
            "\n",
            "Ben s'avvide il poeta ch'io stava\n",
            "stupido tutto al carro de la luce,\n",
            "ove tra 'l bè e 'l sepolcre quindi stinge. \n",
            "\n",
            "Però salta la sempiterante forte\n",
            "che lagrime tra noi, e quella è contegno,\n",
            "mentre che i primi bianchi\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 2\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Attibili già che non s'ammormando,\n",
            "non t' ho fatto che tu dicheto al Nostro\n",
            "guarda con la prima e con segno dov'e'. \n",
            "\n",
            "Se raro e già da l'alto Lucigione\n",
            "Prende l'alma che fuor mi serra prega!\n",
            "consi or vede; e quando tu riedi, \n",
            "\n",
            "vadi a mia bella figlia, nè la sola\n",
            "tuo metulato aveste risponde\",\n",
            "cominciò el, \"coi son par che biglio\". \n",
            "\n",
            "E io: \"Maestro, molto sarei vago\n",
            "di vederlo attuffare in questa broda\n",
            "prima che noi uscisser, tra quei morse. \n",
            "\n",
            "Sù mi spronaron le parole sue,\n",
            "ch'i' ho le membra legate in quella oscura,\n",
            "parrò la difalta con dritta spera. \n",
            "\n",
            "Prima che sie là sù, tornar per traparsi\n",
            "dove acquistar, la bene al fondo,\n",
            "s'accorser contra sè andata poco, \n",
            "\n",
            "quando un'altra storpiar maligno e 'l brigata,\n",
            "libertà fu de l'orlo, uno in quella,\n",
            "e recò a già di quel nome è vacante\". \n",
            "\n",
            "Con men sì alcun legno spiro s'insala,\n",
            "ch'avèa in ella Crima che s'asconde\n",
            "sotto 'l velame sinfin testè piaggia. \n",
            "\n",
            "Alte terrà così veloce l'amore\n",
            "fischiato vi la donna mia scida e Zarte,\n",
            "per quella com' era\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKysyBfpAtUS",
        "outputId": "7b6280d8-34fd-4c38-87f7-2f4888a2384c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    Dense(embedding_size, activation = relu),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "\n",
        "    Dropout(0.3),\n",
        "\n",
        "    LSTM(len_input, return_sequences = True),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "RNN.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (200, None, 300)          18600     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (200, None, 300)          90300     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (200, None, 1024)         5427200   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (200, None, 1024)         0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (200, None, 300)          307500    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (200, None, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (200, None, 1024)         5427200   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (200, None, 62)           63550     \n",
            "=================================================================\n",
            "Total params: 11,334,350\n",
            "Trainable params: 11,334,350\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9Tldq4qAtXr"
      },
      "source": [
        "n_epochs = 20\n",
        "\n",
        "learning_rate = 0.001  # 0.0001\n",
        "optimizer = tf.keras.optimizers.Adamax(learning_rate = learning_rate)  # Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUgGeC5ZAtbb"
      },
      "source": [
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                y, RNN(x), from_logits = True))\n",
        "    gradients = tape.gradient(current_loss, RNN.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, RNN.trainable_variables))\n",
        "    return current_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-x5RxKrAtnR",
        "outputId": "c039a8de-7cee-4451-950e-e8d4e9d9d984",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "    \n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"{}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch+1, current_loss.numpy(), round(time.time()-start, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.  \t  Loss: 2.9850523471832275  \t  Time: 51.65ss\n",
            "2.  \t  Loss: 2.395160436630249  \t  Time: 50.83ss\n",
            "3.  \t  Loss: 2.1662957668304443  \t  Time: 50.68ss\n",
            "4.  \t  Loss: 2.0440170764923096  \t  Time: 50.71ss\n",
            "5.  \t  Loss: 1.9688301086425781  \t  Time: 50.72ss\n",
            "6.  \t  Loss: 1.8783867359161377  \t  Time: 50.68ss\n",
            "7.  \t  Loss: 1.8288464546203613  \t  Time: 50.73ss\n",
            "8.  \t  Loss: 1.7976200580596924  \t  Time: 50.65ss\n",
            "9.  \t  Loss: 1.750290870666504  \t  Time: 50.76ss\n",
            "10.  \t  Loss: 1.7273554801940918  \t  Time: 50.68ss\n",
            "11.  \t  Loss: 1.6812254190444946  \t  Time: 50.65ss\n",
            "12.  \t  Loss: 1.6205040216445923  \t  Time: 50.74ss\n",
            "13.  \t  Loss: 1.595969796180725  \t  Time: 50.67ss\n",
            "14.  \t  Loss: 1.5555707216262817  \t  Time: 50.72ss\n",
            "15.  \t  Loss: 1.5181407928466797  \t  Time: 50.72ss\n",
            "16.  \t  Loss: 1.501092791557312  \t  Time: 50.77ss\n",
            "17.  \t  Loss: 1.4644851684570312  \t  Time: 50.7ss\n",
            "18.  \t  Loss: 1.4569607973098755  \t  Time: 50.63ss\n",
            "19.  \t  Loss: 1.4066871404647827  \t  Time: 50.63ss\n",
            "20.  \t  Loss: 1.3883055448532104  \t  Time: 50.69ss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6uAhmV2Atth",
        "outputId": "85594edf-fec3-4cbb-ac2b-01e07258985a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCsDapUTBBrE"
      },
      "source": [
        "RNN.save( \"text_generator_RNN_03.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrMdonRckAWB"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "At this point, let's check how the model generates text. In order to do it, I must make some changes to my RNN architecture above.\n",
        "\n",
        "First, I must change the fixed batch size. After training, I want to feed just one sentence into my Network to make it continue the character sequence. I will feed a string into the model, make it predict the next character, update the input sequence, and repeat the process until a long generated text is obtained. Because of this, the succession of input sequences is now different from training session, in which portions of text were sampled randomly. I now have to set `stateufl = True` in the `LSTM()` layer, so that each LSTM cell will keep in memory the internal state from the previous sequence. With this I hope the model will better remember sequential information while generating text.\n",
        "\n",
        "I will instantiate a new `generator` RNN with these new features, and transfer the trained weights of my `RNN` into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69aV7FGVAtxY",
        "outputId": "a7659852-e5a5-4d12-f9cc-dbaa71a17060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "generator = Sequential([\n",
        "   Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    Dense(embedding_size, activation = relu),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=1),\n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "\n",
        "    Dropout(0.3),\n",
        "\n",
        "    LSTM(len_input, return_sequences = True, stateful=1),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 300)            18600     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (1, None, 300)            90300     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (1, None, 1024)           5427200   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 300)            307500    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (1, None, 300)            0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (1, None, 1024)           5427200   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (1, None, 62)             63550     \n",
            "=================================================================\n",
            "Total params: 11,334,350\n",
            "Trainable params: 11,334,350\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whB1azhVAtrp"
      },
      "source": [
        "# Import trained weights from RNN to generator\n",
        "generator.set_weights(RNN.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2hYSqAAtkn"
      },
      "source": [
        "def generate_text(start_string, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    generator.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hBCF5YrqBtG"
      },
      "source": [
        "(This function is based on [this tutorial](https://www.tensorflow.org/tutorials/text/text_generation).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_NWeo1fAtiC",
        "outputId": "a5a4a6e9-1b00-489e-eb3b-db23872d16d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's feed the first lines:\n",
        "start_string = \"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for t in [0.1, 0.5, 1.0, 1.5, 2]:\n",
        "    print(\"####### TEXT GENERATION - temperature = {}\\n\".format(t))\n",
        "    print(generate_text(start_string = start_string, num_generate = 1000, temperature = 1.0))\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####### TEXT GENERATION - temperature = 0.1\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "\n",
            "\n",
            "Ora questo che tenna con esso pianta,\n",
            "fonno de li altri rispiose poscegno,\n",
            "che idronemo e menera a fruttorna\n",
            "nei perchè tanta inverno, tutte 'l puoi\".\n",
            "\n",
            "\n",
            "\n",
            "in vendi ' pietra il tanto parte era,\n",
            "primandono entro, evei nè me si stesse\n",
            "buono alqual ch'ogne non aperto leto\n",
            "ch'el tanto inghiecco quell'ca divina. \n",
            "\n",
            "Quasi alloral lorar com'io duca;\n",
            "e riguarrille, duo per malco a brace\n",
            "me che già veglia, quanto purto colpa\n",
            "fora d'un che, profodo scallegio\n",
            "ch'io disse: \"E questo fiate menore. \n",
            "\n",
            "Centor Berve fui che tant' io luoto:\n",
            "chè, quando le cagion che s'accriga una;\n",
            "la cima prei del vide, ed esse, anda\n",
            "sodra, andarata drite, e il trascosto,\n",
            "il parlarento e 'l pèotò come avveggia,\n",
            "tal che d'angella sua leggia condetta,\n",
            "sopr'alla voi di mezzo solo è benno. \n",
            "\n",
            "Io mi fua lantia, inverabon pria 'ntrima\n",
            "di ruoi senti\n",
            "la vente mio menando me ormi?\",\n",
            "donne di Romagni, disso d'io mi,\n",
            "non tu maria, sì come in altro samme.\n",
            "\n",
            "\n",
            "\n",
            "Sovr'elli a altro con lor de la Dena arco,\n",
            "sanza 'l mondo: \"Te del vazion mi\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 0.5\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "\n",
            "\n",
            "Di ebbe di sè, ond'ei, a me secorci,\n",
            "vanno tanto fu giù o con s'imparto,\n",
            "come volonci loro salir porta. \n",
            "\n",
            "Quidi mai se ne cosa un socondo\n",
            "al letizion come frutro ha temo\n",
            "futa! m'asplendo del grande quitiega\n",
            "tanto ne le ciglia nel vepo s'avvanta, \n",
            "\n",
            "di nostro ascoppiavar per lo Carcolsi. \n",
            "\n",
            "Sì, stalinte, orai: 'D accoltati pruncia\n",
            "ch'ai per son medo in tanto, dolce, e stani\n",
            "con la terra era d'il facer l'altra grudla;\n",
            "ma tenea id essa, lungua c'egno a lume\n",
            "del maggio de le morti e mal par notto,\n",
            "perchè di fermal sotto d'acunto Mero; \n",
            "\n",
            "e in ciascungando, a guorda intero,\n",
            "più che raccopar vegge e veniresse\n",
            "li colonte: \"Deguatili altriti quanti\n",
            "vorte, perchè il son, ch'a' non vi voi nice. \n",
            "\n",
            "Ond'ei la Sùcci a donna mia costuida\n",
            "percha quel nostro casper nalevata\n",
            "infini a penedèo di nostra verta,\n",
            "forta fare in tanto ch'i' poè erano\n",
            "si proversi per attizioni domoni;\n",
            "e sè non sogvaca mano e per la motta. \n",
            "\n",
            "Boragavammo e del maglio ch'io sani\n",
            "feggiro di luci, e 'oa dempre a me, asco'\n",
            "vedregna vo\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 1.0\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "\n",
            "\n",
            "I' son durin Migliei e canterai tuggi\n",
            "un come store in te che tu dicendava\n",
            "tre non conforto che 'l batto I' su' Fivelle\n",
            "\n",
            "da tal del mio de da Carinati umica,\n",
            "quando abbrocito de l'accelsere,\n",
            "confanonel tempre che pranti il bello\n",
            "che mi facea la trezze poi mi spirgo\n",
            "intratar sì, di quaste era la scira,\n",
            "che la, dove' ole carifi pennardi\n",
            "a l'altrer di li ammapiò che laccia; \n",
            "\n",
            "per che 'l monne in persona tira a seli\n",
            "li occhiaturi, se non mai meriguera,\n",
            "e facce fosse, li occhi non la mistela\n",
            "fè, per contretto come 'l mio disir Morti,\n",
            "per questa lume che non ere congagna. \n",
            "\n",
            "Na parer giù di lubine grazia\n",
            "ogni la siauperacca la qui gintora;\n",
            "chè pè non m'amari si frusia distra\n",
            "ond' era battiai del faggine e loco\n",
            "sol miscorte a me ò a le sensunaglie\n",
            "de l'esser di Donderno passi Duone. \n",
            "\n",
            "Così andando, e anchunata bena;\n",
            "ond' elli a Dispondio era d'altro era,\n",
            "però s'amposo mov'io bello molto\n",
            "avere: \"Ragione, dicean sovra imelazzo. \n",
            "\n",
            "Giusavanlo a li lui, che s'adea ciaca\n",
            "che fuor di Nrigi stagi; e\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 1.5\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Mostra che la vista mia mai truttuto\n",
            "ovea tutto somenta che calce\n",
            "non è come va cle da ciEsto spallo. \n",
            "\n",
            "Frensi molte sterò, e se tii, pria guardo?\".\n",
            "\n",
            "Niondo ch'io ti disse: \"Già nel mio scliuna\n",
            "quante rispare a mira, per lo callo\n",
            "del marcoso e lomoro e rappio dolo\n",
            "di Sania, quando reminando sprava,\n",
            "foziando oltre altre 'mbe, e pur che vanti\n",
            "luce l'uno i tanto oi qui tutt' inziade; \n",
            "\n",
            "però ch'altra 'l bolo incura intueme in giusto?\n",
            "per che 'l temesta la costa voluta\n",
            "quanto avea\n",
            "di tre di noi padre più segno; \n",
            "\n",
            "e figura e d'interno; e questi palmo,\n",
            "ne li occhi ingiuro e suobissivo laglo\n",
            "de l'uome infesse e con questo forsuni\n",
            "mosse curato santa che chi soglio,\n",
            "come tu' sondi ora sì di lei tronco:\n",
            "\"Se n'è coglia di là Pibar sì sùgni: \n",
            "\n",
            "Che men ch'io non ch'a non mosser sottimi segni\n",
            "l'una figliarì benchè a l'altro era pronte\n",
            "che la passor tivilla questè saliggia\n",
            "di noi ch'ora sotto l'uni a tu' Farmagna;\n",
            "persa: \"\n",
            "chi ten, fiena alte sperante canto\n",
            "sienzo a la soli n'è, per grazia spiore\n",
            "ad u\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 2\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "\n",
            "\n",
            "Indi turbesi genti del mado spregna,\n",
            "e 'l solpo frendo: \"Se non cenì guida,\n",
            "mader volti e di fiero a li portivi\n",
            "onde parte dal ferto suo nonnello\". \n",
            "\n",
            "Ed elli a padre un Boco in sopra sue\n",
            "capel pettalso nè lo luce si porta,\n",
            "e, disse l'altro magnor vivoneste\n",
            "leggrempi in al testaer, per ferma l'an. \n",
            "\n",
            "Sì come colfurmi far la gran sole\n",
            "quello sormor mio, ch'i' peate intaro\n",
            "quando di Siguar, ch'a è di tutti passi,\n",
            "secondo me che occhi ogne sua propria,\n",
            "va si stucca. \n",
            "Poi i genti uncir de la, taldo famme\n",
            "di qui navvelletta, e puòse a li orsigni\n",
            "votan sanzi a noi: esser si var come, \n",
            "\n",
            "cui mi feate che 'ntichi paria quivi\n",
            "lui che voludi' la mostra, fosserco\n",
            "a matitai; e cominciò a Dio cose,\n",
            "sì com'era parer sperchè l'amorello. \n",
            "\n",
            "Innanziammo a lo intorno marcallo\n",
            "che conquintare li occhi in sì pur aita,\n",
            "velidente fa due Crederesta\n",
            "quel che gira ceduto lo fampuotante\n",
            "me a le Zeginte che lacea la stella, \n",
            "\n",
            "quell'i' penti, \"Dirgendui lui la calla\n",
            "de le vise a le terre degno pare\n",
            "di tutto innanzi\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8KlRGTqmGBS"
      },
      "source": [
        "The best generation is, IMHO, the one with `temperature = 1.5`. The sentences of course do not make sense, but it's amazing that such a simple model could achieve similar results, and generate absolutely Dante-esque text with just ~40 minutes of GPU training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtB2gvhimUEs"
      },
      "source": [
        "Many things could be done at this point:\n",
        "\n",
        "\n",
        "\n",
        "*   Try fancier architectures, such as seq2seq. (I must say though that stacked RNNs didn't provide better results during prototyping.)\n",
        "*   Try Attention models.\n",
        "*   Longer training.\n",
        "*   Adversarial training.\n",
        "\n",
        "I'll try a lot of these techniques, alone and combined. My goal is to make a model that can learn the amazing structure of syllables and rhymes of the whole Comedy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbXuy2OopudL"
      },
      "source": [
        "# NEW IDEAS\n",
        "\n",
        "#### Training:\n",
        "*   Cross validation\n",
        "*   Insert Rhyme as feature to learn as haiku\n",
        "*   Use syllable as input and not word\n",
        "*   Different training on different dataset\n",
        "* Use categorical_crossentropy instead of sparse_ but with one-hot encoded inputs\n",
        "* Symbols for explicit start and end terzina\n",
        "* training as classificator for structure: like \"these two world are rhymes\" or \"this is a endecasillable and this not\" or \"this is a terzina and this not\" then generation\n",
        "* use dropout \n",
        "* use two lstm\n",
        "* \n",
        "\n",
        "#### Presentation\n",
        "* graphs over the vocabulary like distribution of used words\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7hZnK6wpy0j"
      },
      "source": [
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "              \n",
        "    Dense(embedding_size, activation = relu),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "RNN.summary()\n",
        "\n",
        "generator = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,batch_input_shape=(1, None)),\n",
        "\n",
        "    Dense(embedding_size, activation = relu),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}