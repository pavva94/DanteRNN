{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m8xB0z_hYbG"
      },
      "source": [
        "# DeepComedy: AI Generated Divine Comedy\n",
        "\n",
        "Author: **Alessandro Pavesi, Federico Battistella**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncDUryHdqckI"
      },
      "source": [
        "This Notebook contains a **text generator RNN** that was trained on the **Divina Commedia** (the *Divine Comedy*) by **Dante Alighieri**. \n",
        "\n",
        "It's structure is extremely complex: the poem is composed by three Cantiche, each Cantica has 33 Terzine, each Terzina is composed by three verses, each verse is composed of 11 syllables, and its rhymes follow an **A-B-A-B-C-B-C** structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8FnhxiWY_Y1",
        "outputId": "611d69b9-6bed-40b3-9e59-51e2c18a83f4"
      },
      "source": [
        "import time\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Attention, Flatten, Input, BatchNormalization\n",
        "from tensorflow.keras.activations import elu, relu, softmax\n",
        "from tensorflow.keras.metrics import categorical_accuracy, sparse_categorical_crossentropy, categorical_crossentropy\n",
        "\n",
        "from matplotlib import pyplot as plt "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVW2XbaZ_GGM"
      },
      "source": [
        "# Preliminaries Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkhp6e4Sztim"
      },
      "source": [
        "## Import and initial cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5YiWU0daYkT"
      },
      "source": [
        "# Read the Divina Commedia\n",
        "with open( \"DivinaCommedia.txt\", 'r', encoding=\"utf8\") as file:\n",
        "    divina_commedia = file.read()\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'\\[.*\\r?\\n', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'.*Canto.*\\r?\\n', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'.*?\\n\\n\\n\\n', \"\", divina_commedia)  # remove the last row of each Canto, it's alone and can ruin the generation on correct terzine\n",
        "\n",
        "# divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")  # with this i lose the \"terzina\": results are not so exciting\n",
        "#divina_commedia = divina_commedia.replace(\" \\n\", \"<eot>\")  # end of terzina\n",
        "#divina_commedia = divina_commedia.replace(\"\\n\", \"<eor>\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynI7x9Rp7yQe",
        "outputId": "04776d0e-db21-4cee-9196-d2a99c5ccd8e"
      },
      "source": [
        "print(divina_commedia[1:15000])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "el mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita. \n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura\n",
            "esta selva selvaggia e aspra e forte\n",
            "che nel pensier rinova la paura! \n",
            "\n",
            "Tant'è amara che poco è più morte;\n",
            "ma per trattar del ben ch'i' vi trovai,\n",
            "dirò de l'altre cose ch'i' v' ho scorte. \n",
            "\n",
            "Io non so ben ridir com'i' v'intrai,\n",
            "tant'era pien di sonno a quel punto\n",
            "che la verace via abbandonai. \n",
            "\n",
            "Ma poi ch'i' fui al piè d'un colle giunto,\n",
            "là dove terminava quella valle\n",
            "che m'avea di paura il cor compunto, \n",
            "\n",
            "guardai in alto e vidi le sue spalle\n",
            "vestite già de' raggi del pianeta\n",
            "che mena dritto altrui per ogne calle. \n",
            "\n",
            "Allor fu la paura un poco queta,\n",
            "che nel lago del cor m'era durata\n",
            "la notte ch'i' passai con tanta pieta. \n",
            "\n",
            "E come quei che con lena affannata,\n",
            "uscito fuor del pelago a la riva,\n",
            "si volge a l'acqua perigliosa e guata, \n",
            "\n",
            "così l'animo mio, ch'ancor fuggiva,\n",
            "si volse a retro a rimirar lo passo\n",
            "che non lasciò già mai persona viva. \n",
            "\n",
            "Poi ch'èi posato un poco il corpo lasso,\n",
            "ripresi via per la piaggia diserta,\n",
            "sì che 'l piè fermo sempre era 'l più basso. \n",
            "\n",
            "Ed ecco, quasi al cominciar de l'erta,\n",
            "una lonza leggera e presta molto,\n",
            "che di pel macolato era coverta; \n",
            "\n",
            "e non mi si partia dinanzi al volto,\n",
            "anzi 'mpediva tanto il mio cammino,\n",
            "ch'i' fui per ritornar più volte vòlto. \n",
            "\n",
            "Temp'era dal principio del mattino,\n",
            "e 'l sol montava 'n sù con quelle stelle\n",
            "ch'eran con lui quando l'amor divino \n",
            "\n",
            "mosse di prima quelle cose belle;\n",
            "sì ch'a bene sperar m'era cagione\n",
            "di quella fiera a la gaetta pelle \n",
            "\n",
            "l'ora del tempo e la dolce stagione;\n",
            "ma non sì che paura non mi desse\n",
            "la vista che m'apparve d'un leone. \n",
            "\n",
            "Questi parea che contra me venisse\n",
            "con la test'alta e con rabbiosa fame,\n",
            "sì che parea che l'aere ne tremesse. \n",
            "\n",
            "Ed una lupa, che di tutte brame\n",
            "sembiava carca ne la sua magrezza,\n",
            "e molte genti fè già viver grame, \n",
            "\n",
            "questa mi porse tanto di gravezza\n",
            "con la paura ch'uscia di sua vista,\n",
            "ch'io perdei la speranza de l'altezza. \n",
            "\n",
            "E qual è quei che volontieri acquista,\n",
            "e giugne 'l tempo che perder lo face,\n",
            "che 'n tutti suoi pensier piange e s'attrista; \n",
            "\n",
            "tal mi fece la bestia sanza pace,\n",
            "che, venendomi 'ncontro, a poco a poco\n",
            "mi ripigneva là dove 'l sol tace. \n",
            "\n",
            "Mentre ch'i' rovinava in basso loco,\n",
            "dinanzi a li occhi mi si fu offerto\n",
            "chi per lungo silenzio parea fioco. \n",
            "\n",
            "Quando vidi costui nel gran diserto,\n",
            "\"Miserere di me\", gridai a lui,\n",
            "\"qual che tu sii, od ombra od omo certo!\". \n",
            "\n",
            "Rispuosemi: \"Non omo, omo già fui,\n",
            "e li parenti miei furon lombardi,\n",
            "mantoani per patria ambedui. \n",
            "\n",
            "Nacqui sub Iulio, ancor che fosse tardi,\n",
            "e vissi a Roma sotto 'l buono Augusto\n",
            "nel tempo de li dèi falsi e bugiardi. \n",
            "\n",
            "Poeta fui, e cantai di quel giusto\n",
            "figliuol d'Anchise che venne di Troia,\n",
            "poi che 'l superbo Iliòn fu combusto. \n",
            "\n",
            "Ma tu perchè ritorni a tanta noia?\n",
            "perchè non sali il dilettoso monte\n",
            "ch'è principio e cagion di tutta gioia?\". \n",
            "\n",
            "\"Or se' tu quel Virgilio e quella fonte\n",
            "che spandi di parlar sì largo fiume?\",\n",
            "rispuos'io lui con vergognosa fronte. \n",
            "\n",
            "\"O de li altri poeti onore e lume,\n",
            "vagliami 'l lungo studio e 'l grande amore\n",
            "che m' ha fatto cercar lo tuo volume. \n",
            "\n",
            "Tu se' lo mio maestro e 'l mio autore,\n",
            "tu se' solo colui da cu' io tolsi\n",
            "lo bello stilo che m' ha fatto onore. \n",
            "\n",
            "Vedi la bestia per cu' io mi volsi;\n",
            "aiutami da lei, famoso saggio,\n",
            "ch'ella mi fa tremar le vene e i polsi\". \n",
            "\n",
            "\"A te convien tenere altro viaggio\",\n",
            "rispuose, poi che lagrimar mi vide,\n",
            "\"se vuo' campar d'esto loco selvaggio; \n",
            "\n",
            "chè questa bestia, per la qual tu gride,\n",
            "non lascia altrui passar per la sua via,\n",
            "ma tanto lo 'mpedisce che l'uccide; \n",
            "\n",
            "e ha natura sì malvagia e ria,\n",
            "che mai non empie la bramosa voglia,\n",
            "e dopo 'l pasto ha più fame che pria. \n",
            "\n",
            "Molti son li animali a cui s'ammoglia,\n",
            "e più saranno ancora, infin che 'l veltro\n",
            "verrà, che la farà morir con doglia. \n",
            "\n",
            "Questi non ciberà terra nè peltro,\n",
            "ma sapienza, amore e virtute,\n",
            "e sua nazion sarà tra feltro e feltro. \n",
            "\n",
            "Di quella umile Italia fia salute\n",
            "per cui morì la vergine Cammilla,\n",
            "Eurialo e Turno e Niso di ferute. \n",
            "\n",
            "Questi la caccerà per ogne villa,\n",
            "fin che l'avrà rimessa ne lo 'nferno,\n",
            "là onde 'nvidia prima dipartilla. \n",
            "\n",
            "Ond'io per lo tuo me' penso e discerno\n",
            "che tu mi segui, e io sarò tua guida,\n",
            "e trarrotti di qui per loco etterno; \n",
            "\n",
            "ove udirai le disperate strida,\n",
            "vedrai li antichi spiriti dolenti,\n",
            "ch'a la seconda morte ciascun grida; \n",
            "\n",
            "e vederai color che son contenti\n",
            "nel foco, perchè speran di venire\n",
            "quando che sia a le beate genti. \n",
            "\n",
            "A le quai poi se tu vorrai salire,\n",
            "anima fia a ciò più di me degna:\n",
            "con lei ti lascerò nel mio partire; \n",
            "\n",
            "chè quello imperador che là sù regna,\n",
            "perch'i' fu' ribellante a la sua legge,\n",
            "non vuol che 'n sua città per me si vegna. \n",
            "\n",
            "In tutte parti impera e quivi regge;\n",
            "quivi è la sua città e l'alto seggio:\n",
            "oh felice colui cu' ivi elegge!\". \n",
            "\n",
            "E io a lui: \"Poeta, io ti richeggio\n",
            "per quello Dio che tu non conoscesti,\n",
            "acciò ch'io fugga questo male e peggio, \n",
            "\n",
            "che tu mi meni là dov'or dicesti,\n",
            "sì ch'io veggia la porta di san Pietro\n",
            "e color cui tu fai cotanto mesti\". \n",
            "\n",
            "Lo giorno se n'andava, e l'aere bruno\n",
            "toglieva li animai che sono in terra\n",
            "da le fatiche loro; e io sol uno\n",
            "\n",
            "m'apparecchiava a sostener la guerra\n",
            "sì del cammino e sì de la pietate,\n",
            "che ritrarrà la mente che non erra. \n",
            "\n",
            "O muse, o alto ingegno, or m'aiutate;\n",
            "o mente che scrivesti ciò ch'io vidi,\n",
            "qui si parrà la tua nobilitate. \n",
            "\n",
            "Io cominciai: \"Poeta che mi guidi,\n",
            "guarda la mia virtù s'ell'è possente,\n",
            "prima ch'a l'alto passo tu mi fidi. \n",
            "\n",
            "Tu dici che di Silvio il parente,\n",
            "corruttibile ancora, ad immortale\n",
            "secolo andò, e fu sensibilmente. \n",
            "\n",
            "Però, se l'avversario d'ogne male\n",
            "cortese i fu, pensando l'alto effetto\n",
            "ch'uscir dovea di lui, e 'l chi e 'l quale \n",
            "\n",
            "non pare indegno ad omo d'intelletto;\n",
            "ch'e' fu de l'alma Roma e di suo impero\n",
            "ne l'empireo ciel per padre eletto: \n",
            "\n",
            "la quale e 'l quale, a voler dir lo vero,\n",
            "fu stabilita per lo loco santo\n",
            "u' siede il successor del maggior Piero. \n",
            "\n",
            "Per quest'andata onde li dai tu vanto,\n",
            "intese cose che furon cagione\n",
            "di sua vittoria e del papale ammanto. \n",
            "\n",
            "Andovvi poi lo Vas d'elezione,\n",
            "per recarne conforto a quella fede\n",
            "ch'è principio a la via di salvazione. \n",
            "\n",
            "Ma io, perchè venirvi? o chi 'l concede?\n",
            "Io non Enèa, io non Paulo sono;\n",
            "me degno a ciò nè io nè altri 'l crede. \n",
            "\n",
            "Per che, se del venire io m'abbandono,\n",
            "temo che la venuta non sia folle.\n",
            "Se' savio; intendi me' ch'i' non ragiono\". \n",
            "\n",
            "E qual è quei che disvuol ciò che volle\n",
            "e per novi pensier cangia proposta,\n",
            "sì che dal cominciar tutto si tolle, \n",
            "\n",
            "tal mi fec'io 'n quella oscura costa,\n",
            "perchè, pensando, consumai la 'mpresa\n",
            "che fu nel cominciar cotanto tosta. \n",
            "\n",
            "\"S'i' ho ben la parola tua intesa\",\n",
            "rispuose del magnanimo quell'ombra,\n",
            "\"l'anima tua è da viltade offesa; \n",
            "\n",
            "la qual molte fiate l'omo ingombra\n",
            "sì che d'onrata impresa lo rivolve,\n",
            "come falso veder bestia quand'ombra. \n",
            "\n",
            "Da questa tema acciò che tu ti solve,\n",
            "dirotti perch'io venni e quel ch'io 'ntesi\n",
            "nel primo punto che di te mi dolve. \n",
            "\n",
            "Io era tra color che son sospesi,\n",
            "e donna mi chiamò beata e bella,\n",
            "tal che di comandare io la richiesi. \n",
            "\n",
            "Lucevan li occhi suoi più che la stella;\n",
            "e cominciommi a dir soave e piana,\n",
            "con angelica voce, in sua favella: \n",
            "\n",
            "\"O anima cortese mantoana,\n",
            "di cui la fama ancor nel mondo dura,\n",
            "e durerà quanto 'l mondo lontana, \n",
            "\n",
            "l'amico mio, e non de la ventura,\n",
            "ne la diserta piaggia è impedito\n",
            "sì nel cammin, che vòlt'è per paura; \n",
            "\n",
            "e temo che non sia già sì smarrito,\n",
            "ch'io mi sia tardi al soccorso levata,\n",
            "per quel ch'i' ho di lui nel cielo udito. \n",
            "\n",
            "Or movi, e con la tua parola ornata\n",
            "e con ciò c' ha mestieri al suo campare,\n",
            "l'aiuta sì ch'i' ne sia consolata. \n",
            "\n",
            "I' son Beatrice che ti faccio andare;\n",
            "vegno del loco ove tornar disio;\n",
            "amor mi mosse, che mi fa parlare. \n",
            "\n",
            "Quando sarò dinanzi al segnor mio,\n",
            "di te mi loderò sovente a lui\".\n",
            "Tacette allora, e poi comincia' io: \n",
            "\n",
            "\"O donna di virtù sola per cui\n",
            "l'umana spezie eccede ogne contento\n",
            "di quel ciel c' ha minor li cerchi sui, \n",
            "\n",
            "tanto m'aggrada il tuo comandamento,\n",
            "che l'ubidir, se già fosse, m'è tardi;\n",
            "più non t'è uo' ch'aprirmi il tuo talento. \n",
            "\n",
            "Ma dimmi la cagion che non ti guardi\n",
            "de lo scender qua giuso in questo centro\n",
            "de l'ampio loco ove tornar tu ardi\". \n",
            "\n",
            "\"Da che tu vuo' saver cotanto a dentro,\n",
            "dirotti brievemente\", mi rispuose,\n",
            "\"perch'i' non temo di venir qua entro. \n",
            "\n",
            "Temer si dee di sole quelle cose\n",
            "c'hanno potenza di fare altrui male;\n",
            "de l'altre no, chè non son paurose. \n",
            "\n",
            "I' son fatta da Dio, sua mercè, tale,\n",
            "che la vostra miseria non mi tange,\n",
            "nè fiamma d'esto 'ncendio non m'assale. \n",
            "\n",
            "Donna è gentil nel ciel che si compiange\n",
            "di questo 'mpedimento ov'io ti mando,\n",
            "sì che duro giudicio là sù frange. \n",
            "\n",
            "Questa chiese Lucia in suo dimando\n",
            "e disse: - Or ha bisogno il tuo fedele\n",
            "di te, e io a te lo raccomando -. \n",
            "\n",
            "Lucia, nimica di ciascun crudele,\n",
            "si mosse, e venne al loco dov'i' era,\n",
            "che mi sedea con l'antica Rachele. \n",
            "\n",
            "Disse: - Beatrice, loda di Dio vera,\n",
            "chè non soccorri quei che t'amò tanto,\n",
            "ch'uscì per te de la volgare schiera? \n",
            "\n",
            "Non odi tu la pieta del suo pianto,\n",
            "non vedi tu la morte che 'l combatte\n",
            "su la fiumana ove 'l mar non ha vanto? -. \n",
            "\n",
            "Al mondo non fur mai persone ratte\n",
            "a far lor pro o a fuggir lor danno,\n",
            "com'io, dopo cotai parole fatte, \n",
            "\n",
            "venni qua giù del mio beato scanno,\n",
            "fidandomi del tuo parlare onesto,\n",
            "ch'onora te e quei ch'udito l' hanno\". \n",
            "\n",
            "Poscia che m'ebbe ragionato questo,\n",
            "li occhi lucenti lagrimando volse,\n",
            "per che mi fece del venir più presto. \n",
            "\n",
            "E venni a te così com'ella volse:\n",
            "d'inanzi a quella fiera ti levai\n",
            "che del bel monte il corto andar ti tolse. \n",
            "\n",
            "Dunque: che è perchè, perchè restai,\n",
            "perchè tanta viltà nel core allette,\n",
            "perchè ardire e franchezza non hai, \n",
            "\n",
            "poscia che tai tre donne benedette\n",
            "curan di te ne la corte del cielo,\n",
            "e 'l mio parlar tanto ben ti promette?\". \n",
            "\n",
            "Quali fioretti dal notturno gelo\n",
            "chinati e chiusi, poi che 'l sol li 'mbianca,\n",
            "si drizzan tutti aperti in loro stelo, \n",
            "\n",
            "tal mi fec'io di mia virtude stanca,\n",
            "e tanto buono ardire al cor mi corse,\n",
            "ch'i' cominciai come persona franca: \n",
            "\n",
            "\"Oh pietosa colei che mi soccorse!\n",
            "e te cortese ch'ubidisti tosto\n",
            "a le vere parole che ti porse! \n",
            "\n",
            "Tu m' hai con disiderio il cor disposto\n",
            "sì al venir con le parole tue,\n",
            "ch'i' son tornato nel primo proposto. \n",
            "\n",
            "Or va, ch'un sol volere è d'ambedue:\n",
            "tu duca, tu segnore e tu maestro\".\n",
            "Così li dissi; e poi che mosso fue, \n",
            "\n",
            "'Per me si va ne la città dolente,\n",
            "per me si va ne l'etterno dolore,\n",
            "per me si va tra la perduta gente.\n",
            "\n",
            "Giustizia mosse il mio alto fattore;\n",
            "fecemi la divina podestate,\n",
            "la somma sapienza e 'l primo amore. \n",
            "\n",
            "Dinanzi a me non fuor cose create\n",
            "se non etterne, e io etterna duro.\n",
            "Lasciate ogne speranza, voi ch'intrate'. \n",
            "\n",
            "Queste parole di colore oscuro\n",
            "vid'io scritte al sommo d'una porta;\n",
            "per ch'io: \"Maestro, il senso lor m'è duro\". \n",
            "\n",
            "Ed elli a me, come persona accorta:\n",
            "\"Qui si convien lasciare ogne sospetto;\n",
            "ogne viltà convien che qui sia morta. \n",
            "\n",
            "Noi siam venuti al loco ov'i' t' ho detto\n",
            "che tu vedrai le genti dolorose\n",
            "c'hanno perduto il ben de l'intelletto\". \n",
            "\n",
            "E poi che la sua mano a la mia puose\n",
            "con lieto volto, ond'io mi confortai,\n",
            "mi mise dentro a le segrete cose. \n",
            "\n",
            "Quivi sospiri, pianti e alti guai\n",
            "risonavan per l'aere sanza stelle,\n",
            "per ch'io al cominciar ne lagrimai. \n",
            "\n",
            "Diverse lingue, orribili favelle,\n",
            "parole di dolore, accenti d'ira,\n",
            "voci alte e fioche, e suon di man con elle \n",
            "\n",
            "facevano un tumulto, il qual s'aggira\n",
            "sempre in quell'aura sanza tempo tinta,\n",
            "come la rena quando turbo spira. \n",
            "\n",
            "E io ch'avea d'error la testa cinta,\n",
            "dissi: \"Maestro, che è quel ch'i' odo?\n",
            "e che gent'è che par nel duol sì vinta?\". \n",
            "\n",
            "Ed elli a me: \"Questo misero modo\n",
            "tegnon l'anime triste di coloro\n",
            "che visser sanza 'nfamia e sanza lodo. \n",
            "\n",
            "Mischiate sono a quel cattivo coro\n",
            "de li angeli che non furon ribelli\n",
            "nè fur fedeli a Dio, ma per sè fuoro. \n",
            "\n",
            "Caccianli i ciel per non esser men belli,\n",
            "nè lo profondo inferno li riceve,\n",
            "ch'alcuna gloria i rei avrebber d'elli\". \n",
            "\n",
            "E io: \"Maestro, che è tanto greve\n",
            "a lor che lamentar li fa sì forte?\".\n",
            "Rispuose: \"Dicerolti molto breve. \n",
            "\n",
            "Questi non hanno speranza di morte,\n",
            "e la lor cieca vita è tanto bassa,\n",
            "che 'nvidiosi son d'ogne altra sorte. \n",
            "\n",
            "Fama di loro il mondo esser non lassa;\n",
            "misericordia e giustizia li sdegna:\n",
            "non ragioniam di lor, ma guarda e passa\". \n",
            "\n",
            "E io, che riguardai, vidi una 'nsegna\n",
            "che girando correva tanto ratta,\n",
            "che d'ogne posa mi parea indegna; \n",
            "\n",
            "e dietro le venìa sì lunga tratta\n",
            "di gente, ch'i' non averei creduto\n",
            "che morte tanta n'avesse disfatta. \n",
            "\n",
            "Poscia ch'io v'ebbi alcun riconosciuto,\n",
            "vidi e conobbi l'ombra di colui\n",
            "che fece per viltade il gran rifiuto. \n",
            "\n",
            "Incontanente intesi e certo fui\n",
            "che questa era la setta d'i cattivi,\n",
            "a Dio spiacenti e a' nemici sui. \n",
            "\n",
            "Questi sciaurati, che mai non fur vivi,\n",
            "erano ignudi e stimolati molto\n",
            "da mosconi e da vespe ch'eran ivi. \n",
            "\n",
            "Elle rigavan lor di sangue il volto,\n",
            "che, mischiato di lagrime, a' lor piedi\n",
            "da fastidiosi vermi era ricolto. \n",
            "\n",
            "E poi ch'a riguardar oltre mi diedi,\n",
            "vidi genti a la riva d'un gran fiume;\n",
            "per ch'io dissi: \"Maestro, or mi concedi \n",
            "\n",
            "ch'i' sappia quali sono, e qual costume\n",
            "le fa di trapassar parer sì pronte,\n",
            "com'i' discerno per lo fioco lume\". \n",
            "\n",
            "Ed elli a me: \"Le cose ti fier conte\n",
            "quando noi fermerem li nostri passi\n",
            "su la trista riviera d'Acheronte\". \n",
            "\n",
            "Allor con li occhi vergognosi e bassi,\n",
            "temendo no 'l mio dir li fosse grave,\n",
            "infino al fiume del parlar mi trassi. \n",
            "\n",
            "Ed ecco verso noi venir per nave\n",
            "un vecchio, bianco per antico pelo,\n",
            "gridando: \"Guai a voi, anime prave! \n",
            "\n",
            "Non isperate mai veder lo cielo:\n",
            "i' vegno per menarvi a l'altra riva\n",
            "ne le tenebre etterne, in caldo e 'n gelo. \n",
            "\n",
            "E tu che se' costì, anima viva,\n",
            "pàrtiti da cotesti che son morti\".\n",
            "Ma poi che vide ch'io non mi partiva, \n",
            "\n",
            "disse: \"Per altra via, per altri porti\n",
            "verrai a piaggia, non qui, per passare:\n",
            "più lieve legno convien che ti porti\". \n",
            "\n",
            "E 'l duca lui: \"Caron, non ti crucciare:\n",
            "vuolsi così colà dove si puote\n",
            "ciò che si vuole, e più non dimandare\". \n",
            "\n",
            "Quinci fuor quete le lanose gote\n",
            "al nocchier de la livida palude,\n",
            "che 'ntorno a li occhi avea di fiamme rote. \n",
            "\n",
            "Ma quell'anime, ch'eran lasse e nude,\n",
            "cangiar colore e dibattero i denti,\n",
            "ratto che 'nteser le parole crude. \n",
            "\n",
            "Bestemmiavano Dio e lor parenti,\n",
            "l'umana spezie e 'l loco e 'l tempo e 'l seme\n",
            "di lor semenza e di lor nascimenti. \n",
            "\n",
            "Poi si ritrasser tutte quante insieme,\n",
            "forte piangendo, a la riva malvagia\n",
            "ch'attende ciascun uom che Dio non teme. \n",
            "\n",
            "Caron dimonio, con occhi di bragia\n",
            "loro accennando, tutte le raccoglie;\n",
            "batte col remo qualunque s'adagia. \n",
            "\n",
            "Come d'autunno si levan le foglie\n",
            "l'una appresso de l'altra, fin che 'l ramo\n",
            "vede a la terra tutte le sue spoglie, \n",
            "\n",
            "similemente il mal seme d'Adamo\n",
            "gittansi di quel lito ad una ad una,\n",
            "per cenni come augel per suo richiamo. \n",
            "\n",
            "Così sen vanno su per l'onda bruna,\n",
            "e avanti che sien di là discese,\n",
            "anche di qua nuova schiera s'auna. \n",
            "\n",
            "\"Figliuol mio\", disse 'l maestro cortese,\n",
            "\"quelli che muoion ne l'ira di Dio\n",
            "tutti convegnon qui d'ogne paese; \n",
            "\n",
            "e pronti sono a trapassar lo rio,\n",
            "chè la divina giustizia li sprona,\n",
            "sì che la tema si volve in disio. \n",
            "\n",
            "Quinci non passa mai anima \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILphRXIXaYrP",
        "outputId": "d61c2d75-1e50-4e7f-bbe2-162476c1fddd"
      },
      "source": [
        "# Check lenght of text\n",
        "print(len(divina_commedia))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "529957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZw3-Joyhg5l"
      },
      "source": [
        "## Vocabulary and Char2Idx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwvjeLGWAVE4"
      },
      "source": [
        "# Store unique characters into a dict with numerical encoding\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "\n",
        "# Store them in a dict, associated with a numerical index\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkXOJ3LGAVCF",
        "outputId": "078f41b8-47dc-49f5-dce2-00913d3590fe"
      },
      "source": [
        "print(len(char2idx))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sieJxDhAU_V",
        "outputId": "31adbcb4-1e0a-4f26-9aab-3d5881e357bd"
      },
      "source": [
        "char2idx"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " ',': 5,\n",
              " '-': 6,\n",
              " '.': 7,\n",
              " ':': 8,\n",
              " ';': 9,\n",
              " '?': 10,\n",
              " 'A': 11,\n",
              " 'B': 12,\n",
              " 'C': 13,\n",
              " 'D': 14,\n",
              " 'E': 15,\n",
              " 'F': 16,\n",
              " 'G': 17,\n",
              " 'H': 18,\n",
              " 'I': 19,\n",
              " 'L': 20,\n",
              " 'M': 21,\n",
              " 'N': 22,\n",
              " 'O': 23,\n",
              " 'P': 24,\n",
              " 'Q': 25,\n",
              " 'R': 26,\n",
              " 'S': 27,\n",
              " 'T': 28,\n",
              " 'U': 29,\n",
              " 'V': 30,\n",
              " 'Z': 31,\n",
              " 'a': 32,\n",
              " 'b': 33,\n",
              " 'c': 34,\n",
              " 'd': 35,\n",
              " 'e': 36,\n",
              " 'f': 37,\n",
              " 'g': 38,\n",
              " 'h': 39,\n",
              " 'i': 40,\n",
              " 'j': 41,\n",
              " 'l': 42,\n",
              " 'm': 43,\n",
              " 'n': 44,\n",
              " 'o': 45,\n",
              " 'p': 46,\n",
              " 'q': 47,\n",
              " 'r': 48,\n",
              " 's': 49,\n",
              " 't': 50,\n",
              " 'u': 51,\n",
              " 'v': 52,\n",
              " 'x': 53,\n",
              " 'y': 54,\n",
              " 'z': 55,\n",
              " 'È': 56,\n",
              " 'à': 57,\n",
              " 'è': 58,\n",
              " 'ì': 59,\n",
              " 'ò': 60,\n",
              " 'ù': 61}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2H5G86HhyvE"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk_BqFK4AU9H"
      },
      "source": [
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7JpYN9LAU7U",
        "outputId": "065fa66f-64df-4add-80f6-168642a8d5bc"
      },
      "source": [
        "# Let's see what the first line will look like\n",
        "print(\"{}\".format(divina_commedia[276:511]))\n",
        "print(\"\\nbecomes:\")\n",
        "print(numerical_encoding(divina_commedia[276:511], char2idx))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i' vi trovai,\n",
            "dirò de l'altre cose ch'i' v' ho scorte. \n",
            "\n",
            "Io non so ben ridir com'i' v'intrai,\n",
            "tant'era pien di sonno a quel punto\n",
            "che la verace via abbandonai. \n",
            "\n",
            "Ma poi ch'i' fui al piè d'un colle giunto,\n",
            "là dove terminava quella valle\n",
            "\n",
            "becomes:\n",
            "[40  4  1 52 40  1 50 48 45 52 32 40  5  0 35 40 48 60  1 35 36  1 42  4\n",
            " 32 42 50 48 36  1 34 45 49 36  1 34 39  4 40  4  1 52  4  1 39 45  1 49\n",
            " 34 45 48 50 36  7  1  0  0 19 45  1 44 45 44  1 49 45  1 33 36 44  1 48\n",
            " 40 35 40 48  1 34 45 43  4 40  4  1 52  4 40 44 50 48 32 40  5  0 50 32\n",
            " 44 50  4 36 48 32  1 46 40 36 44  1 35 40  1 49 45 44 44 45  1 32  1 47\n",
            " 51 36 42  1 46 51 44 50 45  0 34 39 36  1 42 32  1 52 36 48 32 34 36  1\n",
            " 52 40 32  1 32 33 33 32 44 35 45 44 32 40  7  1  0  0 21 32  1 46 45 40\n",
            "  1 34 39  4 40  4  1 37 51 40  1 32 42  1 46 40 58  1 35  4 51 44  1 34\n",
            " 45 42 42 36  1 38 40 51 44 50 45  5  0 42 57  1 35 45 52 36  1 50 36 48\n",
            " 43 40 44 32 52 32  1 47 51 36 42 42 32  1 52 32 42 42 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqALeTUmnl0X"
      },
      "source": [
        "# Processing Data for DanteRNN\n",
        "\n",
        "We need to generate the input for our RNN, the input sequence and an output sequence needs to be of equal length, in which each character is shifted left of one position.\n",
        "\n",
        "For example, the first verse:\n",
        "\n",
        "> Nel mezzo del cammin di nostra vita\n",
        "\n",
        "would be translated in a train sequence as:\n",
        "\n",
        "`Nel mezzo del cammin di nostra vit`\n",
        "\n",
        "be associated with the target sequence:\n",
        "\n",
        "`el mezzo del cammin di nostra vita`\n",
        "\n",
        "The following function is a preparatory step for that. More generally, given a sequence:\n",
        "\n",
        "```\n",
        "A B C D E F G H I\n",
        "```\n",
        "\n",
        "and assuming input sequences of length 5, it will generate a matrix like:\n",
        "\n",
        "```\n",
        "A B C D E\n",
        "B C D E F\n",
        "C D E F G\n",
        "D E F G H\n",
        "E F G H I\n",
        "```\n",
        "\n",
        "The split between train and target sets will be as:\n",
        "\n",
        "```\n",
        " Train:           Target:\n",
        "                 \n",
        "A B C D E        B C D E F\n",
        "B C D E F        C D E F G\n",
        "C D E F G        D E F G H\n",
        "D E F G H        E F G H I\n",
        "                 \n",
        "```\n",
        "\n",
        "Train and target sets are fundamentally the same matrix, with the train having the last row removed, and the target set having the first removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWcZzOJdG6X9"
      },
      "source": [
        "# Apply it on the whole Comedy\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foyC3ZnGgKLN",
        "outputId": "2509451b-9cc2-4c8e-a2fb-cb233acd1e76"
      },
      "source": [
        "print(encoded_text[311:600])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[34 39  4 40  4  1 52  4  1 39 45  1 49 34 45 48 50 36  7  1  0  0 19 45\n",
            "  1 44 45 44  1 49 45  1 33 36 44  1 48 40 35 40 48  1 34 45 43  4 40  4\n",
            "  1 52  4 40 44 50 48 32 40  5  0 50 32 44 50  4 36 48 32  1 46 40 36 44\n",
            "  1 35 40  1 49 45 44 44 45  1 32  1 47 51 36 42  1 46 51 44 50 45  0 34\n",
            " 39 36  1 42 32  1 52 36 48 32 34 36  1 52 40 32  1 32 33 33 32 44 35 45\n",
            " 44 32 40  7  1  0  0 21 32  1 46 45 40  1 34 39  4 40  4  1 37 51 40  1\n",
            " 32 42  1 46 40 58  1 35  4 51 44  1 34 45 42 42 36  1 38 40 51 44 50 45\n",
            "  5  0 42 57  1 35 45 52 36  1 50 36 48 43 40 44 32 52 32  1 47 51 36 42\n",
            " 42 32  1 52 32 42 42 36  0 34 39 36  1 43  4 32 52 36 32  1 35 40  1 46\n",
            " 32 51 48 32  1 40 42  1 34 45 48  1 34 45 43 46 51 44 50 45  5  1  0  0\n",
            " 38 51 32 48 35 32 40  1 40 44  1 32 42 50 45  1 36  1 52 40 35 40  1 42\n",
            " 36  1 49 51 36  1 49 46 32 42 42 36  0 52 36 49 50 40 50 36  1 38 40 57\n",
            "  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t41gYByxAU4B"
      },
      "source": [
        "def get_text_matrix(sequence, len_input):\n",
        "    \n",
        "    # create empty matrix\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    \n",
        "    # fill each row/time window from input sequence\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "        \n",
        "    return X"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eazAAQiAk0i"
      },
      "source": [
        "len_text = 150\n",
        "text_matrix = get_text_matrix(encoded_text, len_text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KonviQjQAk40",
        "outputId": "7f9ecd79-ab30-43c0-9f62-6c26776c65e2"
      },
      "source": [
        "print(text_matrix.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(529807, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaVngDG7AkyU",
        "outputId": "9fe2ddd3-bda8-4aed-8a0f-fa0a0b2ae183"
      },
      "source": [
        "print(\"100th train sequence:\\n\")\n",
        "print(text_matrix[ 100, : ])\n",
        "print(\"\\n\\n100th target sequence:\\n\")\n",
        "print(text_matrix[ 101, : ])\n",
        "print(\"\\n\\n102th target sequence:\\n\")\n",
        "print(text_matrix[ 102, : ])\n",
        "print(\"\\n\\n115th target sequence:\\n\")\n",
        "print(text_matrix[ 180, : ])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100th train sequence:\n",
            "\n",
            "[32.  7.  1.  0.  0. 11. 39. 40.  1. 47. 51. 32. 44. 50. 45.  1. 32.  1.\n",
            " 35. 40. 48.  1. 47. 51. 32. 42.  1. 36. 48. 32.  1. 58.  1. 34. 45. 49.\n",
            " 32.  1. 35. 51. 48. 32.  0. 36. 49. 50. 32.  1. 49. 36. 42. 52. 32.  1.\n",
            " 49. 36. 42. 52. 32. 38. 38. 40. 32.  1. 36.  1. 32. 49. 46. 48. 32.  1.\n",
            " 36.  1. 37. 45. 48. 50. 36.  0. 34. 39. 36.  1. 44. 36. 42.  1. 46. 36.\n",
            " 44. 49. 40. 36. 48.  1. 48. 40. 44. 45. 52. 32.  1. 42. 32.  1. 46. 32.\n",
            " 51. 48. 32.  2.  1.  0.  0. 28. 32. 44. 50.  4. 58.  1. 32. 43. 32. 48.\n",
            " 32.  1. 34. 39. 36.  1. 46. 45. 34. 45.  1. 58.  1. 46. 40. 61.  1. 43.\n",
            " 45. 48. 50. 36.  9.  0.]\n",
            "\n",
            "\n",
            "100th target sequence:\n",
            "\n",
            "[ 7.  1.  0.  0. 11. 39. 40.  1. 47. 51. 32. 44. 50. 45.  1. 32.  1. 35.\n",
            " 40. 48.  1. 47. 51. 32. 42.  1. 36. 48. 32.  1. 58.  1. 34. 45. 49. 32.\n",
            "  1. 35. 51. 48. 32.  0. 36. 49. 50. 32.  1. 49. 36. 42. 52. 32.  1. 49.\n",
            " 36. 42. 52. 32. 38. 38. 40. 32.  1. 36.  1. 32. 49. 46. 48. 32.  1. 36.\n",
            "  1. 37. 45. 48. 50. 36.  0. 34. 39. 36.  1. 44. 36. 42.  1. 46. 36. 44.\n",
            " 49. 40. 36. 48.  1. 48. 40. 44. 45. 52. 32.  1. 42. 32.  1. 46. 32. 51.\n",
            " 48. 32.  2.  1.  0.  0. 28. 32. 44. 50.  4. 58.  1. 32. 43. 32. 48. 32.\n",
            "  1. 34. 39. 36.  1. 46. 45. 34. 45.  1. 58.  1. 46. 40. 61.  1. 43. 45.\n",
            " 48. 50. 36.  9.  0. 43.]\n",
            "\n",
            "\n",
            "102th target sequence:\n",
            "\n",
            "[ 1.  0.  0. 11. 39. 40.  1. 47. 51. 32. 44. 50. 45.  1. 32.  1. 35. 40.\n",
            " 48.  1. 47. 51. 32. 42.  1. 36. 48. 32.  1. 58.  1. 34. 45. 49. 32.  1.\n",
            " 35. 51. 48. 32.  0. 36. 49. 50. 32.  1. 49. 36. 42. 52. 32.  1. 49. 36.\n",
            " 42. 52. 32. 38. 38. 40. 32.  1. 36.  1. 32. 49. 46. 48. 32.  1. 36.  1.\n",
            " 37. 45. 48. 50. 36.  0. 34. 39. 36.  1. 44. 36. 42.  1. 46. 36. 44. 49.\n",
            " 40. 36. 48.  1. 48. 40. 44. 45. 52. 32.  1. 42. 32.  1. 46. 32. 51. 48.\n",
            " 32.  2.  1.  0.  0. 28. 32. 44. 50.  4. 58.  1. 32. 43. 32. 48. 32.  1.\n",
            " 34. 39. 36.  1. 46. 45. 34. 45.  1. 58.  1. 46. 40. 61.  1. 43. 45. 48.\n",
            " 50. 36.  9.  0. 43. 32.]\n",
            "\n",
            "\n",
            "115th target sequence:\n",
            "\n",
            "[34. 39. 36.  1. 44. 36. 42.  1. 46. 36. 44. 49. 40. 36. 48.  1. 48. 40.\n",
            " 44. 45. 52. 32.  1. 42. 32.  1. 46. 32. 51. 48. 32.  2.  1.  0.  0. 28.\n",
            " 32. 44. 50.  4. 58.  1. 32. 43. 32. 48. 32.  1. 34. 39. 36.  1. 46. 45.\n",
            " 34. 45.  1. 58.  1. 46. 40. 61.  1. 43. 45. 48. 50. 36.  9.  0. 43. 32.\n",
            "  1. 46. 36. 48.  1. 50. 48. 32. 50. 50. 32. 48.  1. 35. 36. 42.  1. 33.\n",
            " 36. 44.  1. 34. 39.  4. 40.  4.  1. 52. 40.  1. 50. 48. 45. 52. 32. 40.\n",
            "  5.  0. 35. 40. 48. 60.  1. 35. 36.  1. 42.  4. 32. 42. 50. 48. 36.  1.\n",
            " 34. 45. 49. 36.  1. 34. 39.  4. 40.  4.  1. 52.  4.  1. 39. 45.  1. 49.\n",
            " 34. 45. 48. 50. 36.  7.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTb_xjIHFMQg"
      },
      "source": [
        "# Custom Loss\n",
        "Evaluate the structure of the rhymes, based on the real scheme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUfoUSUSXjl5"
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "def divide_versi(y):\n",
        "  doppiozero = False\n",
        "\n",
        "  y_divided = [[]]\n",
        "  for ly in y:\n",
        "    ly = int(ly)\n",
        "\n",
        "    # I have to clean the list of punctuation marks,\n",
        "    # in chartoidx means the numbers 1 to 10 inclusive.\n",
        "    if ly in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
        "        continue\n",
        "    else:\n",
        "      # if it is zero it means \\ n so I add a new line\n",
        "      if ly is 0:\n",
        "        if not doppiozero:\n",
        "          y_divided.append([])\n",
        "        doppiozero = True\n",
        "        continue\n",
        "\n",
        "      y_divided[-1].append(ly)\n",
        "      doppiozero = False\n",
        "\n",
        "  if y_divided is not []:\n",
        "    if y[-1] != 0:\n",
        "      # since the last line does not end with 0 it means that it is incomplete and I remove it\n",
        "      y_divided.pop()\n",
        "\n",
        "  # i need to re check because maybe i pop the only one\n",
        "  if len(y_divided) != 0:\n",
        "    if len(y_divided[0]) < 3:\n",
        "      # if the first line is less than 4 I can't do anything about it so I delete it\n",
        "      y_divided.pop(0)\n",
        "\n",
        "  return y_divided\n",
        "\n",
        "def rhymes_extractor(y_divided):\n",
        "  # I extract the rhyme scheme from y\n",
        "  rhymes = []\n",
        "  for i in range(len(y_divided)):\n",
        "    # with the end of the line (last two letters) I check if the other lines\n",
        "    # end with the same letters\n",
        "    vy = y_divided[i]\n",
        "\n",
        "    last_word_1 = vy[-2:]\n",
        "\n",
        "    # ABA BCB CDC\n",
        "\n",
        "    # I have to check if line i rhymes with line i + 2\n",
        "    if i+2 < len(y_divided):\n",
        "      next_vy = y_divided[i+2]\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+2))\n",
        "    \n",
        "    if i+4 < len(y_divided):\n",
        "      next_vy = y_divided[i+4]\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+4))\n",
        "\n",
        "  return rhymes\n",
        "\n",
        "\n",
        "def get_custom_loss(x_batch, y_batch):\n",
        "  summed_custom_loss = 0\n",
        "\n",
        "  # max number of rhymes (arbitrary choosen, it's an hyperparameter)\n",
        "  max_rhymes = 4\n",
        "\n",
        "  x_bin_tot = np.ones(shape=(len(x_batch), max_rhymes), dtype='float32')\n",
        "  y_bin_tot = np.ones(shape=(len(x_batch), max_rhymes), dtype='float32')\n",
        "\n",
        "  # iterate over each vector\n",
        "  for v in range(len(x_batch)):\n",
        "    x = x_batch[v]\n",
        "    y = y_batch[v]\n",
        "\n",
        "    # given that the model returns a matrix with shape (len_text, vocab_size) with the probability\n",
        "    # for each of the vocab_size character i need to use a categorical to choose the best\n",
        "    # then flatten the matrix into a list for evaluating\n",
        "    predicted_text = list(tf.random.categorical(x, num_samples=1).numpy())\n",
        "    x = np.concatenate(predicted_text).ravel().tolist()\n",
        "\n",
        "    # dividing the vector in verse\n",
        "    x_divided = divide_versi(x)\n",
        "    y_divided = divide_versi(y)\n",
        "\n",
        "    # extract the structure of the rhymes from generated and groud truth\n",
        "    x_rhymes = rhymes_extractor(x_divided)\n",
        "    y_rhymes = rhymes_extractor(y_divided)\n",
        "\n",
        "    # it returns me a list with the number of rhyming lines\n",
        "    # Example: [(1,3), (2,4)] means that lines 1 and 3 rhyme and that the\n",
        "    # lines 2 and 4 as well\n",
        "\n",
        "    # I create a vector of 1 for y because the rhymes are always there\n",
        "    y_bin = np.ones(max_rhymes, dtype='float32')\n",
        "    # I create a vector of 1 for the rhymes generated, I will put 0 if it rhyme\n",
        "    # Is NOT present in dante, discount with a 0.5 since there is at least the rhyme\n",
        "    x_bin = np.ones(max_rhymes, dtype='float32')\n",
        "\n",
        "    if x_rhymes == []:\n",
        "      x_bin = np.zeros(max_rhymes, dtype='float32')\n",
        "\n",
        "    # if the generated rhyme is in Dante's original rhymes then I sign it as valid\n",
        "    # I keep maximum max_ryhmes rhymes: I can because in 150-200 characters I don't have more than 5-6 lines\n",
        "    # so in Dante I would have 2 rhymes, I exceed 2 to help the network create even wrong rhymes\n",
        "    for i in range(max_rhymes+1):\n",
        "      if i < len(y_rhymes):\n",
        "        # check dante's rhyme with predicted rhymes, if it not exist set 0.0\n",
        "        if y_rhymes[i] not in x_rhymes:\n",
        "          x_bin[i] = 0.0\n",
        "        # check predicted rhyme with Dante's rhymes, if not exist set 0.5 to increase number of rhymes produced\n",
        "        if i < len(x_rhymes) and x_rhymes[i] not in y_rhymes:\n",
        "          x_bin[i] = 0.5\n",
        "\n",
        "    # concatenate vectors with rhyming encoding\n",
        "    x_bin_tot[v] = x_bin\n",
        "    y_bin_tot[v] = y_bin\n",
        "  \n",
        "  # MSE over vector\n",
        "  r = tf.keras.losses.mean_squared_error(y_bin_tot, x_bin_tot)\n",
        "\n",
        "  return np.mean(r)\n",
        "\n",
        "# NEW VERSION\n",
        "# creo un vettore con le rime di y reale e di y generato\n",
        "# Ex: in y reale se ho ABABC il vettore è [1,2,1,2,3] con o zero ad indicare nulla\n",
        "# per y generato devo creare un vettore di lunghezza uguale per poi valutarlo con una sparse_crossentropy\n",
        "# problema: non avrà mai le stesse righe\n",
        "\n",
        "# extract matrix of index of where the zeros are\n",
        "#tf.map_fn(fn=lambda t: t.map_fn(fn=lambda x: 1 if x == 0 else 0), elems=x_batch)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPjMSZOzh_60"
      },
      "source": [
        "# Training Model\n",
        "\n",
        "At this point, I can specify the RNN architecture with all its hyperparameters. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kiExaZj-iG1"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFZfbimYAkvk"
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "# size of mini batches during training\n",
        "batch_size = 128  # 100\n",
        "\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "\n",
        "# vector size of char embeddings\n",
        "embedding_size = 200  # 200 250\n",
        "\n",
        "lstm_unit_1 = 2048\n",
        "lstm_unit_2 = 4096\n",
        "\n",
        "# debug variables\n",
        "debug_model = False\n",
        "if debug_model:\n",
        "  lstm_unit_1 = 256\n",
        "  lstm_unit_2 = 512\n",
        "\n",
        "\n",
        "hidden_size = 256  # for Dense() layers 250\n",
        "\n",
        "n_epochs = 58\n",
        "learning_rate = 0.001  # 0.0001"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-LOqWfs06v3"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brY8sqvsD9h9"
      },
      "source": [
        "def perplexity(labels, logits):\n",
        "    \"\"\"Calculates perplexity metric = 2^(entropy) or e^(entropy)\"\"\"\n",
        "    return pow(2, loss(y_true=labels, y_pred=logits))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHDktX8rF2Iz"
      },
      "source": [
        "## Custom learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "3yRTdMjQCvIJ",
        "outputId": "aac96845-8f96-421d-c0f9-6fea72eec5e2"
      },
      "source": [
        "#@title\n",
        "#learning_rate_tot = []\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=10):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step ** 1.5)\n",
        "    arg2 = step * ((self.warmup_steps+10) ** -1.3)\n",
        "    lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    print(step)\n",
        "\n",
        "    return lr\n",
        "\n",
        "d_model = 500\n",
        "learning_rate_custom_1 = CustomSchedule(d_model)\n",
        "plt.plot(learning_rate_custom_1(tf.range(50, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")\n",
        "\n",
        "\n",
        "learning_rate_custom_2 = tf.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=5,\n",
        "    decay_rate=0.80,\n",
        "    staircase=True)\n",
        "plt.plot(learning_rate_custom_2(tf.range(50, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.], shape=(50,), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IS2bRMqCjhb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ng05biTbx6U"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate)  # Adamax"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r22pA00o-fb7"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbtlhhW4bPyE",
        "outputId": "8420c7bd-a6a6-45a1-8120-77b72bd15d0a"
      },
      "source": [
        "# Input Layer\n",
        "X = Input(shape=(None, ), batch_size=batch_size)\n",
        "\n",
        "# Embedding Layer\n",
        "embedded = Embedding(vocab_size, embedding_size, \n",
        "                     batch_input_shape=(batch_size, None), \n",
        "                     embeddings_regularizer=tf.keras.regularizers.L2()\n",
        "                     )(X)\n",
        "\n",
        "# Dense layer\n",
        "embedded = Dense(embedding_size, relu)(embedded)\n",
        "\n",
        "# First LSTM\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_1,return_sequences=True,return_state=True)(embedded)\n",
        "encoder_output = BatchNormalization()(encoder_output)\n",
        "\n",
        "# Dropout\n",
        "encoder_output = Dropout(0.5)(encoder_output)\n",
        "# Dense layer\n",
        "encoder_output = Dense(embedding_size, activation='relu')(encoder_output)\n",
        "\n",
        "# Dropout\n",
        "encoder_output = Dropout(0.5)(encoder_output)\n",
        "\n",
        "# Concat of first LSTM hidden state\n",
        "initial_state_double = [tf.concat([hidden_state, hidden_state], 1), tf.concat([hidden_state, hidden_state], 1)]\n",
        "\n",
        "# Second LSTM\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_2,return_sequences=True, return_state=True)(encoder_output, initial_state=initial_state_double) \n",
        "encoder_output = BatchNormalization()(encoder_output)\n",
        "\n",
        "# Dropout\n",
        "encoder_output = Dropout(0.5)(encoder_output)\n",
        "# Dense layer\n",
        "encoder_output = Dense(hidden_size, activation='relu')(encoder_output)\n",
        "\n",
        "# Dropout\n",
        "encoder_output = Dropout(0.5)(encoder_output)\n",
        "\n",
        "# Prediction Layer\n",
        "Y = Dense(units=vocab_size)(encoder_output)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=X, outputs=Y)\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer=optimizer, metrics=[perplexity, sparse_categorical_crossentropy])\n",
        "print(model.summary())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(128, None)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (128, None, 200)     12400       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (128, None, 200)     40200       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(128, None, 2048),  18423808    dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (128, None, 2048)    8192        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (128, None, 2048)    0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (128, None, 200)     409800      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (128, None, 200)     0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat (TFOpLambda)          (None, 4096)         0           lstm[0][1]                       \n",
            "                                                                 lstm[0][1]                       \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_1 (TFOpLambda)        (None, 4096)         0           lstm[0][1]                       \n",
            "                                                                 lstm[0][1]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(128, None, 4096),  70402048    dropout_1[0][0]                  \n",
            "                                                                 tf.concat[0][0]                  \n",
            "                                                                 tf.concat_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (128, None, 4096)    16384       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (128, None, 4096)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (128, None, 256)     1048832     dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (128, None, 256)     0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (128, None, 62)      15934       dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 90,377,598\n",
            "Trainable params: 90,365,310\n",
            "Non-trainable params: 12,288\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPcE1ZuK-mkS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK1OwjD1IDD_",
        "outputId": "f44eb07d-1563-4c25-834a-d82af3f27ae5"
      },
      "source": [
        "min_custom_loss = 1.0\n",
        "\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # returns a tensor with shape (batch_size, len_text)\n",
        "        x_predicted = model(x)\n",
        "\n",
        "        scce = tf.keras.losses.sparse_categorical_crossentropy(y, x_predicted, from_logits = True)\n",
        "        # we cant return a tensor with that shape so we return a float that are summed\n",
        "        custom = get_custom_loss(x_predicted, y)\n",
        "\n",
        "        current_loss = tf.reduce_mean(scce + custom)\n",
        "\n",
        "    gradients = tape.gradient(current_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # checking for the best model using custom loss\n",
        "    if custom_loss < min_custom_loss:\n",
        "      min_custom_loss = custom_loss\n",
        "      model.save(\"best_model.h5\", overwrite=True)\n",
        "    return current_loss, scce, custom\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "custom_loss_history = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "\n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss, scce, custom = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "        custom_loss_history.append(custom)\n",
        "    \n",
        "    print(\"{}.  \\t  Total-Loss: {}  \\t  Custom-Loss: {}  \\t Time: {} sec/epoch\".format(\n",
        "        epoch+1, current_loss.numpy(),custom, round(time.time()-start, 2)))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.  \t  Total-Loss: 3.288701295852661  \t  Custom-Loss: 1.0  \t Time: 325.33 sec/epoch\n",
            "2.  \t  Total-Loss: 3.1027631759643555  \t  Custom-Loss: 0.97900390625  \t Time: 326.52 sec/epoch\n",
            "3.  \t  Total-Loss: 3.008928060531616  \t  Custom-Loss: 0.98486328125  \t Time: 326.37 sec/epoch\n",
            "4.  \t  Total-Loss: 2.876944065093994  \t  Custom-Loss: 0.9873046875  \t Time: 326.79 sec/epoch\n",
            "5.  \t  Total-Loss: 2.7901909351348877  \t  Custom-Loss: 0.9775390625  \t Time: 326.59 sec/epoch\n",
            "6.  \t  Total-Loss: 2.7348694801330566  \t  Custom-Loss: 0.9921875  \t Time: 326.89 sec/epoch\n",
            "7.  \t  Total-Loss: 2.6651716232299805  \t  Custom-Loss: 0.98046875  \t Time: 326.54 sec/epoch\n",
            "8.  \t  Total-Loss: 2.5798499584198  \t  Custom-Loss: 0.9697265625  \t Time: 326.55 sec/epoch\n",
            "9.  \t  Total-Loss: 2.5042731761932373  \t  Custom-Loss: 0.96875  \t Time: 326.72 sec/epoch\n",
            "10.  \t  Total-Loss: 2.4652204513549805  \t  Custom-Loss: 0.95654296875  \t Time: 326.7 sec/epoch\n",
            "11.  \t  Total-Loss: 2.429509401321411  \t  Custom-Loss: 0.96435546875  \t Time: 326.7 sec/epoch\n",
            "12.  \t  Total-Loss: 2.3589022159576416  \t  Custom-Loss: 0.9501953125  \t Time: 326.82 sec/epoch\n",
            "13.  \t  Total-Loss: 2.26596999168396  \t  Custom-Loss: 0.9091796875  \t Time: 326.94 sec/epoch\n",
            "14.  \t  Total-Loss: 2.2946221828460693  \t  Custom-Loss: 0.95751953125  \t Time: 326.86 sec/epoch\n",
            "15.  \t  Total-Loss: 2.1887094974517822  \t  Custom-Loss: 0.90869140625  \t Time: 326.77 sec/epoch\n",
            "16.  \t  Total-Loss: 2.1425817012786865  \t  Custom-Loss: 0.9150390625  \t Time: 327.14 sec/epoch\n",
            "17.  \t  Total-Loss: 2.128709316253662  \t  Custom-Loss: 0.93212890625  \t Time: 327.48 sec/epoch\n",
            "18.  \t  Total-Loss: 2.048099994659424  \t  Custom-Loss: 0.90771484375  \t Time: 326.48 sec/epoch\n",
            "19.  \t  Total-Loss: 1.950701117515564  \t  Custom-Loss: 0.8740234375  \t Time: 327.03 sec/epoch\n",
            "20.  \t  Total-Loss: 1.8877441883087158  \t  Custom-Loss: 0.87451171875  \t Time: 327.02 sec/epoch\n",
            "21.  \t  Total-Loss: 1.807325839996338  \t  Custom-Loss: 0.8447265625  \t Time: 326.75 sec/epoch\n",
            "22.  \t  Total-Loss: 1.7100675106048584  \t  Custom-Loss: 0.7939453125  \t Time: 327.11 sec/epoch\n",
            "23.  \t  Total-Loss: 1.6200239658355713  \t  Custom-Loss: 0.77099609375  \t Time: 326.78 sec/epoch\n",
            "24.  \t  Total-Loss: 1.4800968170166016  \t  Custom-Loss: 0.7119140625  \t Time: 327.07 sec/epoch\n",
            "25.  \t  Total-Loss: 1.4758635759353638  \t  Custom-Loss: 0.7734375  \t Time: 326.87 sec/epoch\n",
            "26.  \t  Total-Loss: 1.2749842405319214  \t  Custom-Loss: 0.66015625  \t Time: 326.72 sec/epoch\n",
            "27.  \t  Total-Loss: 1.2129164934158325  \t  Custom-Loss: 0.66943359375  \t Time: 327.18 sec/epoch\n",
            "28.  \t  Total-Loss: 1.0698984861373901  \t  Custom-Loss: 0.58154296875  \t Time: 327.26 sec/epoch\n",
            "29.  \t  Total-Loss: 0.921798586845398  \t  Custom-Loss: 0.51708984375  \t Time: 327.26 sec/epoch\n",
            "30.  \t  Total-Loss: 0.9649533033370972  \t  Custom-Loss: 0.5908203125  \t Time: 327.08 sec/epoch\n",
            "31.  \t  Total-Loss: 0.8449962139129639  \t  Custom-Loss: 0.51513671875  \t Time: 327.09 sec/epoch\n",
            "32.  \t  Total-Loss: 0.8092749118804932  \t  Custom-Loss: 0.5126953125  \t Time: 327.3 sec/epoch\n",
            "33.  \t  Total-Loss: 0.6759042143821716  \t  Custom-Loss: 0.4296875  \t Time: 326.82 sec/epoch\n",
            "34.  \t  Total-Loss: 0.5952429175376892  \t  Custom-Loss: 0.3671875  \t Time: 326.95 sec/epoch\n",
            "35.  \t  Total-Loss: 0.5896492600440979  \t  Custom-Loss: 0.38818359375  \t Time: 326.54 sec/epoch\n",
            "36.  \t  Total-Loss: 0.5651531219482422  \t  Custom-Loss: 0.3701171875  \t Time: 326.66 sec/epoch\n",
            "37.  \t  Total-Loss: 0.45183447003364563  \t  Custom-Loss: 0.26953125  \t Time: 326.7 sec/epoch\n",
            "38.  \t  Total-Loss: 0.4793596863746643  \t  Custom-Loss: 0.2998046875  \t Time: 326.64 sec/epoch\n",
            "39.  \t  Total-Loss: 0.3846549391746521  \t  Custom-Loss: 0.2177734375  \t Time: 326.48 sec/epoch\n",
            "40.  \t  Total-Loss: 0.42562493681907654  \t  Custom-Loss: 0.265625  \t Time: 326.14 sec/epoch\n",
            "41.  \t  Total-Loss: 0.47121378779411316  \t  Custom-Loss: 0.31591796875  \t Time: 326.46 sec/epoch\n",
            "42.  \t  Total-Loss: 0.42826297879219055  \t  Custom-Loss: 0.2783203125  \t Time: 326.0 sec/epoch\n",
            "43.  \t  Total-Loss: 0.38785043358802795  \t  Custom-Loss: 0.2353515625  \t Time: 325.64 sec/epoch\n",
            "44.  \t  Total-Loss: 0.41208145022392273  \t  Custom-Loss: 0.271484375  \t Time: 326.44 sec/epoch\n",
            "45.  \t  Total-Loss: 0.400811105966568  \t  Custom-Loss: 0.2587890625  \t Time: 326.32 sec/epoch\n",
            "46.  \t  Total-Loss: 0.42523327469825745  \t  Custom-Loss: 0.28515625  \t Time: 326.18 sec/epoch\n",
            "47.  \t  Total-Loss: 0.3356218934059143  \t  Custom-Loss: 0.20166015625  \t Time: 326.09 sec/epoch\n",
            "48.  \t  Total-Loss: 0.342660516500473  \t  Custom-Loss: 0.20166015625  \t Time: 326.18 sec/epoch\n",
            "49.  \t  Total-Loss: 0.35357579588890076  \t  Custom-Loss: 0.2158203125  \t Time: 325.54 sec/epoch\n",
            "50.  \t  Total-Loss: 0.3954502046108246  \t  Custom-Loss: 0.2646484375  \t Time: 325.82 sec/epoch\n",
            "51.  \t  Total-Loss: 0.3940888047218323  \t  Custom-Loss: 0.255859375  \t Time: 326.0 sec/epoch\n",
            "52.  \t  Total-Loss: 0.30562418699264526  \t  Custom-Loss: 0.17529296875  \t Time: 326.27 sec/epoch\n",
            "53.  \t  Total-Loss: 0.3833795189857483  \t  Custom-Loss: 0.2529296875  \t Time: 325.81 sec/epoch\n",
            "54.  \t  Total-Loss: 0.4273799657821655  \t  Custom-Loss: 0.296875  \t Time: 325.94 sec/epoch\n",
            "55.  \t  Total-Loss: 0.3536514937877655  \t  Custom-Loss: 0.23193359375  \t Time: 326.02 sec/epoch\n",
            "56.  \t  Total-Loss: 0.3545386791229248  \t  Custom-Loss: 0.23095703125  \t Time: 325.88 sec/epoch\n",
            "57.  \t  Total-Loss: 0.2855149209499359  \t  Custom-Loss: 0.15625  \t Time: 325.62 sec/epoch\n",
            "58.  \t  Total-Loss: 0.3163108229637146  \t  Custom-Loss: 0.197265625  \t Time: 326.28 sec/epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWovlkWZ_28a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f4307fa-611c-4b34-d556-89db72e4798f"
      },
      "source": [
        "model.save(\"deep_comedy_custom_loss_01_62char.h5\")\n",
        "from google.colab import files\n",
        "files.download('deep_comedy_custom_loss_01_62char.h5') "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d2c6b988-a554-45ef-8fe8-a4c04a5f77f8\", \"deep_comedy_custom_loss_01_62char.h5\", 1084528168)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CS75-X9-TBd"
      },
      "source": [
        "## Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qArT0IfJ2M3M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f61d72ab-9c2b-45b3-f4b3-d07671513933"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Iterations')\n",
        "ax1.set_ylabel('Total Loss', color=color)\n",
        "ax1.plot(loss_history, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Custom Loss', color=color)  # we already handled the x-label with ax1\n",
        "ax2.plot(custom_loss_history, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6uAhmV2Atth",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2c5ef590-d85d-4b1a-e657-537b31d9a5d0"
      },
      "source": [
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Total Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAnothsm80Fj"
      },
      "source": [
        "## Saved model\n",
        "file name, len_matrix, batch_size, embedding_size, lstm_1_unit, lstm_2_unit, hidden_unit, epochs, learning_rate\n",
        "\n",
        "model_custom_loss_01.h5: 150,100, 200, 2048,4096,300,50,0.001 BASE MODEL\n",
        "\n",
        "model_custom_loss_02.h5: 150,**150**, 200, 2048,4096,300,50,0.001\n",
        "\n",
        "model_custom_loss_03.h5: 150,100, 200, 2048,4096,300,50,**custom_exponential**\n",
        "\n",
        "model_custom_loss_04.h5: 150,**150**, 200, 2048,4096,300,50,**custom_exponential**\n",
        "\n",
        "model_custom_loss_05.h5: **200**,150, 200, 2048,4096,300,50,0.001\n",
        "\n",
        "model_custom_loss_06.h5: **100**,150, 200, 2048,4096,300,50,0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrMdonRckAWB"
      },
      "source": [
        "# Generative Model\n",
        "\n",
        "At this point, let's check how the model generates text. In order to do it, I must make some changes to my RNN architecture above.\n",
        "\n",
        "First, I must change the fixed batch size. After training, I want to feed just one sentence into my Network to make it continue the character sequence. I will feed a string into the model, make it predict the next character, update the input sequence, and repeat the process until a long generated text is obtained. Because of this, the succession of input sequences is now different from training session, in which portions of text were sampled randomly. I now have to set `stateufl = True` in the `LSTM()` layer, so that each LSTM cell will keep in memory the internal state from the previous sequence. With this I hope the model will better remember sequential information while generating text.\n",
        "\n",
        "I will instantiate a new `generator` RNN with these new features, and transfer the trained weights of my `RNN` into it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcggT7ZY14NS"
      },
      "source": [
        "## Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyS28-ZAEcxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c979e629-27cf-4712-8f07-7a39a2c3b794"
      },
      "source": [
        "# Input Layer\n",
        "X = Input(shape=(None, ), batch_size=1)\n",
        "embedded = Embedding(vocab_size, embedding_size)(X)\n",
        "embedded = Dense(embedding_size, relu)(embedded)\n",
        "\n",
        "#lnLSTMCell = tfa.rnn.LayerNormLSTMCell(units=lstm_unit_1)\n",
        "#encoder_output, hidden_state, cell_state = tf.keras.layers.RNN(lnLSTMCell,\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_1,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True,\n",
        "                                              stateful=True)(embedded)\n",
        "encoder_output = BatchNormalization()(encoder_output)\n",
        "encoder_output = Dropout(0.3)(encoder_output)\n",
        "encoder_output = Dense(embedding_size, activation='relu')(encoder_output)\n",
        "\n",
        "initial_state_double = [tf.concat([hidden_state, hidden_state], 1), tf.concat([hidden_state, hidden_state], 1)]\n",
        "\n",
        "#lnLSTMCell2 = tfa.rnn.LayerNormLSTMCell(units=lstm_unit_2)\n",
        "#encoder_output, hidden_state, cell_state = tf.keras.layers.RNN(lnLSTMCell2,\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_2,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True,\n",
        "                                                stateful=True)(encoder_output, initial_state=initial_state_double)\n",
        "\n",
        "encoder_output = BatchNormalization()(encoder_output)\n",
        "encoder_output = Dropout(0.3)(encoder_output)\n",
        "encoder_output = Dense(hidden_size, activation='relu')(encoder_output)\n",
        "\n",
        "Y = Dense(units=vocab_size)(encoder_output)\n",
        "\n",
        "# Compile model\n",
        "generator = Model(inputs=X, outputs=Y)\n",
        "generator.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer='adam', metrics=[perplexity, sparse_categorical_crossentropy])\n",
        "print(generator.summary())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(1, None)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (1, None, 200)       12400       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (1, None, 200)       40200       embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(1, None, 2048), (1 18423808    dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (1, None, 2048)      8192        lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (1, None, 2048)      0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (1, None, 200)       409800      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_4 (TFOpLambda)        (1, 4096)            0           lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][1]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_5 (TFOpLambda)        (1, 4096)            0           lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][1]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(1, None, 4096), (1 70402048    dense_10[0][0]                   \n",
            "                                                                 tf.concat_4[0][0]                \n",
            "                                                                 tf.concat_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (1, None, 4096)      16384       lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (1, None, 4096)      0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (1, None, 256)       1048832     dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (1, None, 62)        15934       dense_11[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 90,377,598\n",
            "Trainable params: 90,365,310\n",
            "Non-trainable params: 12,288\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jozUIEF19Qn"
      },
      "source": [
        "## Loading weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whB1azhVAtrp"
      },
      "source": [
        "# Import trained weights from RNN to generator\n",
        "load_file = False\n",
        "if load_file:\n",
        "  generator.load_weights(\"deep_comedy_final_1_batch.h5\")\n",
        "else:\n",
        "  generator.set_weights(model.get_weights())"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC8i-OMS2BEX"
      },
      "source": [
        "## Generating Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2hYSqAAtkn"
      },
      "source": [
        "def generate_text(start_string, model, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    predicted_ids = []\n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    model.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)  # one letter input\n",
        "        \n",
        "        # build the input for the next iteration, based on the last 5 characters generated\n",
        "        # become like a poetry!\n",
        "        #predicted_ids.append(predicted_id)\n",
        "        #if len(predicted_ids) > 5:\n",
        "        #  predicted_ids = predicted_ids[1:]\n",
        "        #input_eval = tf.expand_dims(predicted_ids, 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDzwpt1L2IAQ"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_NWeo1fAtiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d293ae00-4de0-45d1-9066-eb814622f490"
      },
      "source": [
        "# Let's feed the first lines:\n",
        "start_string = \"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for t in [0.1, 0.2, 0.3, 0.5, 1.0]:\n",
        "    print(\"####### TEXT GENERATION - temperature = {}\\n\".format(t))\n",
        "    print(generate_text(start_string, generator, num_generate = 1000, temperature = t))\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####### TEXT GENERATION - temperature = 0.1\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Colui che la cagion di che tu miri\",\n",
            "diss'io, \"beato spirto, sì che nulla\n",
            "voglia di sè a te puot' esser fuia. \n",
            "\n",
            "Dunque la voce tua, che 'l ciel trastulla\n",
            "sempre col canto di quei fuochi pii\n",
            "che di sei ali facen la coculla, \n",
            "\n",
            "perchè non satisface a' miei disii?\n",
            "Già non attendere' io ti maravigli,\n",
            "antico spirto, del rider ch'io fei;\n",
            "ma più d'ammirazion vo' che ti pigli. \n",
            "\n",
            "Questi che guida in alto li occhi miei,\n",
            "è quel Virgilio dal qual tu togliesti\n",
            "forte a cantar de li uomini e d'i dèi. \n",
            "\n",
            "Se cagion altra al mio rider credesti,\n",
            "lasciala per non vera, ed esser credi\n",
            "quelle parole che di lui dicesti\". \n",
            "\n",
            "Già s'inchinava ad abbracciar li piedi\n",
            "al mio dottor, ma el li disse: \"Frate,\n",
            "non far, chè tu se' ombra e ombra vedi\". \n",
            "\n",
            "Ed ei surgendo: \"Or puoi la quantitate\n",
            "comprender de l'amor ch'a te mi scalda,\n",
            "quand'io dismento nostra vanitate, \n",
            "\n",
            "Già era l'angel dietro a noi rimaso,\n",
            "l'angel che n'avea vòlti al sesto giro,\n",
            "avendomi dal viso un colpo raso;\n",
            "\n",
            "e quei c' hanno a giustizia lor disiro\n",
            "detto n\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 0.2\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Con l'Altra volta per suo smarrito\n",
            "milse costui di padre nostro, e forse\n",
            "per fuggir lui lasciò qui loco vòto\". \n",
            "\n",
            "E io a lui: \"Con piangere e con lutto,\n",
            "spirito maladetto, ti rimani;\n",
            "ch'i' ti conosco, ancor sie lordo tutto\". \n",
            "\n",
            "Allor distese al legno ambo le mani;\n",
            "per che 'l maestro accorto lo sospinse,\n",
            "dicendo: \"Via costà con li altri cani!\". \n",
            "\n",
            "Lo collo poi con le braccia mi cinse;\n",
            "basciommi 'l volto e disse: \"Alma sdegnosa,\n",
            "benedetta colei che 'l trapasser ch'esco tene. \n",
            "\n",
            "Lo duca mio discese ne la barca,\n",
            "e poi mi fece intrare appresso lui,\n",
            "e sol quand'io fui dentro parve carca. \n",
            "\n",
            "Tosto che 'l duca e io nel legno fui,\n",
            "segando se ne va l'antica prora\n",
            "de l'acqua più che non suol con altrui. \n",
            "\n",
            "Mentre noi corravam la morta gora,\n",
            "dinanzi mi si fece un pien di fango,\n",
            "e disse: \"Chi se' tu che visita dire:\n",
            "questa giunti, del te che fu le note? \n",
            "\n",
            "\"Chi fu colui da cui sala notte a la cammina\n",
            "di Val di Magra o di parte vicina\n",
            "sai, dillo a me, che già grande là era. \n",
            "\n",
            "Fui chiamato Currado Malaspina\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 0.3\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Colui che la carni e l'altra festa\n",
            "principa ogne baldezza e le reni,\n",
            "non guorda, non perchè morta la selvaggia; \n",
            "\n",
            "chè quella voglia a li alberi ci mena\n",
            "che menò Cristo lieto a dire 'Elì',\n",
            "quando ne liberò con la sua vena\". \n",
            "\n",
            "E io a lui: \"Forese, da quel dì\n",
            "nel qual mutasti mondo a miglior vita,\n",
            "cinqu' anni non son vòlti infino a qui. \n",
            "\n",
            "Se prima fu la possa in te finita\n",
            "di peccar più, che sovvenisse l'ora\n",
            "del buon dolor ch'a Dio ne rimarita, \n",
            "\n",
            "come se' tu qua sù venuto ancora?\n",
            "Io ti credea trovar là giù di sotto,\n",
            "dove tempo per tempo si ristora\". \n",
            "\n",
            "Ond'elli a me: \"Sì tosto m' ha condotto\n",
            "a ber lo dolce assenzo d'i martìri\n",
            "la Nella mia con suo pianger dirotto. \n",
            "\n",
            "Con suoi prieghi devoti e con sospiri\n",
            "tratto m' ha de la costa ove s'aspetta,\n",
            "e liberato m' ha de li altri giri. \n",
            "\n",
            "Tanto è a Dio più cara e più diletta\n",
            "la vedovella mia, che molto amai,\n",
            "quanto in bene operare è più soletta; \n",
            "\n",
            "chè la Barbagia di Sardigna assai\n",
            "ne le femmine sue più è pudica\n",
            "che la Barbagia dov'io la lasciai. \n",
            "\n",
            "O d\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 0.5\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Colui che la carne sì bella e ride\n",
            "con le bellezze d'ogne sua paroffia; \n",
            "\n",
            "così fec'io, poi che mi provide\n",
            "la donna mia del suo risponder chiaro,\n",
            "e come stella in cielo il ver si vide. \n",
            "\n",
            "E poi che le parole sue restaro,\n",
            "non altrimenti fidoon come fermi. \n",
            "\n",
            "Tosto che dal circo si dileguan tutti,\n",
            "e li 'ntimando in là, e fatti far credenza\n",
            "che stral nova notizie prima s'appunta\n",
            "la tua seme parea figlio fede. \n",
            "\n",
            "La turba che rimase lì, selvaggia\n",
            "parea del loco, rimirando intorno\n",
            "come colui che nove cose assaggia. \n",
            "\n",
            "Da tutte parti saettava il giorno\n",
            "lo sol, ch'avea con le saette conte\n",
            "di mezzo 'l ciel cacciato Capricorno, \n",
            "\n",
            "quando la nova gente alzò la fronte\n",
            "ver' noi, dicendo a noi: \"Se voi sapete,\n",
            "mostratene la via di gire al monte\". \n",
            "\n",
            "E Virgilio rispuose: \"Voi credete\n",
            "forse che siamo esperti d'esto loco;\n",
            "ma noi siam peregrin come voi siete. \n",
            "\n",
            "Dianzi vidi mostrar lugidar s'appressa:\n",
            "chè saetta prega sotto l'alta Libra,\n",
            "e fisse pria che Fallan li pecca; \n",
            "\n",
            "e quel da culucchio sotto a quel cass\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 1.0\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Colui che la carni e l'altra frontesca\n",
            "che partinti, e più dolor qua sù non tremembre, \n",
            "\n",
            "mi dice diventaro sì verace\n",
            "quivi intagliato in un atto soave,\n",
            "che non sembiava imagine che tace. \n",
            "\n",
            "Giurato si saria ch'el dicesse 'Ave,\n",
            "ne li occhi miei che, voglie alcun tal, ch'alcuna stipa\n",
            "non vidi spirto in Dio tanto superbo,\n",
            "non quel che cadde a Tebe giù da' muri. \n",
            "\n",
            "El si fuggì che non parlò più verbo;\n",
            "e io vidi un centauro pien di raggi\n",
            "melosi si studia, sì che tua chiusi\n",
            "e senti' modo\" pria sentì sadisfar: \n",
            "\n",
            "Mosso la mente in Dio grata, Nosca,\n",
            "di retro a me che Dio stato fè sebbre. \n",
            "\n",
            "E se misesi del ciel mi mosse,\n",
            "ch'a tal mi fece sì ch'un sciolto\n",
            "s'appressa un sasso che da la gran fessa?\n",
            "e quel di mezzo 'l cibo che s'appone; \n",
            "\n",
            "e cieco toro più aspettava il sesto\n",
            "bisobile o salta Virgilio misura,\n",
            "nulla già mai sì giustamente morse; \n",
            "\n",
            "e così nulla fu di tanta ingiura,\n",
            "guardando a la persona che sofferse,\n",
            "in che era contratta tal natura. \n",
            "\n",
            "Però d'un atto uscir cose diverse:\n",
            "che ti mangiar ne \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed-aAA291bUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd534632-4bc2-4eb9-e9f9-72612aab887b"
      },
      "source": [
        "# Exam mode for 1 Canto so 33 terzine. 4000 characters to write\n",
        "start_inferno = \"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "start_purgatorio = \"\"\"\n",
        "Per correr miglior acque alza le vele\n",
        "omai la navicella del mio ingegno,\n",
        "che lascia dietro a se mar si crudele;\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "start_paradiso = \"\"\"\n",
        "La gloria di colui che tutto move\n",
        "per l'universo penetra, e risplende\n",
        "in una parte più e meno altrove.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "start_new = \"\"\" \"\"\"\n",
        "\n",
        "print(generate_text(start_new, generator, num_generate = 7000, temperature = 0.1))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " merto, per cui trarre un poco mi fissi. \n",
            "\n",
            "Ma la divina bontà, che da sè sperne\n",
            "ogne livore, ardendo in sè, sfavilla\n",
            "sì che dispiega le bellezze etterne. \n",
            "\n",
            "Ciò che da lei sanza mezzo distilla\n",
            "non ha poi fine, perchè non si move\n",
            "la sua imprenta quand' ella sigilla. \n",
            "\n",
            "Ciò che da essa sanza mezzo piove\n",
            "libero è tutto, perchè non soggiace\n",
            "a la virtute de le cose nove. \n",
            "\n",
            "Più l'è conforme, e però più le piace;\n",
            "chè l'ardor santo ch'ogne cosa raggia,\n",
            "ne la più somigliante è più vivace. \n",
            "\n",
            "Di tutte queste dote s'avvantaggia\n",
            "l'umana creatura, e s'una manca,\n",
            "di sua nobilità con sì bregna stola; \n",
            "\n",
            "chè la gente che fonde a goccia a goccia\n",
            "per li occhi il mal che tutto 'l verno,\n",
            "però ch'eran già titto in una loda,\n",
            "poca sarebbe a fornir questa vice. \n",
            "\n",
            "La bellezza ch'io vidi si trasmoda\n",
            "non pur di là da noi, ma certo io credo\n",
            "che solo il suo fattor tutta la goda. \n",
            "\n",
            "Da questo passo vinto mi concedo,\n",
            "più tosto dentro il suo novo ristrinse\n",
            "la nostra bianca mia è lor disira, \n",
            "\n",
            "così di retro a noi, più tosto mota,\n",
            "venendo e trapassando ci ammirava\n",
            "d'anime turba tacita e devota. \n",
            "\n",
            "Ne li occhi era ciascuna oscura e cava,\n",
            "palida ne la faccia, e tanto scema\n",
            "che da l'ossa la pelle s'informava. \n",
            "\n",
            "Non credo che così a buccia strema\n",
            "Erisittone fosse fatto secco,\n",
            "per digiunar, quando più n'ebbe tema. \n",
            "\n",
            "Io dicea fra me stesso pensando: 'Ecco\n",
            "la gente che perdè Ierusalemme,\n",
            "quando Maria nel figlio diè di becco!'. \n",
            "\n",
            "Parean l'occhiaie anella sanza gemme:\n",
            "chi nel viso de li uomini legge 'omo'\n",
            "ben avria quivi conosciuta l'emme. \n",
            "\n",
            "Chi crederebbe che l'odor d'un pomo\n",
            "sì governasse, generando brama,\n",
            "e quel d'un'acqua, non sappiendo como? \n",
            "\n",
            "Già era in ammirar che sì li affama,\n",
            "per la cagione ancor non manifesta\n",
            "di lor magrezza e di lor trista squama, \n",
            "\n",
            "ed ecco del profondo de la testa\n",
            "volse a me li occhi un'ombra e guardò fiso;\n",
            "poi gridò forte: \"Qual grazia m'è questa?\". \n",
            "\n",
            "Mai non l'avrei riconosciuto al viso;\n",
            "e quanto fia piacer del giusto Sire,\n",
            "tanto staremo immobili e distesi\". \n",
            "\n",
            "Io m'era inginocchiato e volea dire;\n",
            "ma com'io cominciai ed el s'accorse,\n",
            "solo ascoltando, del mio reverire, \n",
            "\n",
            "\"Qual cagion\", disse, \"in giù così ti torse?\".\n",
            "E io a lui: \"Per vostra dignitate\n",
            "mia coscienza dritto mi rimorse\". \n",
            "\n",
            "\"Drizza le gambe, lèvati sù, frate!\",\n",
            "rispuose; \"non errar: conservo sono\n",
            "teco e con li altri ad una podestate. \n",
            "\n",
            "Se mai quel santo evangelico suono\n",
            "che dice 'Neque nubent'intendesti,\n",
            "ben puoi veder perch'io così ragiono. \n",
            "\n",
            "Vattene omai: non vo' che più t'arresti;\n",
            "chè la tua stanza mio pianger disagia,\n",
            "col qual maturo ciò che tu dicesti. \n",
            "\n",
            "Nepote ho io di là c' ha nome Alagia,\n",
            "buona da sè, pur che la nostra casa\n",
            "non faccia lei per essempro malvagia; \n",
            "\n",
            "Contra miglior voler voler mal pugna;\n",
            "onde contra 'l piacer mio, per piacerli,\n",
            "trassi de l'acqua non sazia la spugna.\n",
            "\n",
            "Mossimi; e 'l duca mio si mosse per li\n",
            "luoghi spediti pur lungo la roccia,\n",
            "come si va per muro stretto a' merli; \n",
            "\n",
            "chè la gente che fonde a goccia a goccia\n",
            "per li occhi il mal che tutto 'l verno,\n",
            "però ch'eran già titto in una loda,\n",
            "poca sarebbe a fornir questa vice. \n",
            "\n",
            "La bellezza ch'io vidi si trasmoda\n",
            "non pur di là da noi, ma certo io credo\n",
            "che solo il suo fattor tutta la goda. \n",
            "\n",
            "Da questo passo vinto mi concedo,\n",
            "più tosto dentro il suo novo ristrinse\n",
            "la nostra bianca mia è lor disira, \n",
            "\n",
            "così di retro a noi, più tosto mota,\n",
            "venendo e trapassando ci ammirava\n",
            "d'anime turba tacita e devota. \n",
            "\n",
            "Ne li occhi era ciascuna oscura e cava,\n",
            "palida ne la faccia, e tanto scema\n",
            "che da l'ossa la pelle s'informava. \n",
            "\n",
            "Non credo che così a buccia strema\n",
            "Erisittone fosse fatto secco,\n",
            "per digiunar, quando più n'ebbe tema. \n",
            "\n",
            "Io dicea fra me stesso pensando: 'Ecco\n",
            "la gente che perdè Ierusalemme,\n",
            "quando Maria nel figlio diè di becco!'. \n",
            "\n",
            "Parean l'occhiaie anella sanza gemme:\n",
            "chi nel viso de li uomini legge 'omo'\n",
            "ben avria quivi conosciuta l'emme. \n",
            "\n",
            "Chi crederebbe che l'odor d'un pomo\n",
            "sì governasse, generando brama,\n",
            "e quel d'un'acqua, non sappiendo como? \n",
            "\n",
            "Già era in ammirar che sì li affama,\n",
            "per la cagione ancor non manifesta\n",
            "di lor magrezza e di lor trista squama, \n",
            "\n",
            "ed ecco del profondo de la testa\n",
            "volse a me li occhi un'ombra e guardò fiso;\n",
            "poi gridò forte: \"Qual grazia m'è questa?\". \n",
            "\n",
            "Mai non l'avrei riconosciuto al viso;\n",
            "ma ne la voce sua mi fu palese\n",
            "ciò che l'aspetto in sè avea conquiso. \n",
            "\n",
            "Questa favilla tutta mi raccese\n",
            "mia conoscenza a la cangiata labbia,\n",
            "e ravvisai la faccia di Forese. \n",
            "\n",
            "\"Deh, non contendere a l'asciutta scabbia\n",
            "che mi scolora\", pregava, \"la pelle,\n",
            "nè a difetto di carne ch'io abbia; \n",
            "\n",
            "ma dimmi il ver di te, dì chi son quelle\n",
            "due anime che là ti fanno scorta;\n",
            "non rimaner che tu non mi favelle!\". \n",
            "\n",
            "\"La faccia tua, ch'io lagrimai già morta,\n",
            "mi dà di pianger mo non minor doglia\",\n",
            "rispuos'io lui, \"veggendola sì torta. \n",
            "\n",
            "Però mi dì, per Dio, che sì vi sfoglia;\n",
            "non mi far dir mentr'io mi maraviglio,\n",
            "chè mal può dir chi è pien d'altra voglia\". \n",
            "\n",
            "Ed elli a me: \"De l'etterno consiglio\n",
            "cade vertù ne l'acqua e ne la pianta\n",
            "rimasa dietro, ond'io sì m'assottiglio. \n",
            "\n",
            "Tutta esta gente che piangendo canta\n",
            "per seguitar la gola oltra misura,\n",
            "in fame e 'n sete qui si rifà santa. \n",
            "\n",
            "Di bere e di mangiar n'accende cura\n",
            "l'odor ch'esser tì si fè, distributa\n",
            "in più posseditor, faccia più ricchi\n",
            "di sè che se da pochi è posseduto?\". \n",
            "\n",
            "Ed elli a me: \"Però che tu rificchi\n",
            "la mente pur a le cose terrene,\n",
            "di vera luce tenebre dispicchi. \n",
            "\n",
            "Quello infinito e ineffabil bene\n",
            "che là sù è, così corre ad amore\n",
            "com'a lucido corpo raggio vene. \n",
            "\n",
            "Tanto si dà quanto trova d'ardore;\n",
            "sì che, quantunque carità si stende,\n",
            "cresce sovr'essa l'etterno valore. \n",
            "\n",
            "E quanta gente più là sù s'intende,\n",
            "più v'è da bene amare, e più si confessa;\n",
            "e quel conoscitor de le peccata \n",
            "\n",
            "vede qual loco d'inferno è da essa;\n",
            "cignesi con la coda tante volte\n",
            "quantunque gradi vuol che giù sia messa. \n",
            "\n",
            "Sempre dinanzi a lui ne stanno molte:\n",
            "vanno a vicenda ciascuna al giudizio,\n",
            "dicono e odono e poi son giù volte. \n",
            "\n",
            "\"O tu che vieni al doloroso ospizio\",\n",
            "disse Minòs a me quando mi vide,\n",
            "lasciando l'atto di cotanto offizio, \n",
            "\n",
            "\"guarda com'entri e di cui tu ti fide;\n",
            "non t'inganni l'ampiezza de l'intrare!\".\n",
            "E 'l duca mio a lui: \"Perchè pur gride? \n",
            "\n",
            "Non impedir lo suo fatale andare:\n",
            "vuolsi così colà dove si puote\n",
            "ciò che si vuole, e più non dimandare\". \n",
            "\n",
            "Quinci fuor quete le lanose gote\n",
            "al nocchier de la livida palude,\n",
            "che 'ntorno a li occhi avea di fiamme rote. \n",
            "\n",
            "Ma quell'anime, ch'eran lasse e nude,\n",
            "cangiar colore e dibattero i denti,\n",
            "ratto che 'nteser le parole crude. \n",
            "\n",
            "Bestemmiavano Dio e lor parenti,\n",
            "l'umana spezie e 'l loco e 'l tempo e 'l seme\n",
            "di lor semenza e di lor nascimenti. \n",
            "\n",
            "Poi si ritrasser tutte quante insieme,\n",
            "forte piangendo, a la riva malvagia\n",
            "ch'attende ciascun uom che Dio non teme. \n",
            "\n",
            "Caron dimonio, con occhi distinto\n",
            "nel gran seggio a che tu li occhi tieni\n",
            "per la corona che già v'è sù posta, \n",
            "\n",
            "perch' elli è quelli che più h'è propinqua,\n",
            "grande fima nel suo farsi pusillo, \n",
            "\n",
            "a' frati suoi, sì com' a giuste rede,\n",
            "raccomandò la donna sua più cara,\n",
            "e comandò che l'amassero a fede; \n",
            "\n",
            "e del suo grembo l'ani\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fNIK2Qk4mxS"
      },
      "source": [
        "# Test Plagiarism\n",
        "\n",
        "Include the file **ngrams_plagiarism.py** downloaded from Virtuale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwHvXTTf6lgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6efb41f1-a210-48f9-ab15-305483abdd5f"
      },
      "source": [
        "from ngrams_plagiarism import ngrams_plagiarism\n",
        "\n",
        "gen = open('generated.txt').read()\n",
        "truth = open('Inferno.txt').read()\n",
        "ngrams_plagiarism(gen, truth)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.999231950844854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJH4vkZy0I3d"
      },
      "source": [
        "# Metrics\n",
        "\n",
        "Include the content of the folder **Deep Comedy Metrics** downloaded from Virtuale.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8hQBBUr0IgX",
        "outputId": "97ba8561-53e6-4d7a-da83-541c6fa09d99"
      },
      "source": [
        "!python3 main.py"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "fregiavan sì la sua faccia distretta\n",
            "l'appostalica dolce di maggior piee\n",
            "di splendor modi ch'el si confida: \n",
            "\n",
            "per cui tanta stoltezza in terra crebbe,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "per cui tanta stoltezza in terra crebbe,\n",
            "che, sanza prova d'alcun testimonio,\n",
            "ad ogne promession ricchi se' ? \n",
            "\n",
            "\"Là, dì quello che tra li altri è più tardo,\n",
            "\n",
            "Hendecasyllabicness: 0.975, Rhymeness: 1.0\n",
            "\n",
            "\"Là, dì quello che tra li altri è più tardo,\n",
            "sì che s'io non avessi un ronchion preso,\n",
            "caduto sarei giù sanz'esser urto. \n",
            "\n",
            "E 'l duca, che mi vide tanto atteso,\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "E 'l duca, che mi vide tanto atteso,\n",
            "disse: \"Dentro dai fuochi son li spirti;\n",
            "catun si fascia di quel ch'elli è inceso\". \n",
            "\n",
            "\"Maestro mio\", rispuos'io, \"per difetro,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.3333333333333333\n",
            "\n",
            "\"Maestro mio\", rispuos'io, \"per difetro,\n",
            "ma di giovanetto che retro a poco,\n",
            "perchè 'l tavilla sì lascia priva. \n",
            "\n",
            "La sua chiarezza sèguita l'ardore;\n",
            "\n",
            "Hendecasyllabicness: 1.0, Rhymeness: 0.0\n",
            "\n",
            "La sua chiarezza sèguita l'ardore;\n",
            "l'ardor la visione, e quella è tanta,\n",
            "quant' ha di grazia sovra suo valore. \n",
            "\n",
            "Come la carne gloriosa e santa\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181819, Rhymeness: 0.3333333333333333\n",
            "\n",
            "Come la carne gloriosa e santa\n",
            "fia rivestita, la nostra persona\n",
            "più grata fia per esser tutta quanta; \n",
            "\n",
            "per che s'accrescerà ciò che ne dona\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "per che s'accrescerà ciò che ne dona\n",
            "di gratuito lume il sommo bene,\n",
            "perchè la foga l'un de l'altro insolla\". \n",
            "\n",
            "Che potea io ridir, se non \"Io vegno\"?\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181819, Rhymeness: 0.3333333333333333\n",
            "\n",
            "Che potea io ridir, se non \"Io vegno\"?\n",
            "Dissilo, alquanto del color consperso\n",
            "che fa l'uom di perdon talvolta degno. \n",
            "\n",
            "E 'ntanto per la costa di traverso\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.3333333333333333\n",
            "\n",
            "E 'ntanto per la costa di traverso\n",
            "venivan genti innanzi a noi un poco,\n",
            "per altra via, che fu sì aspra e forte, \n",
            "\n",
            "così Beatrice trasmutato spiro,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Mentr'io mi dilettava di gire al monte,\n",
            "seder sovresso una puttana sciolta\n",
            "m'apparve con le ciglia intorno pronte; \n",
            "\n",
            "e come perchè non li fosse tolta,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "e come perchè non li fosse tolta,\n",
            "vidi di costa a lei dritto un gigante;\n",
            "e basciavansi insieme alcuna volta. \n",
            "\n",
            "Ma perchè l'occhio cupido e vagante\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Ma perchè l'occhio cupido e vagante\n",
            "a me rivolse, quel feroce drudo\n",
            "la flagellò dal capo infin le piante; \n",
            "\n",
            "poi, di sospetto pieno e d'ira crudo,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "poi, di sospetto pieno e d'ira crudo,\n",
            "disciolse il mostro, e trassel per la selva,\n",
            "tanto che sol di lei mi fece scudo \n",
            "\n",
            "'Deus, venerunt gentes', alternando\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.3333333333333333\n",
            "\n",
            "'Deus, venerunt gentes', alternando\n",
            "or tre or quattro dolce salmodia,\n",
            "le donne incominciaro, e lagrimando;\n",
            "\n",
            "e Bèatrice, sospirosa e pia,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "e Bèatrice, sospirosa e pia,\n",
            "quelle ascoltava sì fatta, che poco\n",
            "più a la croce si cambiò Maria. \n",
            "\n",
            "Ma poi che l'altre vergini dier loco\n",
            "\n",
            "Hendecasyllabicness: 1.0, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Ma poi che l'altre vergini dier loco\n",
            "a lei di dir, levata dritta in pè,\n",
            "rispuose, colorata come foco: \n",
            "\n",
            "'Modicum, et non videbitis me;\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "'Modicum, et non videbitis me;\n",
            "et iterum, sorelle mie dilette,\n",
            "modicum, et vos videbitis me'. \n",
            "\n",
            "Poi le si mise innanzi tutte e sette,\n",
            "\n",
            "Hendecasyllabicness: 1.0, Rhymeness: 1.0\n",
            "\n",
            "Poi le si mise innanzi tutte e sette,\n",
            "e dopo sè, solo accennando, mosse\n",
            "me e la donna e 'l savio che ristette. \n",
            "\n",
            "Così sen giva; e non credo che fosse\n",
            "\n",
            "Hendecasyllabicness: 1.0, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Così sen giva; e non credo che fosse\n",
            "lo decimo suo passo in terra posto,\n",
            "quando con li occhi li occhi mi percosse; \n",
            "\n",
            "e con tranquillo aspetto \"Vien più tosto\",\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "e con tranquillo aspetto \"Vien più tosto\",\n",
            "mi disse, \"tanto che, s'io parlo teco,\n",
            "ad ascoltarmi tu sie ben disposto\". \n",
            "\n",
            "Sì com'io fui, com'io dovèa, seco,\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Sì com'io fui, com'io dovèa, seco,\n",
            "dissemi: \"Frate, perchè non t'attenti\n",
            "a domandarmi omai venendo meco?\". \n",
            "\n",
            "Come a color che troppo reverenti\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Come a color che troppo reverenti\n",
            "dinanzi a suo maggior parlando sono,\n",
            "che non traggon la voce viva ai denti, \n",
            "\n",
            "avvenne a me, che sanza intero suono\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "avvenne a me, che sanza intero suono\n",
            "incominciai: \"Madonna, mia bisogna\n",
            "voi conoscete, e ciò ch'ad essa è buono\". \n",
            "\n",
            "Ed ella a me: \"Da tema e da vergogna\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Ed ella a me: \"Da tema e da vergogna\n",
            "voglio che tu omai ti disviluppe,\n",
            "sì che non parli più com'om che sogna. \n",
            "\n",
            "Sappi che 'l vaso che 'l serpente ruppe,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Sappi che 'l vaso che 'l serpente ruppe,\n",
            "fu e non è; ma chi n' ha colpa, creda\n",
            "che vendetta di Dio non teme suppe. \n",
            "\n",
            "Non sarà tutto tempo sanza reda\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Non sarà tutto tempo sanza reda\n",
            "l'aguglia che lasciò le penne al carro,\n",
            "per che divenne mostro e poscia preda; \n",
            "\n",
            "ch'io veggio certamente, e però il narro,\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "ch'io veggio certamente, e però il narro,\n",
            "a darne tempo già stelle propinque,\n",
            "secure d'ogn'intoppo e d'ogne sbarro, \n",
            "\n",
            "nel quale un cinquecento diece e cinque,\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "nel quale un cinquecento diece e cinque,\n",
            "messo di Dio, anciderà la fuia\n",
            "con quel gigante che con lei delinque. \n",
            "\n",
            "E forse che la mia narrazion buia,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "E forse che la mia narrazion buia,\n",
            "qual Temi e Sfinge, men ti persuade,\n",
            "perch'a lor modo lo 'ntelletto attuia; \n",
            "\n",
            "ma tosto fier li fatti le Naiade,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "ma tosto fier li fatti le Naiade,\n",
            "che solveranno questo enigma forte\n",
            "sanza danno di pecore o di biade. \n",
            "\n",
            "Tu nota; e sì come da me son porte,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Tu nota; e sì come da me son porte,\n",
            "così queste parole segna a' vivi\n",
            "del viver ch'è un correre a la morte. \n",
            "\n",
            "E aggi a mente, quando tu le scrivi,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "E aggi a mente, quando tu le scrivi,\n",
            "di non celar qual hai vista la pianta\n",
            "ch'è or due volte dirubata quivi. \n",
            "\n",
            "Qualunque ruba quella o quella schianta,\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Qualunque ruba quella o quella schianta,\n",
            "con bestemmia di fatto offende a Dio,\n",
            "che solo a l'uso suo la creò santa. \n",
            "\n",
            "Per morder quella, in pena e in disio\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181819, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Per morder quella, in pena e in disio\n",
            "cinquemilia anni e più l'anima prima\n",
            "bramò colui che 'l morso in sè punio. \n",
            "\n",
            "Dorme lo 'ngegno tuo, se non estima\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Dorme lo 'ngegno tuo, se non estima\n",
            "per singular cagione essere eccelsa\n",
            "lei tanto e sì travolta ne la cima. \n",
            "\n",
            "E se stati non fossero acqua d'Elsa\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "E se stati non fossero acqua d'Elsa\n",
            "li pensier vani intorno a la tua mente,\n",
            "e 'l piacer loro un Piramo a la gelsa, \n",
            "\n",
            "per tante circostanze solamente\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "per tante circostanze solamente\n",
            "la giustizia di Dio, ne l'interdetto,\n",
            "conosceresti a l'arbor moralmente. \n",
            "\n",
            "Ma perch'io veggio te ne lo 'ntelletto\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Ma perch'io veggio te ne lo 'ntelletto\n",
            "fatto di pietra e, impetrato, tinto,\n",
            "sì che t'abbaglia il lume del mio detto, \n",
            "\n",
            "voglio anco, e se non scritto, almen dipinto,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "voglio anco, e se non scritto, almen dipinto,\n",
            "che 'l te ne porti dentro a te per quello\n",
            "che si reca il bordon di palma cinto\". \n",
            "\n",
            "E io: \"Sì come cera da suggello,\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "E io: \"Sì come cera da suggello,\n",
            "che la figura impressa non trasmuta,\n",
            "segnato è or da voi lo mio cervello. \n",
            "\n",
            "Ma perchè tanto sovra mia veduta\n",
            "\n",
            "Hendecasyllabicness: 1.0, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Ma perchè tanto sovra mia veduta\n",
            "vostra parola disiata vola,\n",
            "che più la perde quanto più s'aiuta?\". \n",
            "\n",
            "\"Perchè conoschi\", disse, \"quella scuola\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181819, Rhymeness: 0.6666666666666666\n",
            "\n",
            "\"Perchè conoschi\", disse, \"quella scuola\n",
            "c' hai seguitata, e veder si trova\n",
            "altro non è ch'un lume di suo raggio, \n",
            "\n",
            "più che in altra convien che si saglia;\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181819, Rhymeness: 0.6666666666666666\n",
            "\n",
            "più che in altra convien che si saglia;\n",
            "non basta da costoro esser partito.\n",
            "Se tu mi 'ntendi, or fa sì che ti vaglia\". \n",
            "\n",
            "Leva' mi allor, mostrandomi fornito\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181819, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Leva' mi allor, mostrandomi fornito\n",
            "meglio di lena ch'i' non mi sentia,\n",
            "e dissi: \"Va, ch'i' son forte e ardito\". \n",
            "\n",
            "Su per lo scoglio prendemmo la via,\n",
            "\n",
            "Hendecasyllabicness: 0.9090909090909091, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Su per lo scoglio prendemmo la via,\n",
            "ch'era ronchioso, stretto e malagevole,\n",
            "ed erto più assai che quel di pria. \n",
            "\n",
            "Parlando andava per non parer fievole;\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Parlando andava per non parer fievole;\n",
            "onde una voce uscì de l'altro fosso,\n",
            "a parole formar disconvenevole. \n",
            "\n",
            "Non so che disse, ancor che sovra 'l dosso\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181819, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Non so che disse, ancor che sovra 'l dosso\n",
            "fossi de l'arco già che varca quivi;\n",
            "ma chi parlava ad ire parea mosso. \n",
            "\n",
            "E mentre ch'io là giù con l'occhio cerco,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.3333333333333333\n",
            "\n",
            "E mentre ch'io là giù con l'occhio cerco,\n",
            "vidi un col capo sì di merda lordo,\n",
            "che non parèa s'era laico o cherco. \n",
            "\n",
            "Quei mi sgridò: \"Perchè se' tu sì gordo\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.3333333333333333\n",
            "\n",
            "Quei mi sgridò: \"Perchè se' tu sì gordo\n",
            "di riguardar più me che li altri brutti?\".\n",
            "E io a lui: \"Perchè, se ben ricordo, \n",
            "\n",
            "già t' ho veduto coi capelli asciutti,\n",
            "\n",
            "Hendecasyllabicness: 0.9772727272727273, Rhymeness: 0.6666666666666666\n",
            "\n",
            "già t' ho veduto coi capelli asciutti,\n",
            "e se' Alessio Interminei da Lucca:\n",
            "però t'adocchio più che li altri tutti\". \n",
            "\n",
            "Ed elli allor, battendosi la zucca:\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181818, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Ed elli allor, battendosi la zucca:\n",
            "\"Qua giù m' hanno sommerso le lusinghe\n",
            "ond'io non ebbi mai la lingua stucca\". \n",
            "\n",
            "Appresso ciò lo duca \"Fa che pinghe\",\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Appresso ciò lo duca \"Fa che pinghe\",\n",
            "mi disse, \"il viso un poco più avante,\n",
            "sì che la faccia ben con l'occhio attinghe \n",
            "\n",
            "di quella sozza e scapigliata fante\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "di quella sozza e scapigliata fante\n",
            "che là si graffia con l'unghie merdose,\n",
            "e or s'accoscia e ora è in piedi stante. \n",
            "\n",
            "Taide è, la puttana che rispuose\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Taide è, la puttana che rispuose\n",
            "al drudo suo quando disse \"Ho io grazie\n",
            "grandi apo te?\": \"Anzi maravigliose!\". \n",
            "\n",
            "O Simon mago, o miseri seguaci\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.3333333333333333\n",
            "\n",
            "O Simon mago, o miseri seguaci\n",
            "che le cose di Dio, che di bontate\n",
            "deon essere spose, e voi rapaci\n",
            "\n",
            "per oro e per argento avolterate,\n",
            "\n",
            "Hendecasyllabicness: 1.0, Rhymeness: 0.6666666666666666\n",
            "\n",
            "per oro e per argento avolterate,\n",
            "or convien che per voi suoni la tromba,\n",
            "però che ne la terza bolgia state. \n",
            "\n",
            "Già eravamo, a la seguente tomba,\n",
            "\n",
            "Hendecasyllabicness: 0.9545454545454546, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Già eravamo, a la seguente tomba,\n",
            "montati de lo scoglio in quella parte\n",
            "ch'a punto sovra mezzo 'l fosso piomba. \n",
            "\n",
            "O somma sapienza, quanta è l'arte\n",
            "\n",
            "Hendecasyllabicness: 0.9318181818181819, Rhymeness: 0.6666666666666666\n",
            "\n",
            "Number of putative terzine: 61\n",
            "Number of well formed terzine: 59\n",
            "Average structuredness: 0.9635627530364372\n",
            "Average hendecasyllabicness: 0.9460323574730347\n",
            "Average rhymeness: 0.6206896551724139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbXuy2OopudL"
      },
      "source": [
        "# NEW IDEAS\n",
        "\n",
        "#### Training:\n",
        "*   Cross validation\n",
        "*   Insert Rhyme as feature to learn as haiku\n",
        "*   Use syllable as input and not word\n",
        "*   Different training on different dataset\n",
        "* Use categorical_crossentropy instead of sparse_ but with one-hot encoded inputs\n",
        "* Symbols for explicit start and end terzina\n",
        "* training as classificator for structure: like \"these two world are rhymes\" or \"this is a endecasillable and this not\" or \"this is a terzina and this not\" then generation\n",
        "* use dropout \n",
        "* use two lstm\n",
        "* \n",
        "\n",
        "#### Presentation\n",
        "* graphs over the vocabulary like distribution of used words\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7hZnK6wpy0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "96cd4b72-9654-4e18-a355-96bb0798a7d6"
      },
      "source": [
        "       RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "              \n",
        "    Dense(embedding_size, activation = relu),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "RNN.summary()\n",
        "\n",
        "generator = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,batch_input_shape=(1, None)),\n",
        "\n",
        "    Dense(embedding_size, activation = relu),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-805d388e0281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'len_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRSpzyH2BaG4"
      },
      "source": [
        "#@title\n",
        "#@DEBUG CUSTOM LOSS\n",
        "\n",
        "x = [[49, 46, 36, 44, 49, 32, 48, 36,  1, 45,  1, 35, 51, 36,  1, 45,  1, 50,\n",
        " 48, 36,  1, 46, 36, 48,  1, 49, 36, 30,  5,  0, 44, 45, 44,  1, 42, 32,\n",
        "  1, 37, 45, 48, 50, 51, 44, 32,  1, 35, 40,  1, 46, 48, 40, 43, 32,  1,\n",
        " 52, 32, 34, 32, 44, 50, 36,  5,  0, 44, 45, 44,  1, 35, 36, 34, 40, 43,\n",
        " 32, 49,  5,  1, 47, 51, 32, 36,  1, 49, 51, 44, 50,  1, 46, 32, 51, 46,\n",
        " 36, 48, 51, 43,  1, 14, 36, 40,  5,  1,  0,  0, 32, 35, 35, 40, 43, 32,\n",
        " 44, 35, 60,  5,  1, 43, 32,  1, 34, 45, 44, 50, 48, 45,  1, 32, 42,  1,\n",
        " 43, 45, 44, 35, 45,  1, 36, 48, 48, 32, 44, 50, 36,  0, 42, 40, 34, 36,\n",
        " 44, 55, 32,  1, 35, 40], \n",
        " [42,  1, 34, 45, 44, 49, 40, 38, 42, 40, 45,  1, 44, 36, 42,  1, 47, 51,\n",
        " 32, 42, 36,  1, 45, 38, 44, 36,  1, 32, 49, 46, 36, 50, 50, 45,  0, 34,\n",
        " 48, 36, 32, 50, 45,  1, 58,  1, 52, 40, 44, 50, 45,  1, 46, 48, 40, 32,\n",
        "  1, 34, 39, 36,  1, 52, 32, 35, 32,  1, 32, 42,  1, 37, 45, 44, 35, 45,\n",
        "  5,  1,  0,  0, 46, 36, 48, 60,  1, 34, 39, 36,  1, 32, 44, 35, 32, 49,\n",
        " 49, 36,  1, 52, 36, 48,  4,  1, 42, 45,  1, 49, 51, 45,  1, 35, 40, 42,\n",
        " 36, 50, 50, 45,  0, 42, 32,  1, 49, 46, 45, 49, 32,  1, 35, 40,  1, 34,\n",
        " 45, 42, 51, 40,  1, 34, 39,  4, 32, 35,  1, 32, 42, 50, 36,  1, 38, 48,\n",
        " 40, 35, 32,  0, 35, 40,]]\n",
        "y = [[46, 36, 44, 49, 32, 48, 36,  1, 45,  1, 35, 51, 36,  1, 45,  1, 50, 48,\n",
        " 36,  1, 46, 36, 48,  1, 49, 36, 40,  5,  0, 44, 45, 44,  1, 42, 32,  1,\n",
        " 37, 45, 48, 50, 51, 44, 32,  1, 35, 40,  1, 46, 48, 40, 43, 32,  1, 52,\n",
        " 32, 34, 32, 44, 50, 36,  5,  0, 44, 45, 44,  1, 35, 36, 34, 40, 43, 32,\n",
        " 49,  5,  1, 47, 51, 32, 36,  1, 49, 51, 44, 50,  1, 46, 32, 51, 46, 36,\n",
        " 48, 51, 43,  1, 14, 36, 40,  5,  1,  0,  0, 32, 35, 35, 40, 43, 32, 44,\n",
        " 35, 60,  5,  1, 43, 32,  1, 34, 45, 44, 50, 48, 45,  1, 32, 42,  1, 43,\n",
        " 45, 44, 35, 45,  1, 36, 48, 48, 32, 44, 50, 36,  0, 42, 40, 34, 36, 44,\n",
        " 55, 32,  1, 35, 40,  1], [ 1, 34, 45, 44, 49, 40, 38, 42, 40, 45,  1, 44, 36, 42,  1, 47, 51, 32,\n",
        " 42, 36,  1, 45, 38, 44, 36,  1, 32, 49, 46, 36, 50, 50, 45,  0, 34, 48,\n",
        " 36, 32, 50, 45,  1, 58,  1, 52, 40, 44, 50, 45,  1, 46, 48, 40, 32,  1,\n",
        " 34, 39, 36,  1, 52, 32, 35, 32,  1, 32, 42,  1, 37, 45, 44, 35, 45,  5,\n",
        "  1,  0,  0, 46, 36, 48, 60,  1, 34, 39, 36,  1, 32, 44, 35, 32, 49, 49,\n",
        " 36,  1, 52, 36, 48,  4,  1, 42, 45,  1, 49, 51, 45,  1, 35, 40, 42, 36,\n",
        " 50, 50, 45,  0, 42, 32,  1, 49, 46, 45, 49, 32,  1, 35, 40,  1, 34, 45,\n",
        " 42, 51, 40,  1, 34, 39,  4, 32, 35,  1, 32, 42, 50, 36,  1, 38, 48, 40,\n",
        " 35, 32,  0, 35, 40, 49,] ]\n",
        "\n",
        "'''\n",
        "EXPERIMENT\n",
        "CUSTOM LOSS\n",
        "'''\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "def divide_versi(y):\n",
        "  doppiozero = False\n",
        "\n",
        "  y_divided = [[]]\n",
        "  for ly in y:\n",
        "    ly = int(ly)\n",
        "\n",
        "    # devo pulire la lista dai segni di punteggiatura, \n",
        "    # in chartoidx significa i numeri da 1 a 10 compresi.\n",
        "    if ly in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:  # con i Tensor non funziona\n",
        "    # if ly is 1 or ly is 2 or ly is 3 or ly is 4 or ly is 5 or ly is 6 or ly is 7 \\\n",
        "    #     or ly is 8 or ly is 9 or ly is 10:\n",
        "        continue\n",
        "    else:\n",
        "      # se è zero vuol dire \\n quindi aggiungo una nuova riga\n",
        "      if ly is 0:\n",
        "        if not doppiozero:\n",
        "          y_divided.append([])\n",
        "        doppiozero = True\n",
        "        continue\n",
        "\n",
        "      y_divided[-1].append(ly)\n",
        "      doppiozero = False\n",
        "\n",
        "  if y_divided is not []:\n",
        "    if y[-1] != 0:\n",
        "      # dato che l'ultima riga non finisce con 0 vuol dire che è incompleta e la rimuovo\n",
        "      y_divided.pop()\n",
        "\n",
        "  # i need to re check because maybe i pop the only one\n",
        "  if len(y_divided) != 0:\n",
        "    if len(y_divided[0]) < 3:\n",
        "      # se la prima riga è minore di 4 non posso farci nulla quindi la elimino\n",
        "      y_divided.pop(0)\n",
        "\n",
        "  return y_divided\n",
        "\n",
        "def rhymes_extractor(y_divided):\n",
        "  # estraggo lo schema di rime da y\n",
        "  rhymes = []\n",
        "  for i in range(len(y_divided)):\n",
        "    # con la fine del verso (ultime due lettere) controllo se le altre righe \n",
        "    # finiscono con le stesse lettere\n",
        "    vy = y_divided[i]\n",
        "\n",
        "    last_word_1 = vy[-2:]\n",
        "\n",
        "    # ABA BCB CDC\n",
        "\n",
        "    # devo controllare se la riga i fa rima con la riga i+2 \n",
        "    if i+2 < len(y_divided):\n",
        "      next_vy = y_divided[i+2]\n",
        "      # print(vy[-2:])\n",
        "      # print(next_vy[-2:])\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+2))\n",
        "    \n",
        "    if i+4 < len(y_divided):\n",
        "      # print(vy[-2:])\n",
        "      # print(next_vy[-2:])\n",
        "      next_vy = y_divided[i+4]\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+4))\n",
        "\n",
        "  # print(rhymes)\n",
        "  return rhymes\n",
        "\n",
        "\n",
        "def get_custom_loss(x_batch, y_batch):\n",
        "  summed_custom_loss = 0\n",
        "  # x_batch ha lo shape (200, 200) quindi ho 200 vettori con 200 lettere ognuno\n",
        "  # le 200 lettere sono le feature\n",
        "\n",
        "  # max numero di rime possibili (arbitrario)\n",
        "  max_rhymes = 4\n",
        "\n",
        "  print(\"Shape di x_batch e y_batch\")\n",
        "  print((len(x_batch), len(x_batch[0])))\n",
        "  print((len(y_batch), len(y_batch[0])))\n",
        "\n",
        "  x_bin_tot = np.ones(shape=(len(x_batch), max_rhymes), dtype='float32')\n",
        "  y_bin_tot = np.ones(shape=(len(x_batch), max_rhymes), dtype='float32')\n",
        "\n",
        "  # scorro i 200 vettori\n",
        "  # for (x, y) in zip(x_batch, y_batch):  # Non funziona con i tensori\n",
        "  for v in range(len(x_batch)):\n",
        "    x = x_batch[v]\n",
        "    y = y_batch[v]\n",
        "\n",
        "    # given that the model returns a matrix with shape (150, 62) with the probability\n",
        "    # for each of the 62 character i need to use a categorical to choose the best\n",
        "    # then flatten the matrix into a list for evaluating\n",
        "    predicted_text = list(tf.random.categorical(x, num_samples=1).numpy())\n",
        "    x = np.concatenate(predicted_text).ravel().tolist()\n",
        "\n",
        "    # dividio il vettore in versi utili\n",
        "    x_divided = divide_versi(x)\n",
        "    y_divided = divide_versi(y)\n",
        "\n",
        "    print(\"Divisione in versi di x_batch e y_batch\")\n",
        "    print(x_divided)\n",
        "    print(y_divided)\n",
        "\n",
        "    # assicuro che il numero di versi siano uguali\n",
        "    # !!! non posso perchè il generato può avere errori e quindi, per esempio,\n",
        "    # avere più o meno versi\n",
        "    # assert len(x_divided) == len(y_divided)\n",
        "\n",
        "    # estraggo lo schema di rime\n",
        "    x_rhymes = rhymes_extractor(x_divided)\n",
        "    y_rhymes = rhymes_extractor(y_divided)\n",
        "\n",
        "    print(\"Rime dei versi di x_batch e y_batch\")\n",
        "    print(x_rhymes)\n",
        "    print(y_rhymes)\n",
        "\n",
        "    # mi ritorna una lista con il numero delle righe che fanno rima\n",
        "    # Esempio: [(1,3), (2,4)] significa che le righe 1 e 3 fanno rima e che le \n",
        "    # righe 2 e 4 pure \n",
        "    # TODO se avessimo due terzine intere si potrebbe valutare rime a 3 righe [aBaBcB]\n",
        "\n",
        "    # creo un vettore di 1 per la y perchè le rime ci sono sempre\n",
        "    y_bin = np.ones(max_rhymes, dtype='float32')\n",
        "    # creo un vettore di 1 per le rime generate, metterò 0 se la rima \n",
        "    # NON è presente in dante, abbuono con uno 0.5 visto che c'è la rima almeno\n",
        "    x_bin = np.ones(max_rhymes, dtype='float32')\n",
        "\n",
        "    if x_rhymes == []:\n",
        "      x_bin = np.zeros(max_rhymes, dtype='float32')\n",
        "\n",
        "    # se la rima generata è nelle rime originali di Dante allora la segno come valida\n",
        "    # tengo massimo max_ryhmes rime: posso perchè in 150-200 caratteri non ho più di 5-6 righe\n",
        "    # quindi in Dante avrei 2 rime, eccedo di 2 per aiutare la rete a creare rime anche sbagliate\n",
        "    for i in range(max_rhymes+1):\n",
        "      if i < len(x_rhymes):\n",
        "        if y_rhymes[i] not in x_rhymes:\n",
        "          x_bin[i] = 0.\n",
        "        if x_rhymes[i] not in y_rhymes:\n",
        "          x_bin[i] = 0.5\n",
        "\n",
        "    print(\"Vettore che rappresenta il confronto delle rime tra il generato e Dante dei versi di x_batch e y_batch \\n y è sempre 1 mentre il generato ha 1 se la rima c'è in dante o 0.5 se non c'è \")\n",
        "    print(x_bin)\n",
        "    print(y_bin)\n",
        "      \n",
        "    # concateno i vettori con l'encoding delle rime\n",
        "    x_bin_tot[v] = x_bin\n",
        "    y_bin_tot[v] = y_bin\n",
        "\n",
        "  print(\"Matrice dei vettori su cui eseguo la MSE: (x,y)\")\n",
        "  print(x_bin_tot)\n",
        "  print(y_bin_tot)\n",
        "  r = tf.keras.losses.mean_squared_error(y_bin_tot, x_bin_tot)\n",
        "\n",
        "  print(\"Risultato della MSE:\")\n",
        "  print(r)\n",
        "\n",
        "  print(\"Loss finale fatta con la media della MSE\")\n",
        "  print(np.mean(r))\n",
        "  # MSE sui vettori\n",
        "  return np.mean(r)\n",
        "\n",
        "\n",
        "\n",
        "debug_loss = True\n",
        "custom_loss = get_custom_loss(x,y)\n",
        "print(custom_loss)\n",
        "\n",
        "# NEW VERSION\n",
        "# creo un vettore con le rime di y reale e di y generato\n",
        "# Ex: in y reale se ho ABABC il vettore è [1,2,1,2,3] con o zero ad indicare nulla\n",
        "# per y generato devo creare un vettore di lunghezza uguale per poi valutarlo con una sparse_crossentropy\n",
        "# problema: non avrà mai le stesse righe\n",
        "\n",
        "# extract matrix of index of where the zeros are\n",
        "#tf.map_fn(fn=lambda t: t.map_fn(fn=lambda x: 1 if x == 0 else 0), elems=x_batch)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}