{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepComedy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m8xB0z_hYbG"
      },
      "source": [
        "# DeepComedy: AI Generated Divine Comedy\n",
        "\n",
        "Author: **Alessandro Pavesi, Federico Battistella**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncDUryHdqckI"
      },
      "source": [
        "This Notebook contains a **text generator RNN** that was trained on the **Divina Commedia** (the *Divine Comedy*) by **Dante Alighieri**. \n",
        "\n",
        "The structure is extremely complex: the poem is composed by three Cantiche, each Cantica has 33 Terzine, each Terzina is composed by three verses, each verse is composed of 11 syllables, and its rhymes follow an **A-B-A-B-C-B-C** structure.\n",
        "\n",
        "The final goal of this project is to rewrite one Canto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8FnhxiWY_Y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b26185-647b-40d7-89cb-6931b3432066"
      },
      "source": [
        "import time\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Attention, Flatten, Input, BatchNormalization\n",
        "from tensorflow.keras.activations import elu, relu, softmax\n",
        "from tensorflow.keras.metrics import categorical_accuracy, sparse_categorical_crossentropy, categorical_crossentropy\n",
        "\n",
        "from matplotlib import pyplot as plt "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVW2XbaZ_GGM"
      },
      "source": [
        "# Preliminaries Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkhp6e4Sztim"
      },
      "source": [
        "## Import and initial cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc_B0HVqkuRF"
      },
      "source": [
        "We delete all the special character, numbers and brackets to keep a uniform version of the text.\n",
        "We also remove the title of each Canto and each introductory text at his start.\n",
        "Moreover we remove the last row of each Canto to have only terzine. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5YiWU0daYkT"
      },
      "source": [
        "# Read the Divina Commedia\n",
        "with open( \"DivinaCommedia.txt\", 'r', encoding=\"utf8\") as file:\n",
        "    divina_commedia = file.read()\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'\\[.*\\r?\\n', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'.*Canto.*\\r?\\n', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'.*?\\n\\n\\n\\n', \"\", divina_commedia)  # remove the last row of each Canto, it's alone and can ruin the generation on correct terzine\n",
        "\n",
        "# divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")  # with this i lose the \"terzina\": results are not so exciting\n",
        "#divina_commedia = divina_commedia.replace(\" \\n\", \"<eot>\")  # end of terzina\n",
        "#divina_commedia = divina_commedia.replace(\"\\n\", \"<eor>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynI7x9Rp7yQe"
      },
      "source": [
        "print(divina_commedia[1:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILphRXIXaYrP"
      },
      "source": [
        "# Check lenght of text\n",
        "print(len(divina_commedia))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZw3-Joyhg5l"
      },
      "source": [
        "## Vocabulary and Char2Idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVbMQ7DPlFNC"
      },
      "source": [
        "Creation of an vector of ids for each character in the Comedy's vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwvjeLGWAVE4"
      },
      "source": [
        "# Store unique characters into a dict with numerical encoding\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "\n",
        "# Store them in a dict, associated with a numerical index\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2H5G86HhyvE"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJqttmDklf-0"
      },
      "source": [
        "Encode each character with a numerical vector of predefined length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk_BqFK4AU9H"
      },
      "source": [
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7JpYN9LAU7U"
      },
      "source": [
        "# Let's see what will look like\n",
        "print(\"{}\".format(divina_commedia[276:511]))\n",
        "print(\"\\nbecomes:\")\n",
        "print(numerical_encoding(divina_commedia[276:511], char2idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqALeTUmnl0X"
      },
      "source": [
        "# Processing Data for DanteRNN\n",
        "\n",
        "We need to generate the input for our RNN, the input sequence and an output sequence needs to be of equal length, in which each character is shifted left of one position.\n",
        "\n",
        "For example, the first verse:\n",
        "\n",
        "> Nel mezzo del cammin di nostra vita\n",
        "\n",
        "would be translated in a train sequence as:\n",
        "\n",
        "`Nel mezzo del cammin di nostra vit`\n",
        "\n",
        "be associated with the target sequence:\n",
        "\n",
        "`el mezzo del cammin di nostra vita`\n",
        "\n",
        "\n",
        "Train and target sets are fundamentally the same matrix, with the train having the last row removed, and the target set having the first removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWcZzOJdG6X9"
      },
      "source": [
        "# Apply it on the whole Comedy\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foyC3ZnGgKLN"
      },
      "source": [
        "print(encoded_text[311:600])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t41gYByxAU4B"
      },
      "source": [
        "def get_text_matrix(sequence, len_input):\n",
        "    \n",
        "    # create empty matrix\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    \n",
        "    # fill each row/time window from input sequence\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "        \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eazAAQiAk0i"
      },
      "source": [
        "len_text = 150\n",
        "text_matrix = get_text_matrix(encoded_text, len_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KonviQjQAk40"
      },
      "source": [
        "print(text_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaVngDG7AkyU"
      },
      "source": [
        "print(\"100th train sequence:\\n\")\n",
        "print(text_matrix[ 100, : ])\n",
        "print(\"\\n\\n100th target sequence:\\n\")\n",
        "print(text_matrix[ 101, : ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTb_xjIHFMQg"
      },
      "source": [
        "# Custom Loss\n",
        "Evaluate the structure of the rhymes, based on the real scheme with the aim to recreate the same exact rhyme structure of the Comedy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUfoUSUSXjl5"
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "def divide_versi(y):\n",
        "  doppiozero = False\n",
        "\n",
        "  y_divided = [[]]\n",
        "  for ly in y:\n",
        "    ly = int(ly)\n",
        "\n",
        "    # I have to clean the list of punctuation marks,\n",
        "    # in chartoidx means the numbers 1 to 10 inclusive.\n",
        "    if ly in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
        "        continue\n",
        "    else:\n",
        "      # if it is zero it means \\ n so I add a new line\n",
        "      if ly is 0:\n",
        "        if not doppiozero:\n",
        "          y_divided.append([])\n",
        "        doppiozero = True\n",
        "        continue\n",
        "\n",
        "      y_divided[-1].append(ly)\n",
        "      doppiozero = False\n",
        "\n",
        "  if y_divided is not []:\n",
        "    if y[-1] != 0:\n",
        "      # since the last line does not end with 0 it means that it is incomplete and I remove it\n",
        "      y_divided.pop()\n",
        "\n",
        "  # i need to re check because maybe i pop the only one\n",
        "  if len(y_divided) != 0:\n",
        "    if len(y_divided[0]) < 3:\n",
        "      # if the first line is less than 4 I can't do anything about it so I delete it\n",
        "      y_divided.pop(0)\n",
        "\n",
        "  return y_divided\n",
        "\n",
        "def rhymes_extractor(y_divided):\n",
        "  # I extract the rhyme scheme from y\n",
        "  rhymes = []\n",
        "  for i in range(len(y_divided)):\n",
        "    # with the end of the line (last two letters) I check if the other lines\n",
        "    # end with the same letters\n",
        "    vy = y_divided[i]\n",
        "\n",
        "    last_word_1 = vy[-2:]\n",
        "\n",
        "    # ABA BCB CDC\n",
        "\n",
        "    # I have to check if line i rhymes with line i + 2\n",
        "    if i+2 < len(y_divided):\n",
        "      next_vy = y_divided[i+2]\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+2))\n",
        "    \n",
        "    if i+4 < len(y_divided):\n",
        "      next_vy = y_divided[i+4]\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+4))\n",
        "\n",
        "  return rhymes\n",
        "\n",
        "\n",
        "def get_custom_loss(x_batch, y_batch):\n",
        "  summed_custom_loss = 0\n",
        "\n",
        "  # max number of rhymes (arbitrary choosen, it's an hyperparameter)\n",
        "  max_rhymes = 4\n",
        "\n",
        "  x_bin_tot = np.ones(shape=(len(x_batch), max_rhymes), dtype='float32')\n",
        "  y_bin_tot = np.ones(shape=(len(x_batch), max_rhymes), dtype='float32')\n",
        "\n",
        "  # iterate over each vector\n",
        "  for v in range(len(x_batch)):\n",
        "    x = x_batch[v]\n",
        "    y = y_batch[v]\n",
        "\n",
        "    # given that the model returns a matrix with shape (len_text, vocab_size) with the probability\n",
        "    # for each of the vocab_size character i need to use a categorical to choose the best\n",
        "    # then flatten the matrix into a list for evaluating\n",
        "    predicted_text = list(tf.random.categorical(x, num_samples=1).numpy())\n",
        "    x = np.concatenate(predicted_text).ravel().tolist()\n",
        "\n",
        "    # dividing the vector in verse\n",
        "    x_divided = divide_versi(x)\n",
        "    y_divided = divide_versi(y)\n",
        "\n",
        "    # extract the structure of the rhymes from generated and groud truth\n",
        "    x_rhymes = rhymes_extractor(x_divided)\n",
        "    y_rhymes = rhymes_extractor(y_divided)\n",
        "\n",
        "    # it returns me a list with the number of rhyming lines\n",
        "    # Example: [(1,3), (2,4)] means that lines 1 and 3 rhyme and that the\n",
        "    # lines 2 and 4 as well\n",
        "\n",
        "    # I create a vector of 1 for y because the rhymes are always there\n",
        "    y_bin = np.ones(max_rhymes, dtype='float32')\n",
        "    # I create a vector of 1 for the rhymes generated, I will put 0 if it rhyme\n",
        "    # Is NOT present in dante, discount with a 0.5 since there is at least the rhyme\n",
        "    x_bin = np.ones(max_rhymes, dtype='float32')\n",
        "\n",
        "    if x_rhymes == []:\n",
        "      x_bin = np.zeros(max_rhymes, dtype='float32')\n",
        "\n",
        "    # if the generated rhyme is in Dante's original rhymes then I sign it as valid\n",
        "    # I keep maximum max_ryhmes rhymes: I can because in 150-200 characters I don't have more than 5-6 lines\n",
        "    # so in Dante I would have 2 rhymes, I exceed 2 to help the network create even wrong rhymes\n",
        "    for i in range(max_rhymes+1):\n",
        "      if i < len(y_rhymes):\n",
        "        # check dante's rhyme with predicted rhymes, if it not exist set 0.0\n",
        "        if y_rhymes[i] not in x_rhymes:\n",
        "          x_bin[i] = 0.0\n",
        "        # check predicted rhyme with Dante's rhymes, if not exist set 0.5 to increase number of rhymes produced\n",
        "        if i < len(x_rhymes) and x_rhymes[i] not in y_rhymes:\n",
        "          x_bin[i] = 0.5\n",
        "\n",
        "    # concatenate vectors with rhyming encoding\n",
        "    x_bin_tot[v] = x_bin\n",
        "    y_bin_tot[v] = y_bin\n",
        "  \n",
        "  # MSE over vector\n",
        "  r = tf.keras.losses.mean_squared_error(y_bin_tot, x_bin_tot)\n",
        "\n",
        "  return np.mean(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPjMSZOzh_60"
      },
      "source": [
        "# Training Model\n",
        "\n",
        "At this point, we can specify the RNN architecture with all its hyperparameters. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kiExaZj-iG1"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFZfbimYAkvk"
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "# size of mini batches during training\n",
        "batch_size = 200  # 100\n",
        "\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "\n",
        "# vector size of char embeddings\n",
        "embedding_size = 200  # 200 250\n",
        "\n",
        "lstm_unit_1 = 2048\n",
        "lstm_unit_2 = 4096\n",
        "\n",
        "# debug variables\n",
        "debug_model = False\n",
        "if debug_model:\n",
        "  lstm_unit_1 = 1024\n",
        "  lstm_unit_2 = 2048\n",
        "\n",
        "dropout_value = 0.5\n",
        "hidden_size = 256  # for Dense() layers 250\n",
        "\n",
        "n_epochs = 75\n",
        "learning_rate = 0.001  # 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-LOqWfs06v3"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brY8sqvsD9h9"
      },
      "source": [
        "def perplexity_metric(loss):\n",
        "    \"\"\"Calculates perplexity metric = 2^(entropy) or e^(entropy)\"\"\"\n",
        "    return tf.exp(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHDktX8rF2Iz"
      },
      "source": [
        "## Custom learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yRTdMjQCvIJ"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=10):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step ** 1.5)\n",
        "    arg2 = step * ((self.warmup_steps+10) ** -1.3)\n",
        "    lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "    return lr\n",
        "\n",
        "d_model = 500\n",
        "learning_rate_custom_1 = CustomSchedule(d_model)\n",
        "plt.plot(learning_rate_custom_1(tf.range(n_epochs, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")\n",
        "\n",
        "\n",
        "learning_rate_custom_2 = tf.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=35,\n",
        "    decay_rate=0.90,\n",
        "    staircase=True)\n",
        "plt.plot(learning_rate_custom_2(tf.range(n_epochs, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOuGVW0YnH1B"
      },
      "source": [
        "Optimizer selected: Adamax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ng05biTbx6U"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate_custom_2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r22pA00o-fb7"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbtlhhW4bPyE"
      },
      "source": [
        "# Input Layer\n",
        "X = Input(shape=(None, ), batch_size=batch_size)\n",
        "\n",
        "# Embedding Layer\n",
        "embedded = Embedding(vocab_size, embedding_size, \n",
        "                     batch_input_shape=(batch_size, None), \n",
        "                     embeddings_regularizer=tf.keras.regularizers.L2()\n",
        "                     )(X)\n",
        "\n",
        "# Dense layer\n",
        "embedded = Dense(embedding_size, relu)(embedded)\n",
        "\n",
        "# First LSTM\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_1,return_sequences=True,return_state=True)(embedded)\n",
        "encoder_output = BatchNormalization()(encoder_output)\n",
        "\n",
        "# Dropout\n",
        "encoder_output = Dropout(dropout_value)(encoder_output)\n",
        "# Dense layer\n",
        "encoder_output = Dense(embedding_size, activation='relu')(encoder_output)\n",
        "\n",
        "# Dropout\n",
        "encoder_output = Dropout(dropout_value)(encoder_output)\n",
        "\n",
        "# Concat of first LSTM hidden state\n",
        "initial_state_double = [tf.concat([hidden_state, hidden_state], 1), tf.concat([hidden_state, hidden_state], 1)]\n",
        "\n",
        "# Second LSTM\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_2,\n",
        "                                                return_sequences=True, \n",
        "                                                return_state=True)(encoder_output, initial_state=initial_state_double) \n",
        "encoder_output = BatchNormalization()(encoder_output)\n",
        "\n",
        "# Dropout\n",
        "encoder_output = Dropout(dropout_value)(encoder_output)\n",
        "# Dense layer\n",
        "encoder_output = Dense(hidden_size, activation='relu')(encoder_output)\n",
        "\n",
        "# Dropout\n",
        "encoder_output = Dropout(dropout_value)(encoder_output)\n",
        "\n",
        "# Prediction Layer\n",
        "Y = Dense(units=vocab_size)(encoder_output)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=X, outputs=Y)\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer=optimizer)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPcE1ZuK-mkS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK1OwjD1IDD_"
      },
      "source": [
        "min_custom_loss = 1.0  # max value for the custom loss\n",
        "min_custom_epoch = 0  # epoch of minimum custom loss\n",
        "\n",
        "def train_on_batch(x, y, min_custom_loss):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # returns a tensor with shape (batch_size, len_text)\n",
        "        y_predicted = model(x)\n",
        "\n",
        "        scce = tf.keras.losses.sparse_categorical_crossentropy(y, y_predicted, from_logits = True)\n",
        "        # we cant return a tensor with that shape so we return a float that are summed\n",
        "        custom = get_custom_loss(y_predicted, y)\n",
        "\n",
        "        current_loss = tf.reduce_mean(scce + custom)\n",
        "\n",
        "    gradients = tape.gradient(current_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    perp = perplexity_metric(tf.reduce_mean(scce))\n",
        "\n",
        "    # checking for the best model using custom loss\n",
        "    # needed to do here because here we can save the model\n",
        "    if custom < min_custom_loss:\n",
        "      min_custom_loss = custom\n",
        "      model.save(\"best_model.h5\", overwrite=True)\n",
        "    return current_loss, scce, custom, perp, min_custom_loss\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "custom_loss_history = []\n",
        "perplexity_history = []\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "\n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss, scce, custom, perplexity, new_min_custom_loss = train_on_batch(x, y, min_custom_loss)\n",
        "\n",
        "        # save infos about the new min_custom_loss\n",
        "        if new_min_custom_loss < min_custom_loss:\n",
        "          min_custom_loss = new_min_custom_loss\n",
        "          min_custom_epoch = epoch\n",
        "\n",
        "        loss_history.append(current_loss)\n",
        "        custom_loss_history.append(custom)\n",
        "        perplexity_history.append(perplexity)\n",
        "    \n",
        "    print(\"{}.  \\t  Total-Loss: {}  \\t  Custom-Loss: {}  \\t Perplexity: {}  \\t Time: {} sec/epoch\".format(\n",
        "        epoch+1, current_loss.numpy(), custom, perplexity, round(time.time()-start, 2)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWovlkWZ_28a"
      },
      "source": [
        "model.save(F\"/content/gdrive/My Drive/DeepComedyModels/deep_comedy_custom_loss_01_62char.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CS75-X9-TBd"
      },
      "source": [
        "## Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qArT0IfJ2M3M"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Iterations')\n",
        "ax1.set_ylabel('Total Loss', color=color)\n",
        "ax1.plot(loss_history, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Custom Loss', color=color)  # we already handled the x-label with ax1\n",
        "ax2.plot(custom_loss_history, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "plt.show()\n",
        "print(\"The min custom loss is at iteration: {}\".format(min_custom_epoch*1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6uAhmV2Atth"
      },
      "source": [
        "plt.plot(perplexity_history)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrMdonRckAWB"
      },
      "source": [
        "# Generative Model\n",
        "\n",
        "At this point, let's check how the model generates text. In order to do it, we must make some changes to my RNN architecture above.\n",
        "\n",
        "First, we must change the fixed batch size. After training, we want to feed just one sentence into my Network to make it continue the character sequence. We will feed a string into the model, make it predict the next character, update the input sequence, and repeat the process until a long generated text is obtained. Because of this, the succession of input sequences is now different from training session, in which portions of text were sampled randomly. we now have to set `stateufl = True` in the `LSTM()` layer, so that each LSTM cell will keep in memory the internal state from the previous sequence. With this we make the model remember better sequential information while generating text.\n",
        "\n",
        "We will instantiate a new `generator` RNN with these new features, and transfer the trained weights of my `RNN` into it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcggT7ZY14NS"
      },
      "source": [
        "## Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyS28-ZAEcxU"
      },
      "source": [
        "# Input Layer\n",
        "X = Input(shape=(None, ), batch_size=1)\n",
        "embedded = Embedding(vocab_size, embedding_size)(X)\n",
        "embedded = Dense(embedding_size, relu)(embedded)\n",
        "\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_1,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True,\n",
        "                                              stateful=True)(embedded)\n",
        "encoder_output = BatchNormalization()(encoder_output)\n",
        "encoder_output = Dropout(dropout_value)(encoder_output)\n",
        "encoder_output = Dense(embedding_size, activation='relu')(encoder_output)\n",
        "\n",
        "initial_state_double = [tf.concat([hidden_state, hidden_state], 1), tf.concat([hidden_state, hidden_state], 1)]\n",
        "\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_2,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True,\n",
        "                                                stateful=True)(encoder_output, initial_state=initial_state_double)\n",
        "\n",
        "encoder_output = BatchNormalization()(encoder_output)\n",
        "encoder_output = Dropout(dropout_value)(encoder_output)\n",
        "encoder_output = Dense(hidden_size, activation='relu')(encoder_output)\n",
        "encoder_output = Dropout(dropout_value)(encoder_output)\n",
        "\n",
        "Y = Dense(units=vocab_size)(encoder_output)\n",
        "\n",
        "# Compile model\n",
        "generator = Model(inputs=X, outputs=Y)\n",
        "generator.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer=optimizer)\n",
        "print(generator.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jozUIEF19Qn"
      },
      "source": [
        "## Loading weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whB1azhVAtrp"
      },
      "source": [
        "# Import trained weights from RNN to generator\n",
        "load_file = True\n",
        "if load_file:\n",
        "  generator.load_weights(\"best_model.h5\")\n",
        "else:\n",
        "  generator.set_weights(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC8i-OMS2BEX"
      },
      "source": [
        "## Generating methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2hYSqAAtkn"
      },
      "source": [
        "def generate_text(start_string, model, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    predicted_ids = []\n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    model.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)  # one letter input\n",
        "        \n",
        "        # build the input for the next iteration, based on the last 5 characters generated\n",
        "        # become like a poetry!\n",
        "        #predicted_ids.append(predicted_id)\n",
        "        #if len(predicted_ids) > 5:\n",
        "        #  predicted_ids = predicted_ids[1:]\n",
        "        #input_eval = tf.expand_dims(predicted_ids, 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDzwpt1L2IAQ"
      },
      "source": [
        "## Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_NWeo1fAtiC"
      },
      "source": [
        "# Let's feed the first lines:\n",
        "start_string = \"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for t in [0.1, 0.2, 0.3, 0.5, 1.0]:\n",
        "    print(\"####### TEXT GENERATION - temperature = {}\\n\".format(t))\n",
        "    print(generate_text(start_string, generator, num_generate = 1000, temperature = t))\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed-aAA291bUU"
      },
      "source": [
        "# Exam mode for 1 Canto so 33 terzine. 4000 characters to write\n",
        "start_inferno = \"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "start_purgatorio = \"\"\"\n",
        "Per correr miglior acque alza le vele\n",
        "omai la navicella del mio ingegno,\n",
        "che lascia dietro a se mar si crudele;\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "start_paradiso = \"\"\"\n",
        "La gloria di colui che tutto move\n",
        "per l'universo penetra, e risplende\n",
        "in una parte più e meno altrove.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "start_new = \"\"\"\n",
        "\"\"\"\n",
        "start = time.time()\n",
        "generated = generate_text(start_inferno, generator, num_generate = 7000, temperature = 0.1)\n",
        "print(\"Time to generate {} characters: {} sec\".format(7000, round(time.time()-start, 2)))\n",
        "\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MvCMK3Dz88t"
      },
      "source": [
        "## Save generated Canto to file for Plagiarism Test and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98XTt7Zf0F4T"
      },
      "source": [
        "with open(\"generated.txt\", \"w+\") as text_file:\n",
        "    text_file.write(generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fNIK2Qk4mxS"
      },
      "source": [
        "# Plagiarism Test\n",
        "\n",
        "Include the file **ngrams_plagiarism.py** downloaded from Virtuale\n",
        "\n",
        "This mehod needs two file, we called it generated.txt (the same for the Metrics) and Inferno.txt (the first Canto of the Inferno)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwHvXTTf6lgw"
      },
      "source": [
        "from ngrams_plagiarism import ngrams_plagiarism\n",
        "\n",
        "gen = open('generated.txt').read()\n",
        "truth = open('Inferno.txt').read()\n",
        "ngrams_plagiarism(gen, truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJH4vkZy0I3d"
      },
      "source": [
        "# Metrics\n",
        "\n",
        "Include the content of the folder **Deep Comedy Metrics** downloaded from Virtuale.\n",
        "\n",
        "This method needs one file:\n",
        "*   generated.txt: the file generated by the network\n",
        "\n",
        "with UTF-8 Encoding!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8hQBBUr0IgX"
      },
      "source": [
        "!python3 main.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Xl-PDK6d2y"
      },
      "source": [
        "# Custom loss used for debug and explaination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRSpzyH2BaG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "4e61b965-f3c3-4354-ca47-9e10f39d9f61"
      },
      "source": [
        "#@title\n",
        "#@DEBUG CUSTOM LOSS\n",
        "\n",
        "x = [[49, 46, 36, 44, 49, 32, 48, 36,  1, 45,  1, 35, 51, 36,  1, 45,  1, 50,\n",
        " 48, 36,  1, 46, 36, 48,  1, 49, 36, 30,  5,  0, 44, 45, 44,  1, 42, 32,\n",
        "  1, 37, 45, 48, 50, 51, 44, 32,  1, 35, 40,  1, 46, 48, 40, 43, 32,  1,\n",
        " 52, 32, 34, 32, 44, 50, 36,  5,  0, 44, 45, 44,  1, 35, 36, 34, 40, 43,\n",
        " 32, 49,  5,  1, 47, 51, 32, 36,  1, 49, 51, 44, 50,  1, 46, 32, 51, 46,\n",
        " 36, 48, 51, 43,  1, 14, 36, 40,  5,  1,  0,  0, 32, 35, 35, 40, 43, 32,\n",
        " 44, 35, 60,  5,  1, 43, 32,  1, 34, 45, 44, 50, 48, 45,  1, 32, 42,  1,\n",
        " 43, 45, 44, 35, 45,  1, 36, 48, 48, 32, 44, 50, 36,  0, 42, 40, 34, 36,\n",
        " 44, 55, 32,  1, 35, 40], \n",
        " [42,  1, 34, 45, 44, 49, 40, 38, 42, 40, 45,  1, 44, 36, 42,  1, 47, 51,\n",
        " 32, 42, 36,  1, 45, 38, 44, 36,  1, 32, 49, 46, 36, 50, 50, 45,  0, 34,\n",
        " 48, 36, 32, 50, 45,  1, 58,  1, 52, 40, 44, 50, 45,  1, 46, 48, 40, 32,\n",
        "  1, 34, 39, 36,  1, 52, 32, 35, 32,  1, 32, 42,  1, 37, 45, 44, 35, 45,\n",
        "  5,  1,  0,  0, 46, 36, 48, 60,  1, 34, 39, 36,  1, 32, 44, 35, 32, 49,\n",
        " 49, 36,  1, 52, 36, 48,  4,  1, 42, 45,  1, 49, 51, 45,  1, 35, 40, 42,\n",
        " 36, 50, 50, 45,  0, 42, 32,  1, 49, 46, 45, 49, 32,  1, 35, 40,  1, 34,\n",
        " 45, 42, 51, 40,  1, 34, 39,  4, 32, 35,  1, 32, 42, 50, 36,  1, 38, 48,\n",
        " 40, 35, 32,  0, 35, 40,]]\n",
        "y = [[46, 36, 44, 49, 32, 48, 36,  1, 45,  1, 35, 51, 36,  1, 45,  1, 50, 48,\n",
        " 36,  1, 46, 36, 48,  1, 49, 36, 40,  5,  0, 44, 45, 44,  1, 42, 32,  1,\n",
        " 37, 45, 48, 50, 51, 44, 32,  1, 35, 40,  1, 46, 48, 40, 43, 32,  1, 52,\n",
        " 32, 34, 32, 44, 50, 36,  5,  0, 44, 45, 44,  1, 35, 36, 34, 40, 43, 32,\n",
        " 49,  5,  1, 47, 51, 32, 36,  1, 49, 51, 44, 50,  1, 46, 32, 51, 46, 36,\n",
        " 48, 51, 43,  1, 14, 36, 40,  5,  1,  0,  0, 32, 35, 35, 40, 43, 32, 44,\n",
        " 35, 60,  5,  1, 43, 32,  1, 34, 45, 44, 50, 48, 45,  1, 32, 42,  1, 43,\n",
        " 45, 44, 35, 45,  1, 36, 48, 48, 32, 44, 50, 36,  0, 42, 40, 34, 36, 44,\n",
        " 55, 32,  1, 35, 40,  1], [ 1, 34, 45, 44, 49, 40, 38, 42, 40, 45,  1, 44, 36, 42,  1, 47, 51, 32,\n",
        " 42, 36,  1, 45, 38, 44, 36,  1, 32, 49, 46, 36, 50, 50, 45,  0, 34, 48,\n",
        " 36, 32, 50, 45,  1, 58,  1, 52, 40, 44, 50, 45,  1, 46, 48, 40, 32,  1,\n",
        " 34, 39, 36,  1, 52, 32, 35, 32,  1, 32, 42,  1, 37, 45, 44, 35, 45,  5,\n",
        "  1,  0,  0, 46, 36, 48, 60,  1, 34, 39, 36,  1, 32, 44, 35, 32, 49, 49,\n",
        " 36,  1, 52, 36, 48,  4,  1, 42, 45,  1, 49, 51, 45,  1, 35, 40, 42, 36,\n",
        " 50, 50, 45,  0, 42, 32,  1, 49, 46, 45, 49, 32,  1, 35, 40,  1, 34, 45,\n",
        " 42, 51, 40,  1, 34, 39,  4, 32, 35,  1, 32, 42, 50, 36,  1, 38, 48, 40,\n",
        " 35, 32,  0, 35, 40, 49,] ]\n",
        "\n",
        "'''\n",
        "EXPERIMENT\n",
        "CUSTOM LOSS\n",
        "'''\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "def divide_versi(y):\n",
        "  doppiozero = False\n",
        "\n",
        "  y_divided = [[]]\n",
        "  for ly in y:\n",
        "    ly = int(ly)\n",
        "\n",
        "    # devo pulire la lista dai segni di punteggiatura, \n",
        "    # in chartoidx significa i numeri da 1 a 10 compresi.\n",
        "    if ly in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:  # con i Tensor non funziona\n",
        "    # if ly is 1 or ly is 2 or ly is 3 or ly is 4 or ly is 5 or ly is 6 or ly is 7 \\\n",
        "    #     or ly is 8 or ly is 9 or ly is 10:\n",
        "        continue\n",
        "    else:\n",
        "      # se è zero vuol dire \\n quindi aggiungo una nuova riga\n",
        "      if ly is 0:\n",
        "        if not doppiozero:\n",
        "          y_divided.append([])\n",
        "        doppiozero = True\n",
        "        continue\n",
        "\n",
        "      y_divided[-1].append(ly)\n",
        "      doppiozero = False\n",
        "\n",
        "  if y_divided is not []:\n",
        "    if y[-1] != 0:\n",
        "      # dato che l'ultima riga non finisce con 0 vuol dire che è incompleta e la rimuovo\n",
        "      y_divided.pop()\n",
        "\n",
        "  # i need to re check because maybe i pop the only one\n",
        "  if len(y_divided) != 0:\n",
        "    if len(y_divided[0]) < 3:\n",
        "      # se la prima riga è minore di 4 non posso farci nulla quindi la elimino\n",
        "      y_divided.pop(0)\n",
        "\n",
        "  return y_divided\n",
        "\n",
        "def rhymes_extractor(y_divided):\n",
        "  # estraggo lo schema di rime da y\n",
        "  rhymes = []\n",
        "  for i in range(len(y_divided)):\n",
        "    # con la fine del verso (ultime due lettere) controllo se le altre righe \n",
        "    # finiscono con le stesse lettere\n",
        "    vy = y_divided[i]\n",
        "\n",
        "    last_word_1 = vy[-2:]\n",
        "\n",
        "    # ABA BCB CDC\n",
        "\n",
        "    # devo controllare se la riga i fa rima con la riga i+2 \n",
        "    if i+2 < len(y_divided):\n",
        "      next_vy = y_divided[i+2]\n",
        "      # print(vy[-2:])\n",
        "      # print(next_vy[-2:])\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+2))\n",
        "    \n",
        "    if i+4 < len(y_divided):\n",
        "      # print(vy[-2:])\n",
        "      # print(next_vy[-2:])\n",
        "      next_vy = y_divided[i+4]\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+4))\n",
        "\n",
        "  # print(rhymes)\n",
        "  return rhymes\n",
        "\n",
        "\n",
        "def get_custom_loss(x_batch, y_batch):\n",
        "  summed_custom_loss = 0\n",
        "  # x_batch ha lo shape (200, 200) quindi ho 200 vettori con 200 lettere ognuno\n",
        "  # le 200 lettere sono le feature\n",
        "\n",
        "  # max numero di rime possibili (arbitrario)\n",
        "  max_rhymes = 4\n",
        "\n",
        "  print(\"Shape di x_batch e y_batch\")\n",
        "  print((len(x_batch), len(x_batch[0])))\n",
        "  print((len(y_batch), len(y_batch[0])))\n",
        "\n",
        "  x_bin_tot = np.ones(shape=(len(x_batch), max_rhymes), dtype='float32')\n",
        "  y_bin_tot = np.ones(shape=(len(x_batch), max_rhymes), dtype='float32')\n",
        "\n",
        "  # scorro i 200 vettori\n",
        "  # for (x, y) in zip(x_batch, y_batch):  # Non funziona con i tensori\n",
        "  for v in range(len(x_batch)):\n",
        "    x = x_batch[v]\n",
        "    y = y_batch[v]\n",
        "\n",
        "    # given that the model returns a matrix with shape (150, 62) with the probability\n",
        "    # for each of the 62 character i need to use a categorical to choose the best\n",
        "    # then flatten the matrix into a list for evaluating\n",
        "    #predicted_text = list(tf.random.categorical(x, num_samples=1).numpy())\n",
        "    #x = np.concatenate(predicted_text).ravel().tolist()\n",
        "\n",
        "    # dividio il vettore in versi utili\n",
        "    x_divided = divide_versi(x)\n",
        "    y_divided = divide_versi(y)\n",
        "\n",
        "    print(\"Divisione in versi di x_batch e y_batch\")\n",
        "    print(x_divided)\n",
        "    print(y_divided)\n",
        "\n",
        "    # assicuro che il numero di versi siano uguali\n",
        "    # !!! non posso perchè il generato può avere errori e quindi, per esempio,\n",
        "    # avere più o meno versi\n",
        "    # assert len(x_divided) == len(y_divided)\n",
        "\n",
        "    # estraggo lo schema di rime\n",
        "    x_rhymes = rhymes_extractor(x_divided)\n",
        "    y_rhymes = rhymes_extractor(y_divided)\n",
        "\n",
        "    print(\"Rime dei versi di x_batch e y_batch\")\n",
        "    print(x_rhymes)\n",
        "    print(y_rhymes)\n",
        "\n",
        "    # mi ritorna una lista con il numero delle righe che fanno rima\n",
        "    # Esempio: [(1,3), (2,4)] significa che le righe 1 e 3 fanno rima e che le \n",
        "    # righe 2 e 4 pure \n",
        "    # TODO se avessimo due terzine intere si potrebbe valutare rime a 3 righe [aBaBcB]\n",
        "\n",
        "    # creo un vettore di 1 per la y perchè le rime ci sono sempre\n",
        "    y_bin = np.ones(max_rhymes, dtype='float32')\n",
        "    # creo un vettore di 1 per le rime generate, metterò 0 se la rima \n",
        "    # NON è presente in dante, abbuono con uno 0.5 visto che c'è la rima almeno\n",
        "    x_bin = np.ones(max_rhymes, dtype='float32')\n",
        "\n",
        "    if x_rhymes == []:\n",
        "      x_bin = np.zeros(max_rhymes, dtype='float32')\n",
        "\n",
        "    # se la rima generata è nelle rime originali di Dante allora la segno come valida\n",
        "    # tengo massimo max_ryhmes rime: posso perchè in 150-200 caratteri non ho più di 5-6 righe\n",
        "    # quindi in Dante avrei 2 rime, eccedo di 2 per aiutare la rete a creare rime anche sbagliate\n",
        "    for i in range(max_rhymes+1):\n",
        "      if i < len(y_rhymes):\n",
        "        if y_rhymes[i] not in x_rhymes:\n",
        "          x_bin[i] = 0.0\n",
        "        if i < len(x_rhymes) and x_rhymes[i] not in y_rhymes:\n",
        "          x_bin[i] = 0.5\n",
        "\n",
        "    print(\"Vettore che rappresenta il confronto delle rime tra il generato e Dante dei versi di x_batch e y_batch \\n y è sempre 1 mentre il generato ha 1 se la rima c'è in dante o 0.5 se non c'è \")\n",
        "    print(x_bin)\n",
        "    print(y_bin)\n",
        "      \n",
        "    # concateno i vettori con l'encoding delle rime\n",
        "    x_bin_tot[v] = x_bin\n",
        "    y_bin_tot[v] = y_bin\n",
        "\n",
        "  print(\"Matrice dei vettori su cui eseguo la MSE: (x,y)\")\n",
        "  print(x_bin_tot)\n",
        "  print(y_bin_tot)\n",
        "  r = tf.keras.losses.mean_squared_error(y_bin_tot, x_bin_tot)\n",
        "\n",
        "  print(\"Risultato della MSE:\")\n",
        "  print(r)\n",
        "\n",
        "  print(\"Loss finale fatta con la media della MSE\")\n",
        "  print(np.mean(r))\n",
        "  # MSE sui vettori\n",
        "  return np.mean(r)\n",
        "\n",
        "\n",
        "custom_loss = get_custom_loss(x,y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape di x_batch e y_batch\n",
            "(2, 150)\n",
            "(2, 150)\n",
            "Divisione in versi di x_batch e y_batch\n",
            "[[49, 46, 36, 44, 49, 32, 48, 36, 45, 35, 51, 36, 45, 50, 48, 36, 46, 36, 48, 49, 36, 30], [44, 45, 44, 42, 32, 37, 45, 48, 50, 51, 44, 32, 35, 40, 46, 48, 40, 43, 32, 52, 32, 34, 32, 44, 50, 36], [44, 45, 44, 35, 36, 34, 40, 43, 32, 49, 47, 51, 32, 36, 49, 51, 44, 50, 46, 32, 51, 46, 36, 48, 51, 43, 14, 36, 40], [32, 35, 35, 40, 43, 32, 44, 35, 60, 43, 32, 34, 45, 44, 50, 48, 45, 32, 42, 43, 45, 44, 35, 45, 36, 48, 48, 32, 44, 50, 36]]\n",
            "[[46, 36, 44, 49, 32, 48, 36, 45, 35, 51, 36, 45, 50, 48, 36, 46, 36, 48, 49, 36, 40], [44, 45, 44, 42, 32, 37, 45, 48, 50, 51, 44, 32, 35, 40, 46, 48, 40, 43, 32, 52, 32, 34, 32, 44, 50, 36], [44, 45, 44, 35, 36, 34, 40, 43, 32, 49, 47, 51, 32, 36, 49, 51, 44, 50, 46, 32, 51, 46, 36, 48, 51, 43, 14, 36, 40], [32, 35, 35, 40, 43, 32, 44, 35, 60, 43, 32, 34, 45, 44, 50, 48, 45, 32, 42, 43, 45, 44, 35, 45, 36, 48, 48, 32, 44, 50, 36]]\n",
            "Rime dei versi di x_batch e y_batch\n",
            "[(1, 3)]\n",
            "[(0, 2), (1, 3)]\n",
            "Vettore che rappresenta il confronto delle rime tra il generato e Dante dei versi di x_batch e y_batch \n",
            " y è sempre 1 mentre il generato ha 1 se la rima c'è in dante o 0.5 se non c'è \n",
            "[0. 1. 1. 1.]\n",
            "[1. 1. 1. 1.]\n",
            "Divisione in versi di x_batch e y_batch\n",
            "[[42, 34, 45, 44, 49, 40, 38, 42, 40, 45, 44, 36, 42, 47, 51, 32, 42, 36, 45, 38, 44, 36, 32, 49, 46, 36, 50, 50, 45], [34, 48, 36, 32, 50, 45, 58, 52, 40, 44, 50, 45, 46, 48, 40, 32, 34, 39, 36, 52, 32, 35, 32, 32, 42, 37, 45, 44, 35, 45], [46, 36, 48, 60, 34, 39, 36, 32, 44, 35, 32, 49, 49, 36, 52, 36, 48, 42, 45, 49, 51, 45, 35, 40, 42, 36, 50, 50, 45], [42, 32, 49, 46, 45, 49, 32, 35, 40, 34, 45, 42, 51, 40, 34, 39, 32, 35, 32, 42, 50, 36, 38, 48, 40, 35, 32]]\n",
            "[[34, 45, 44, 49, 40, 38, 42, 40, 45, 44, 36, 42, 47, 51, 32, 42, 36, 45, 38, 44, 36, 32, 49, 46, 36, 50, 50, 45], [34, 48, 36, 32, 50, 45, 58, 52, 40, 44, 50, 45, 46, 48, 40, 32, 34, 39, 36, 52, 32, 35, 32, 32, 42, 37, 45, 44, 35, 45], [46, 36, 48, 60, 34, 39, 36, 32, 44, 35, 32, 49, 49, 36, 52, 36, 48, 42, 45, 49, 51, 45, 35, 40, 42, 36, 50, 50, 45], [42, 32, 49, 46, 45, 49, 32, 35, 40, 34, 45, 42, 51, 40, 34, 39, 32, 35, 32, 42, 50, 36, 38, 48, 40, 35, 32]]\n",
            "Rime dei versi di x_batch e y_batch\n",
            "[(0, 2)]\n",
            "[(0, 2)]\n",
            "Vettore che rappresenta il confronto delle rime tra il generato e Dante dei versi di x_batch e y_batch \n",
            " y è sempre 1 mentre il generato ha 1 se la rima c'è in dante o 0.5 se non c'è \n",
            "[1. 1. 1. 1.]\n",
            "[1. 1. 1. 1.]\n",
            "Matrice dei vettori su cui eseguo la MSE: (x,y)\n",
            "[[0. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "Risultato della MSE:\n",
            "tf.Tensor([0.25 0.  ], shape=(2,), dtype=float32)\n",
            "Loss finale fatta con la media della MSE\n",
            "0.125\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}