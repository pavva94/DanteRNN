{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_text_generator_00.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m8xB0z_hYbG"
      },
      "source": [
        "# TensorFlow 2 Text generator on Dante Alighieri's Divine Comedy\n",
        "\n",
        "Author: **Ivan Bongiorni**, [LinkedIn profile](https://www.linkedin.com/in/ivan-bongiorni-b8a583164/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncDUryHdqckI"
      },
      "source": [
        "This Notebook contains a **text generator RNN** that was trained on the **Divina Commedia** (the *Divine Comedy*) by **Dante Alighieri**. This is a poem written at the beginning of the XII century. It's hard to explain what it represents for Italian culture: it's without any doubt the main pillar of our national literature, one of the building blocks of modern Italian language, and arguably the gratest poem ever. All modern representations of Hell, Purgatory and Heaven derive from this opera.\n",
        "\n",
        "It's structure is extremely interesting: each verse is composed of 11 syllables, and its rhymes follow an **A-B-A-B** structure. Lot of pattern to be learned! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8FnhxiWY_Y1",
        "outputId": "43992bec-5d54-407b-a5dd-15e87f96233b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import time\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Attention, Flatten, Input\n",
        "from tensorflow.keras.activations import elu, relu, softmax\n",
        "from tensorflow.keras.metrics import categorical_accuracy, sparse_categorical_crossentropy, categorical_crossentropy\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import urllib\n",
        "\n",
        "# Read file from Colab Notebook\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5YiWU0daYkT"
      },
      "source": [
        "# Read the Divina Commedia\n",
        "with open( \"DivinaCommedia.txt\", 'r', encoding=\"utf8\") as file:\n",
        "    divina_commedia = file.read()\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "#divina_commedia = divina_commedia.replace(\"[\", \"\")\n",
        "#divina_commedia = divina_commedia.replace(\"]\", \"\")\n",
        "\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'\\[.*\\r?\\n', '', divina_commedia)\n",
        "divina_commedia = re.sub(r'.*Canto.*\\r?\\n', '', divina_commedia)\n",
        "\n",
        "# divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")  # with this i lose the \"terzina\": results are not so exciting\n",
        "#divina_commedia = divina_commedia.replace(\" \\n\", \"<eot>\")  # end of terzina\n",
        "#divina_commedia = divina_commedia.replace(\"\\n\", \"<eor>\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynI7x9Rp7yQe",
        "outputId": "fa06b9ad-7452-4283-ae12-9c9f3b6b5a42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(divina_commedia[1:1000])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NFERNO\n",
            "\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Ahi quanto a dir qual era è cosa dura\n",
            "esta selva selvaggia e aspra e forte\n",
            "che nel pensier rinova la paura! \n",
            "\n",
            "Tant'è amara che poco è più morte;\n",
            "ma per trattar del ben ch'i' vi trovai,\n",
            "dirò de l'altre cose ch'i' v' ho scorte. \n",
            "\n",
            "Io non so ben ridir com'i' v'intrai,\n",
            "tant'era pien di sonno a quel punto\n",
            "che la verace via abbandonai. \n",
            "\n",
            "Ma poi ch'i' fui al piè d'un colle giunto,\n",
            "là dove terminava quella valle\n",
            "che m'avea di paura il cor compunto, \n",
            "\n",
            "guardai in alto e vidi le sue spalle\n",
            "vestite già de' raggi del pianeta\n",
            "che mena dritto altrui per ogne calle. \n",
            "\n",
            "Allor fu la paura un poco queta,\n",
            "che nel lago del cor m'era durata\n",
            "la notte ch'i' passai con tanta pieta. \n",
            "\n",
            "E come quei che con lena affannata,\n",
            "uscito fuor del pelago a la riva,\n",
            "si volge a l'acqua perigliosa e guata, \n",
            "\n",
            "così l'animo mio, ch'ancor fuggiva,\n",
            "si volse a retro a rimirar lo passo\n",
            "che non lasciò già mai persona viva.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILphRXIXaYrP",
        "outputId": "201b33fc-913f-4df7-a4ce-1a49d0ef938f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check lenght of text\n",
        "print(len(divina_commedia))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "534048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZw3-Joyhg5l"
      },
      "source": [
        "I will now extract the set of unique characters, and create a dictionary for vectorization of text. In order to feed the text into a Neural Network, I must turn each character into a number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwvjeLGWAVE4"
      },
      "source": [
        "# Store unique characters into a dict with numerical encoding\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "\n",
        "# Store them in a dict, associated with a numerical index\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkXOJ3LGAVCF",
        "outputId": "bee8f734-19ec-4fd2-8d39-eb3b1bde1e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(char2idx))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sieJxDhAU_V",
        "outputId": "7f38c360-1dcc-4f72-d3a0-f7f8d09d8e3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "char2idx"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " ',': 5,\n",
              " '-': 6,\n",
              " '.': 7,\n",
              " ':': 8,\n",
              " ';': 9,\n",
              " '?': 10,\n",
              " 'A': 11,\n",
              " 'B': 12,\n",
              " 'C': 13,\n",
              " 'D': 14,\n",
              " 'E': 15,\n",
              " 'F': 16,\n",
              " 'G': 17,\n",
              " 'H': 18,\n",
              " 'I': 19,\n",
              " 'L': 20,\n",
              " 'M': 21,\n",
              " 'N': 22,\n",
              " 'O': 23,\n",
              " 'P': 24,\n",
              " 'Q': 25,\n",
              " 'R': 26,\n",
              " 'S': 27,\n",
              " 'T': 28,\n",
              " 'U': 29,\n",
              " 'V': 30,\n",
              " 'Z': 31,\n",
              " 'a': 32,\n",
              " 'b': 33,\n",
              " 'c': 34,\n",
              " 'd': 35,\n",
              " 'e': 36,\n",
              " 'f': 37,\n",
              " 'g': 38,\n",
              " 'h': 39,\n",
              " 'i': 40,\n",
              " 'j': 41,\n",
              " 'l': 42,\n",
              " 'm': 43,\n",
              " 'n': 44,\n",
              " 'o': 45,\n",
              " 'p': 46,\n",
              " 'q': 47,\n",
              " 'r': 48,\n",
              " 's': 49,\n",
              " 't': 50,\n",
              " 'u': 51,\n",
              " 'v': 52,\n",
              " 'x': 53,\n",
              " 'y': 54,\n",
              " 'z': 55,\n",
              " 'È': 56,\n",
              " 'à': 57,\n",
              " 'è': 58,\n",
              " 'ì': 59,\n",
              " 'ò': 60,\n",
              " 'ù': 61}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2H5G86HhyvE"
      },
      "source": [
        "Once I have a dictionary that maps each characted with its respective numerical index, I can process the whole corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk_BqFK4AU9H"
      },
      "source": [
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7JpYN9LAU7U",
        "outputId": "d989bc96-74ad-4697-c949-7465a880db00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's see what the first line will look like\n",
        "print(\"{}\".format(divina_commedia[276:511]))\n",
        "print(\"\\nbecomes:\")\n",
        "print(numerical_encoding(divina_commedia[276:511], char2idx))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "el ben ch'i' vi trovai,\n",
            "dirò de l'altre cose ch'i' v' ho scorte. \n",
            "\n",
            "Io non so ben ridir com'i' v'intrai,\n",
            "tant'era pien di sonno a quel punto\n",
            "che la verace via abbandonai. \n",
            "\n",
            "Ma poi ch'i' fui al piè d'un colle giunto,\n",
            "là dove terminava qu\n",
            "\n",
            "becomes:\n",
            "[36 42  1 33 36 44  1 34 39  4 40  4  1 52 40  1 50 48 45 52 32 40  5  0\n",
            " 35 40 48 60  1 35 36  1 42  4 32 42 50 48 36  1 34 45 49 36  1 34 39  4\n",
            " 40  4  1 52  4  1 39 45  1 49 34 45 48 50 36  7  1  0  0 19 45  1 44 45\n",
            " 44  1 49 45  1 33 36 44  1 48 40 35 40 48  1 34 45 43  4 40  4  1 52  4\n",
            " 40 44 50 48 32 40  5  0 50 32 44 50  4 36 48 32  1 46 40 36 44  1 35 40\n",
            "  1 49 45 44 44 45  1 32  1 47 51 36 42  1 46 51 44 50 45  0 34 39 36  1\n",
            " 42 32  1 52 36 48 32 34 36  1 52 40 32  1 32 33 33 32 44 35 45 44 32 40\n",
            "  7  1  0  0 21 32  1 46 45 40  1 34 39  4 40  4  1 37 51 40  1 32 42  1\n",
            " 46 40 58  1 35  4 51 44  1 34 45 42 42 36  1 38 40 51 44 50 45  5  0 42\n",
            " 57  1 35 45 52 36  1 50 36 48 43 40 44 32 52 32  1 47 51]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqALeTUmnl0X"
      },
      "source": [
        "## RNN dataprep\n",
        "\n",
        "I need to generate a set of stacked input sequences. My goal is to train a Neural Network to find a mapping between an input sequence and an output sequence of equal length, in which each character is shifted left of one position.\n",
        "\n",
        "For example, the first verse:\n",
        "\n",
        "> Nel mezzo del cammin di nostra vita\n",
        "\n",
        "would be translated in a train sequence as:\n",
        "\n",
        "`Nel mezzo del cammin di nostra vit`\n",
        "\n",
        "be associated with the target sequence:\n",
        "\n",
        "`el mezzo del cammin di nostra vita`\n",
        "\n",
        "The following function is a preparatory step for that. More generally, given a sequence:\n",
        "\n",
        "```\n",
        "A B C D E F G H I\n",
        "```\n",
        "\n",
        "and assuming input sequences of length 5, it will generate a matrix like:\n",
        "\n",
        "```\n",
        "A B C D E\n",
        "B C D E F\n",
        "C D E F G\n",
        "D E F G H\n",
        "E F G H I\n",
        "```\n",
        "\n",
        "I will save that matrix as it is in .csv format, to use it to train the Language Generator later.\n",
        "The split between train and target sets will be as:\n",
        "\n",
        "```\n",
        " Train:           Target:\n",
        "                 \n",
        "A B C D E        B C D E F\n",
        "B C D E F        C D E F G\n",
        "C D E F G        D E F G H\n",
        "D E F G H        E F G H I\n",
        "                 \n",
        "```\n",
        "\n",
        "Train and target sets are fundamentally the same matrix, with the train having the last row removed, and the target set having the first removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWcZzOJdG6X9"
      },
      "source": [
        "# Apply it on the whole Comedy\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foyC3ZnGgKLN",
        "outputId": "761ce27a-e48b-4fc0-aa8f-f6dd424141da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(encoded_text[311:600])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[42 50 48 36  1 34 45 49 36  1 34 39  4 40  4  1 52  4  1 39 45  1 49 34\n",
            " 45 48 50 36  7  1  0  0 19 45  1 44 45 44  1 49 45  1 33 36 44  1 48 40\n",
            " 35 40 48  1 34 45 43  4 40  4  1 52  4 40 44 50 48 32 40  5  0 50 32 44\n",
            " 50  4 36 48 32  1 46 40 36 44  1 35 40  1 49 45 44 44 45  1 32  1 47 51\n",
            " 36 42  1 46 51 44 50 45  0 34 39 36  1 42 32  1 52 36 48 32 34 36  1 52\n",
            " 40 32  1 32 33 33 32 44 35 45 44 32 40  7  1  0  0 21 32  1 46 45 40  1\n",
            " 34 39  4 40  4  1 37 51 40  1 32 42  1 46 40 58  1 35  4 51 44  1 34 45\n",
            " 42 42 36  1 38 40 51 44 50 45  5  0 42 57  1 35 45 52 36  1 50 36 48 43\n",
            " 40 44 32 52 32  1 47 51 36 42 42 32  1 52 32 42 42 36  0 34 39 36  1 43\n",
            "  4 32 52 36 32  1 35 40  1 46 32 51 48 32  1 40 42  1 34 45 48  1 34 45\n",
            " 43 46 51 44 50 45  5  1  0  0 38 51 32 48 35 32 40  1 40 44  1 32 42 50\n",
            " 45  1 36  1 52 40 35 40  1 42 36  1 49 51 36  1 49 46 32 42 42 36  0 52\n",
            " 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t41gYByxAU4B"
      },
      "source": [
        "def get_text_matrix(sequence, len_input):\n",
        "    \n",
        "    # create empty matrix\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    \n",
        "    # fill each row/time window from input sequence\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "        \n",
        "    return X"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eazAAQiAk0i"
      },
      "source": [
        "len_text = 150\n",
        "text_matrix = get_text_matrix(encoded_text, len_text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KonviQjQAk40",
        "outputId": "a67f575b-dcd0-41be-f69d-c5e15f32d13b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_matrix.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(533898, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaVngDG7AkyU",
        "outputId": "c9d0b83e-69c8-468b-bacf-cc1c198927b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"100th train sequence:\\n\")\n",
        "print(text_matrix[ 100, : ])\n",
        "print(\"\\n\\n100th target sequence:\\n\")\n",
        "print(text_matrix[ 101, : ])\n",
        "print(\"\\n\\n102th target sequence:\\n\")\n",
        "print(text_matrix[ 102, : ])\n",
        "print(\"\\n\\n115th target sequence:\\n\")\n",
        "print(text_matrix[ 180, : ])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100th train sequence:\n",
            "\n",
            "[36. 48. 32.  1. 49. 43. 32. 48. 48. 40. 50. 32.  7.  0.  0. 11. 39. 40.\n",
            "  1. 47. 51. 32. 44. 50. 45.  1. 32.  1. 35. 40. 48.  1. 47. 51. 32. 42.\n",
            "  1. 36. 48. 32.  1. 58.  1. 34. 45. 49. 32.  1. 35. 51. 48. 32.  0. 36.\n",
            " 49. 50. 32.  1. 49. 36. 42. 52. 32.  1. 49. 36. 42. 52. 32. 38. 38. 40.\n",
            " 32.  1. 36.  1. 32. 49. 46. 48. 32.  1. 36.  1. 37. 45. 48. 50. 36.  0.\n",
            " 34. 39. 36.  1. 44. 36. 42.  1. 46. 36. 44. 49. 40. 36. 48.  1. 48. 40.\n",
            " 44. 45. 52. 32.  1. 42. 32.  1. 46. 32. 51. 48. 32.  2.  1.  0.  0. 28.\n",
            " 32. 44. 50.  4. 58.  1. 32. 43. 32. 48. 32.  1. 34. 39. 36.  1. 46. 45.\n",
            " 34. 45.  1. 58.  1. 46.]\n",
            "\n",
            "\n",
            "100th target sequence:\n",
            "\n",
            "[48. 32.  1. 49. 43. 32. 48. 48. 40. 50. 32.  7.  0.  0. 11. 39. 40.  1.\n",
            " 47. 51. 32. 44. 50. 45.  1. 32.  1. 35. 40. 48.  1. 47. 51. 32. 42.  1.\n",
            " 36. 48. 32.  1. 58.  1. 34. 45. 49. 32.  1. 35. 51. 48. 32.  0. 36. 49.\n",
            " 50. 32.  1. 49. 36. 42. 52. 32.  1. 49. 36. 42. 52. 32. 38. 38. 40. 32.\n",
            "  1. 36.  1. 32. 49. 46. 48. 32.  1. 36.  1. 37. 45. 48. 50. 36.  0. 34.\n",
            " 39. 36.  1. 44. 36. 42.  1. 46. 36. 44. 49. 40. 36. 48.  1. 48. 40. 44.\n",
            " 45. 52. 32.  1. 42. 32.  1. 46. 32. 51. 48. 32.  2.  1.  0.  0. 28. 32.\n",
            " 44. 50.  4. 58.  1. 32. 43. 32. 48. 32.  1. 34. 39. 36.  1. 46. 45. 34.\n",
            " 45.  1. 58.  1. 46. 40.]\n",
            "\n",
            "\n",
            "102th target sequence:\n",
            "\n",
            "[32.  1. 49. 43. 32. 48. 48. 40. 50. 32.  7.  0.  0. 11. 39. 40.  1. 47.\n",
            " 51. 32. 44. 50. 45.  1. 32.  1. 35. 40. 48.  1. 47. 51. 32. 42.  1. 36.\n",
            " 48. 32.  1. 58.  1. 34. 45. 49. 32.  1. 35. 51. 48. 32.  0. 36. 49. 50.\n",
            " 32.  1. 49. 36. 42. 52. 32.  1. 49. 36. 42. 52. 32. 38. 38. 40. 32.  1.\n",
            " 36.  1. 32. 49. 46. 48. 32.  1. 36.  1. 37. 45. 48. 50. 36.  0. 34. 39.\n",
            " 36.  1. 44. 36. 42.  1. 46. 36. 44. 49. 40. 36. 48.  1. 48. 40. 44. 45.\n",
            " 52. 32.  1. 42. 32.  1. 46. 32. 51. 48. 32.  2.  1.  0.  0. 28. 32. 44.\n",
            " 50.  4. 58.  1. 32. 43. 32. 48. 32.  1. 34. 39. 36.  1. 46. 45. 34. 45.\n",
            "  1. 58.  1. 46. 40. 61.]\n",
            "\n",
            "\n",
            "115th target sequence:\n",
            "\n",
            "[32.  1. 36.  1. 37. 45. 48. 50. 36.  0. 34. 39. 36.  1. 44. 36. 42.  1.\n",
            " 46. 36. 44. 49. 40. 36. 48.  1. 48. 40. 44. 45. 52. 32.  1. 42. 32.  1.\n",
            " 46. 32. 51. 48. 32.  2.  1.  0.  0. 28. 32. 44. 50.  4. 58.  1. 32. 43.\n",
            " 32. 48. 32.  1. 34. 39. 36.  1. 46. 45. 34. 45.  1. 58.  1. 46. 40. 61.\n",
            "  1. 43. 45. 48. 50. 36.  9.  0. 43. 32.  1. 46. 36. 48.  1. 50. 48. 32.\n",
            " 50. 50. 32. 48.  1. 35. 36. 42.  1. 33. 36. 44.  1. 34. 39.  4. 40.  4.\n",
            "  1. 52. 40.  1. 50. 48. 45. 52. 32. 40.  5.  0. 35. 40. 48. 60.  1. 35.\n",
            " 36.  1. 42.  4. 32. 42. 50. 48. 36.  1. 34. 45. 49. 36.  1. 34. 39.  4.\n",
            " 40.  4.  1. 52.  4.  1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTb_xjIHFMQg"
      },
      "source": [
        "## Custom Loss\n",
        "Evaluate the structure of the rhymes, based on the real scheme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRSpzyH2BaG4",
        "outputId": "4d10fe5d-2635-408b-bad9-bd06d62d083e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = [[49, 46, 36, 44, 49, 32, 48, 36,  1, 45,  1, 35, 51, 36,  1, 45,  1, 50,\n",
        " 48, 36,  1, 46, 36, 48,  1, 49, 36, 30,  5,  0, 44, 45, 44,  1, 42, 32,\n",
        "  1, 37, 45, 48, 50, 51, 44, 32,  1, 35, 40,  1, 46, 48, 40, 43, 32,  1,\n",
        " 52, 32, 34, 32, 44, 50, 36,  5,  0, 44, 45, 44,  1, 35, 36, 34, 40, 43,\n",
        " 32, 49,  5,  1, 47, 51, 32, 36,  1, 49, 51, 44, 50,  1, 46, 32, 51, 46,\n",
        " 36, 48, 51, 43,  1, 14, 36, 40,  5,  1,  0,  0, 32, 35, 35, 40, 43, 32,\n",
        " 44, 35, 60,  5,  1, 43, 32,  1, 34, 45, 44, 50, 48, 45,  1, 32, 42,  1,\n",
        " 43, 45, 44, 35, 45,  1, 36, 48, 48, 32, 44, 50, 36,  0, 42, 40, 34, 36,\n",
        " 44, 55, 32,  1, 35, 40], \n",
        " [42,  1, 34, 45, 44, 49, 40, 38, 42, 40, 45,  1, 44, 36, 42,  1, 47, 51,\n",
        " 32, 42, 36,  1, 45, 38, 44, 36,  1, 32, 49, 46, 36, 50, 50, 45,  0, 34,\n",
        " 48, 36, 32, 50, 45,  1, 58,  1, 52, 40, 44, 50, 45,  1, 46, 48, 40, 32,\n",
        "  1, 34, 39, 36,  1, 52, 32, 35, 32,  1, 32, 42,  1, 37, 45, 44, 35, 45,\n",
        "  5,  1,  0,  0, 46, 36, 48, 60,  1, 34, 39, 36,  1, 32, 44, 35, 32, 49,\n",
        " 49, 36,  1, 52, 36, 48,  4,  1, 42, 45,  1, 49, 51, 45,  1, 35, 40, 42,\n",
        " 36, 50, 50, 45,  0, 42, 32,  1, 49, 46, 45, 49, 32,  1, 35, 40,  1, 34,\n",
        " 45, 42, 51, 40,  1, 34, 39,  4, 32, 35,  1, 32, 42, 50, 36,  1, 38, 48,\n",
        " 40, 35, 32,  0, 35, 40,]]\n",
        "y = [[46, 36, 44, 49, 32, 48, 36,  1, 45,  1, 35, 51, 36,  1, 45,  1, 50, 48,\n",
        " 36,  1, 46, 36, 48,  1, 49, 36, 40,  5,  0, 44, 45, 44,  1, 42, 32,  1,\n",
        " 37, 45, 48, 50, 51, 44, 32,  1, 35, 40,  1, 46, 48, 40, 43, 32,  1, 52,\n",
        " 32, 34, 32, 44, 50, 36,  5,  0, 44, 45, 44,  1, 35, 36, 34, 40, 43, 32,\n",
        " 49,  5,  1, 47, 51, 32, 36,  1, 49, 51, 44, 50,  1, 46, 32, 51, 46, 36,\n",
        " 48, 51, 43,  1, 14, 36, 40,  5,  1,  0,  0, 32, 35, 35, 40, 43, 32, 44,\n",
        " 35, 60,  5,  1, 43, 32,  1, 34, 45, 44, 50, 48, 45,  1, 32, 42,  1, 43,\n",
        " 45, 44, 35, 45,  1, 36, 48, 48, 32, 44, 50, 36,  0, 42, 40, 34, 36, 44,\n",
        " 55, 32,  1, 35, 40,  1], [ 1, 34, 45, 44, 49, 40, 38, 42, 40, 45,  1, 44, 36, 42,  1, 47, 51, 32,\n",
        " 42, 36,  1, 45, 38, 44, 36,  1, 32, 49, 46, 36, 50, 50, 45,  0, 34, 48,\n",
        " 36, 32, 50, 45,  1, 58,  1, 52, 40, 44, 50, 45,  1, 46, 48, 40, 32,  1,\n",
        " 34, 39, 36,  1, 52, 32, 35, 32,  1, 32, 42,  1, 37, 45, 44, 35, 45,  5,\n",
        "  1,  0,  0, 46, 36, 48, 60,  1, 34, 39, 36,  1, 32, 44, 35, 32, 49, 49,\n",
        " 36,  1, 52, 36, 48,  4,  1, 42, 45,  1, 49, 51, 45,  1, 35, 40, 42, 36,\n",
        " 50, 50, 45,  0, 42, 32,  1, 49, 46, 45, 49, 32,  1, 35, 40,  1, 34, 45,\n",
        " 42, 51, 40,  1, 34, 39,  4, 32, 35,  1, 32, 42, 50, 36,  1, 38, 48, 40,\n",
        " 35, 32,  0, 35, 40, 49,] ]\n",
        "\n",
        "\n",
        "custom_loss = get_custom_loss(x,y)\n",
        "print(custom_loss)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQEtBHnK26MH",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "def get_custom_loss(x_batch, y_batch):\n",
        "\n",
        "  summed_custom_loss = 0\n",
        "  # x_batch ha lo shape (200, 200) quindi ho 200 vettori con 200 lettere ognuno\n",
        "  # le 200 lettere sono le feature\n",
        "\n",
        "  # scorro i 200 vettori\n",
        "  # for (x, y) in zip(x_batch, y_batch):\n",
        "  for v in range(len(x_batch)):\n",
        "    x = x_batch[v]\n",
        "    y = y_batch[v]\n",
        "\n",
        "    # dividio il vettore in versi utili\n",
        "    x_divided = divide_versi(x)\n",
        "    y_divided = divide_versi(y)\n",
        "    # print(x_divided)\n",
        "    # print(y_divided)\n",
        "\n",
        "    # assicuro che il numero di versi siano uguali\n",
        "    # !!! non posso perchè il generato può avere errori e quindi, per esempio,\n",
        "    # avere più versi\n",
        "    # assert len(x_divided) == len(y_divided)\n",
        "\n",
        "    # estraggo lo schema di rime\n",
        "    x_rhymes = rhymes_extractor(x_divided)\n",
        "    y_rhymes = rhymes_extractor(y_divided)\n",
        "    # print(x_rhymes)\n",
        "    # print(y_rhymes)\n",
        "    # mi ritora una lista con il numero delle righe che fanno rima\n",
        "    # Esempio: [(1,3), (2,4)] significa che le righe 1 e 3 fanno rima e che le \n",
        "    # righe 2 e 4 pure \n",
        "    # TODO se avessimo due terzine intere si potrebbe valutare rime a 3 righe [aBaBcB]\n",
        "\n",
        "    if x_rhymes == []:\n",
        "      return 0.9  # max custom loss\n",
        "    \n",
        "    # se lo schema di rime del generato e di dante è uguale stop\n",
        "    if x_rhymes == y_rhymes:\n",
        "      return -0.2\n",
        "\n",
        "    custom_loss = 0.\n",
        "\n",
        "    # per ogni coppia di righe che fanno rima nel generato controllo che la \n",
        "    # stessa coppia di righe faccia rima in quello di dante\n",
        "    for i in range(len(x_rhymes)):\n",
        "      if x_rhymes[i] not in y_rhymes:\n",
        "        custom_loss += 0.15\n",
        "\n",
        "    # se il numero di rime del generato è minore di quello reale allora aumenta la loss\n",
        "    if len(y_rhymes) - len(x_rhymes) > 0:\n",
        "      custom_loss += 0.15 * (len(y_rhymes) - len(x_rhymes))\n",
        "\n",
        "\n",
        "    # sommo la loss per tutti i 200 vettori\n",
        "    summed_custom_loss += custom_loss\n",
        "  \n",
        "  # faccio una media sulla loss totale\n",
        "  # print(summed_custom_loss/x_batch.shape[0])\n",
        "  return summed_custom_loss/x_batch.shape[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUfoUSUSXjl5"
      },
      "source": [
        "'''\n",
        "EXPERIMENT\n",
        "CUSTOM LOSS\n",
        "'''\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "def divide_versi(y):\n",
        "  doppiozero = False\n",
        "\n",
        "  y_divided = [[]]\n",
        "  for ly in y:\n",
        "    ly = int(ly)\n",
        "\n",
        "    # devo pulire la lista dai segni di punteggiatura, \n",
        "    # in chartoidx significa i numeri da 1 a 10 compresi.\n",
        "    if ly in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:  # con i Tensor non funziona\n",
        "    # if ly is 1 or ly is 2 or ly is 3 or ly is 4 or ly is 5 or ly is 6 or ly is 7 \\\n",
        "    #     or ly is 8 or ly is 9 or ly is 10:\n",
        "        continue\n",
        "    else:\n",
        "      # se è zero vuol dire \\n quindi aggiungo una nuova riga\n",
        "      if ly is 0:\n",
        "        if not doppiozero:\n",
        "          y_divided.append([])\n",
        "        doppiozero = True\n",
        "        continue\n",
        "\n",
        "      y_divided[-1].append(ly)\n",
        "      doppiozero = False\n",
        "\n",
        "  if y_divided is not []:\n",
        "    if y[-1] != 0:\n",
        "      # dato che l'ultima riga non finisce con 0 vuol dire che è incompleta e la rimuovo\n",
        "      y_divided.pop()\n",
        "\n",
        "  # i need to re check because maybe i pop the only one\n",
        "  if len(y_divided) != 0:\n",
        "    if len(y_divided[0]) < 3:\n",
        "      # se la prima riga è minore di 4 non posso farci nulla quindi la elimino\n",
        "      y_divided.pop(0)\n",
        "\n",
        "  return y_divided\n",
        "\n",
        "def rhymes_extractor(y_divided):\n",
        "  # estraggo lo schema di rime da y\n",
        "  rhymes = []\n",
        "  for i in range(len(y_divided)):\n",
        "    # con la fine del verso (ultime due lettere) controllo se le altre righe \n",
        "    # finiscono con le stesse lettere\n",
        "    vy = y_divided[i]\n",
        "\n",
        "    last_word_1 = vy[-2:]\n",
        "\n",
        "    # ABA BCB CDC\n",
        "\n",
        "    # devo controllare se la riga i fa rima con la riga i+2 \n",
        "    if i+2 < len(y_divided):\n",
        "      next_vy = y_divided[i+2]\n",
        "      # print(vy[-2:])\n",
        "      # print(next_vy[-2:])\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+2))\n",
        "    \n",
        "    if i+4 < len(y_divided):\n",
        "      # print(vy[-2:])\n",
        "      # print(next_vy[-2:])\n",
        "      next_vy = y_divided[i+4]\n",
        "      if last_word_1 == next_vy[-2:]:\n",
        "        rhymes.append((i, i+4))\n",
        "\n",
        "  # print(rhymes)\n",
        "  return rhymes\n",
        "\n",
        "\n",
        "def get_custom_loss(x_batch, y_batch):\n",
        "  summed_custom_loss = 0\n",
        "  # x_batch ha lo shape (200, 200) quindi ho 200 vettori con 200 lettere ognuno\n",
        "  # le 200 lettere sono le feature\n",
        "\n",
        "  # max numero di rime possibili (arbitrario)\n",
        "  max_ryhmes = 4\n",
        "\n",
        "  x_bin_tot = np.ones(shape=(len(x_batch), max_ryhmes), dtype='float32')\n",
        "  y_bin_tot = np.ones(shape=(len(x_batch), max_ryhmes), dtype='float32')\n",
        "\n",
        "  # scorro i 200 vettori\n",
        "  # for (x, y) in zip(x_batch, y_batch):  # Non funziona con i tensori\n",
        "  for v in range(len(x_batch)):\n",
        "    x = x_batch[v]\n",
        "    y = y_batch[v]\n",
        "\n",
        "    # given that the model returns a matrix with shape (150, 62) with the probability\n",
        "    # for each of the 62 character i need to use a categorical to choose the best\n",
        "    # then flatten the matrix into a list for evaluating\n",
        "    predicted_text = list(tf.random.categorical(x, num_samples=1).numpy())\n",
        "    x = np.concatenate(predicted_text).ravel().tolist()\n",
        "\n",
        "    # dividio il vettore in versi utili\n",
        "    x_divided = divide_versi(x)\n",
        "    y_divided = divide_versi(y)\n",
        "\n",
        "    # assicuro che il numero di versi siano uguali\n",
        "    # !!! non posso perchè il generato può avere errori e quindi, per esempio,\n",
        "    # avere più o meno versi\n",
        "    # assert len(x_divided) == len(y_divided)\n",
        "\n",
        "    # estraggo lo schema di rime\n",
        "    x_rhymes = rhymes_extractor(x_divided)\n",
        "    y_rhymes = rhymes_extractor(y_divided)\n",
        "\n",
        "    # mi ritorna una lista con il numero delle righe che fanno rima\n",
        "    # Esempio: [(1,3), (2,4)] significa che le righe 1 e 3 fanno rima e che le \n",
        "    # righe 2 e 4 pure \n",
        "    # TODO se avessimo due terzine intere si potrebbe valutare rime a 3 righe [aBaBcB]\n",
        "\n",
        "    # creo un vettore di 1 per la y perchè le rime ci sono sempre\n",
        "    y_bin = np.ones(max_ryhmes, dtype='float32')\n",
        "    # creo un vettore di 0 per le rime generate, metterò 1 se la rima \n",
        "    # corrispondente è valida (cioè in dante)\n",
        "    x_bin = np.ones(max_ryhmes, dtype='float32')\n",
        "\n",
        "    if x_rhymes == []:\n",
        "      x_bin = np.zeros(max_ryhmes, dtype='float32')\n",
        "\n",
        "    # se la rima generata è nelle rime originali di Dante allora la segno come valida\n",
        "    for i in range(len(x_rhymes)):\n",
        "      if x_rhymes[i] not in y_rhymes:\n",
        "        x_bin[i] = 0\n",
        "      \n",
        "    # concateno i vettori con l'encoding delle rime\n",
        "    x_bin_tot[v] = x_bin\n",
        "    y_bin_tot[v] = y_bin\n",
        "\n",
        "  r = tf.keras.losses.mean_squared_error(y_bin_tot, x_bin_tot)\n",
        "\n",
        "  # MSE sui vettori\n",
        "  return np.mean(r)\n",
        "\n",
        "# NEW VERSION\n",
        "# creo un vettore con le rime di y reale e di y generato\n",
        "# Ex: in y reale se ho ABABC il vettore è [1,2,1,2,3] con o zero ad indicare nulla\n",
        "# per y generato devo creare un vettore di lunghezza uguale per poi valutarlo con una sparse_crossentropy\n",
        "# problema: non avrà mai le stesse righe\n",
        "\n",
        "# extract matrix of index of where the zeros are\n",
        "#tf.map_fn(fn=lambda t: t.map_fn(fn=lambda x: 1 if x == 0 else 0), elems=x_batch)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPjMSZOzh_60"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "At this point, I can specify the RNN architecture with all its hyperparameters. An `Embedding()` layer will first learn a representation of each character; the sequence of chracters embedding will then be fed into an `LSTM()` layer, that will extract information from their sequence; `Dense()` layers at the end will produce the next character prediction.\n",
        "\n",
        "The Network is structured to be fed with batches of data of fixed size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFZfbimYAkvk"
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "# size of mini batches during training\n",
        "batch_size = 100  # 100\n",
        "\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "\n",
        "# vector size of char embeddings\n",
        "embedding_size = 200  # 250\n",
        "\n",
        "lstm_unit_1 = 2048\n",
        "lstm_unit_2 = 4096\n",
        "\n",
        "# debug model\n",
        "debug_model = True\n",
        "if debug_model:\n",
        "  lstm_unit_1 = 1024\n",
        "  lstm_unit_2 = 2048\n",
        "\n",
        "\n",
        "hidden_size = 300  # for Dense() layers 250\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "learning_rate = 0.001  # 0.0001"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHDktX8rF2Iz"
      },
      "source": [
        "## Custom learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yRTdMjQCvIJ",
        "outputId": "0833b02c-b074-4fb9-c174-12c2f59102b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "#learning_rate_tot = []\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=10):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step ** 1.5)\n",
        "    arg2 = step * ((self.warmup_steps+10) ** -1.3)\n",
        "    lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    print(step)\n",
        "\n",
        "    return lr\n",
        "\n",
        "d_model = 500\n",
        "# learning_rate = CustomSchedule(d_model)\n",
        "#plt.plot(learning_rate(tf.range(50, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")\n",
        "\n",
        "\n",
        "learning_rate = tf.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=15,\n",
        "    decay_rate=0.80,\n",
        "    staircase=True)\n",
        "plt.plot(learning_rate(tf.range(50, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbtlhhW4bPyE",
        "outputId": "76070afb-e40b-45ff-eb30-2f2f2e5f374b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'''\n",
        "EXPERIMENT\n",
        "MODEL\n",
        "'''\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate)  # Adam\n",
        "\n",
        "def loss(y_true, y_pred):\n",
        "    \"\"\"Calculates categorical crossentropy as loss\"\"\"\n",
        "    return categorical_crossentropy(y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "\n",
        "def perplexity(labels, logits):\n",
        "    \"\"\"Calculates perplexity metric = 2^(entropy) or e^(entropy)\"\"\"\n",
        "    return pow(2, loss(y_true=labels, y_pred=logits))\n",
        "\n",
        "# Input Layer\n",
        "X = Input(shape=(None, ), batch_size=batch_size)  # 100 is the number of features\n",
        "\n",
        "# Word-Embedding Layer\n",
        "embedded = Embedding(vocab_size, embedding_size, \n",
        "                     batch_input_shape=(batch_size, None), \n",
        "                     # embeddings_initializer=tf.keras.initializers.RandomNormal(), \n",
        "                     embeddings_regularizer=tf.keras.regularizers.L1L2()\n",
        "                     )(X)\n",
        "embedded = Dense(embedding_size, relu)(embedded)\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_1,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True)(embedded)\n",
        "#attention_input = [encoder_output, hidden_state]\n",
        "encoder_output = Dropout(0.3)(encoder_output)\n",
        "encoder_output = Dense(embedding_size, activation='relu')(encoder_output)\n",
        "\n",
        "#encoder_output = Attention()(attention_input, training=True)\n",
        "\n",
        "initial_state = [hidden_state, cell_state]\n",
        "\n",
        "initial_state_double = [tf.concat([hidden_state, hidden_state], 1), tf.concat([hidden_state, hidden_state], 1)]\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_2,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True)(encoder_output, initial_state=initial_state_double)\n",
        "encoder_output = Dropout(0.3)(encoder_output)\n",
        "#encoder_output = Flatten()(encoder_output)\n",
        "encoder_output = Dense(hidden_size, activation='relu')(encoder_output)\n",
        "# Prediction Layer\n",
        "Y = Dense(units=vocab_size)(encoder_output)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=X, outputs=Y)\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer='adam', metrics=[perplexity, sparse_categorical_crossentropy])\n",
        "print(model.summary())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(100, None)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (100, None, 200)     12400       input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (100, None, 200)     40200       embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   [(100, None, 1024),  5017600     dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (100, None, 1024)    0           lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (100, None, 200)     205000      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_6 (TensorFlo [(None, 2048)]       0           lstm_6[0][1]                     \n",
            "                                                                 lstm_6[0][1]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_7 (TensorFlo [(None, 2048)]       0           lstm_6[0][1]                     \n",
            "                                                                 lstm_6[0][1]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   [(100, None, 2048),  18423808    dense_13[0][0]                   \n",
            "                                                                 tf_op_layer_concat_6[0][0]       \n",
            "                                                                 tf_op_layer_concat_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (100, None, 2048)    0           lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (100, None, 300)     614700      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (100, None, 62)      18662       dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 24,332,370\n",
            "Trainable params: 24,332,370\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK1OwjD1IDD_",
        "outputId": "bf5ef542-cd70-46c3-a438-14711607fa6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "#@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # returns a tensor with shape (batch_size, len_text)\n",
        "        x_predicted = model(x)\n",
        "        scce = tf.keras.losses.sparse_categorical_crossentropy(y, x_predicted, from_logits = True)\n",
        "        \n",
        "        # we cant return a tensor with that shape so we return a float that are summed\n",
        "        custom = get_custom_loss(x_predicted, y)\n",
        "        current_loss = tf.reduce_mean(scce + custom)\n",
        "        \n",
        "    gradients = tape.gradient(current_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return current_loss\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "\n",
        "\n",
        "    #sample = list(range(su bset_size*epoch, subset_size*(epoch+1)))\n",
        "    #sample_train = text_matrix[ sample , : ]\n",
        "    #next_sample = [x+1 for x in sample]\n",
        "    #sample_target = text_matrix[ next_sample , : ]\n",
        "    \n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        take = iteration * batch_size\n",
        "        x = sample_train[ take:take+batch_size , : ]\n",
        "        y = sample_target[ take:take+batch_size , : ]\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"{}.  \\t  Loss: {}  \\t  Time: {}sec/epoch\".format(\n",
        "        epoch+1, current_loss.numpy(), round(time.time()-start, 2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.  \t  Loss: 4.027477264404297  \t  Time: 48.51sec/epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWovlkWZ_28a"
      },
      "source": [
        "model.save(\"model_custom_loss_01.h5\")\n",
        "from google.colab import files\n",
        "files.download('model_custom_loss_01.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6uAhmV2Atth"
      },
      "source": [
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrMdonRckAWB"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "At this point, let's check how the model generates text. In order to do it, I must make some changes to my RNN architecture above.\n",
        "\n",
        "First, I must change the fixed batch size. After training, I want to feed just one sentence into my Network to make it continue the character sequence. I will feed a string into the model, make it predict the next character, update the input sequence, and repeat the process until a long generated text is obtained. Because of this, the succession of input sequences is now different from training session, in which portions of text were sampled randomly. I now have to set `stateufl = True` in the `LSTM()` layer, so that each LSTM cell will keep in memory the internal state from the previous sequence. With this I hope the model will better remember sequential information while generating text.\n",
        "\n",
        "I will instantiate a new `generator` RNN with these new features, and transfer the trained weights of my `RNN` into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyS28-ZAEcxU"
      },
      "source": [
        "'''\n",
        "EXPERIMENT\n",
        "GENERATOR\n",
        "'''\n",
        "\n",
        "# Input Layer\n",
        "X = Input(shape=(None, ), batch_size=1)  # 100 is the number of features\n",
        "\n",
        "# Word-Embedding Layer\n",
        "embedded = Embedding(vocab_size, embedding_size)(X)\n",
        "embedded = Dense(embedding_size, relu)(embedded)\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_1,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True,\n",
        "                                              stateful=True)(embedded)\n",
        "#attention_input = [encoder_output, hidden_state]\n",
        "\n",
        "encoder_output = Dropout(0.3)(encoder_output)\n",
        "\n",
        "encoder_output = Dense(embedding_size, activation='relu')(encoder_output)\n",
        "\n",
        "# encoder_output = Attention()(attention_input, training=True)\n",
        "initial_state = [hidden_state,  cell_state]\n",
        "\n",
        "initial_state_double = [tf.concat([hidden_state, hidden_state], 1), tf.concat([hidden_state, hidden_state], 1)]\n",
        "encoder_output, hidden_state, cell_state = LSTM(units=lstm_unit_2,\n",
        "                                                         return_sequences=True,\n",
        "                                                         return_state=True,\n",
        "                                                stateful=True)(encoder_output, initial_state=initial_state_double)\n",
        "#encoder_output = Flatten()(encoder_output)\n",
        "encoder_output = Dropout(0.3)(encoder_output)\n",
        "encoder_output = Dense(hidden_size, activation='relu')(encoder_output)\n",
        "# Prediction Layer\n",
        "Y = Dense(units=vocab_size)(encoder_output)\n",
        "\n",
        "# Compile model\n",
        "generator = Model(inputs=X, outputs=Y)\n",
        "generator.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), optimizer='adam', metrics=[perplexity, sparse_categorical_crossentropy])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whB1azhVAtrp"
      },
      "source": [
        "# Import trained weights from RNN to generator\n",
        "generator.set_weights(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2hYSqAAtkn"
      },
      "source": [
        "def generate_text(start_string, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    generator.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_NWeo1fAtiC"
      },
      "source": [
        "# Let's feed the first lines:\n",
        "start_string = \"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for t in [0.1, 0.5, 1.0, 1.5, 2]:\n",
        "    print(\"####### TEXT GENERATION - temperature = {}\\n\".format(t))\n",
        "    print(generate_text(start_string = start_string, num_generate = 1000, temperature = t))\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8KlRGTqmGBS"
      },
      "source": [
        "The best generation is, IMHO, the one with `temperature = 1.5`. The sentences of course do not make sense, but it's amazing that such a simple model could achieve similar results, and generate absolutely Dante-esque text with just ~40 minutes of GPU training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtB2gvhimUEs"
      },
      "source": [
        "Many things could be done at this point:\n",
        "\n",
        "\n",
        "\n",
        "*   Try fancier architectures, such as seq2seq. (I must say though that stacked RNNs didn't provide better results during prototyping.)\n",
        "*   Try Attention models.\n",
        "*   Longer training.\n",
        "*   Adversarial training.\n",
        "\n",
        "I'll try a lot of these techniques, alone and combined. My goal is to make a model that can learn the amazing structure of syllables and rhymes of the whole Comedy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbXuy2OopudL"
      },
      "source": [
        "# NEW IDEAS\n",
        "\n",
        "#### Training:\n",
        "*   Cross validation\n",
        "*   Insert Rhyme as feature to learn as haiku\n",
        "*   Use syllable as input and not word\n",
        "*   Different training on different dataset\n",
        "* Use categorical_crossentropy instead of sparse_ but with one-hot encoded inputs\n",
        "* Symbols for explicit start and end terzina\n",
        "* training as classificator for structure: like \"these two world are rhymes\" or \"this is a endecasillable and this not\" or \"this is a terzina and this not\" then generation\n",
        "* use dropout \n",
        "* use two lstm\n",
        "* \n",
        "\n",
        "#### Presentation\n",
        "* graphs over the vocabulary like distribution of used words\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7hZnK6wpy0j"
      },
      "source": [
        "RNN = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "              \n",
        "    Dense(embedding_size, activation = relu),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "RNN.summary()\n",
        "\n",
        "generator = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,batch_input_shape=(1, None)),\n",
        "\n",
        "    Dense(embedding_size, activation = relu),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "\n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(vocab_size)\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}