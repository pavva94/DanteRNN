{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "DanteRNNNEW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "lKJkhzzNxbHl",
        "colab_type": "heading"
      },
      "source": [
        "# Recursive Neural Network(RNN) for generating the Divina Commedia of Dante Alighieri."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m65QaaDcxbHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d644acf-528c-4eb8-a77e-71eebef4dc29"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint,  CSVLogger\n",
        "from keras.layers import Add, Dense, Input, LSTM, Layer\n",
        "from keras.models import Model, Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lXq7HdGxbH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Settings\n",
        "\n",
        "# Percent of samples to use for training, might be necessary if you're running out of memory\n",
        "sample_size = 1\n",
        "\n",
        "# The latent dimension of the LSTM\n",
        "latent_dim = 2048\n",
        "\n",
        "# Number of epochs to train for\n",
        "epochs = 20\n",
        "\n",
        "# path of the data\n",
        "data_path = 'DivinaCommedia.csv'\n",
        "\n",
        "name = 'all_data_test_2'\n",
        "output_dir = Path('output_%s' % name)\n",
        "try:\n",
        "  output_dir.mkdir()\n",
        "except FileExistsError:\n",
        "  pass"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VOGIQNYUyVQ",
        "colab_type": "text"
      },
      "source": [
        "# Data import and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsxq6dDIxbH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b2a85e77-2c16-4dbe-abfa-450d7de79621"
      },
      "source": [
        "df = pd.read_csv(str(data_path))\n",
        "df = df.sample(frac=sample_size)\n",
        "\n",
        "\n",
        "max_line_length = int(max([df['%s' % i].astype(str).str.len().quantile(.99) for i in range(3)]))\n",
        "\n",
        "df = df[\n",
        "    (df['0'].astype(str).str.len() <= max_line_length) & \n",
        "    (df['1'].astype(str).str.len() <= max_line_length) & \n",
        "    (df['2'].astype(str).str.len() <= max_line_length)\n",
        "].copy()\n",
        "\n",
        "# preprocessing data\n",
        "# Pad the lines to the max line length with new lines\n",
        "for i in range(3):\n",
        "    # For input, duplicate the first character\n",
        "    # TODO - Why?\n",
        "    df['%s_in' % i] = (df[str(i)].str[0] + df[str(i)]).str.pad(max_line_length+2, 'right', '\\n')\n",
        "    \n",
        "    # \n",
        "    #df['%s_out' % i] = df[str(i)].str.pad(max_line_len, 'right', '\\n') + ('\\n' if i == 2 else df[str(i+1)].str[0])\n",
        "    \n",
        "    # TODO - trying to add the next line's first character before the line breaks\n",
        "    if i == 2: # If it's the last line\n",
        "        df['%s_out' % i] = df[str(i)].str.pad(max_line_length+2, 'right', '\\n')\n",
        "    else: \n",
        "        # If it's the first or second line, add the first character of the next line to the end of this line.\n",
        "        # This helps with training so that the next RNN has a better chance of getting the first character right.\n",
        "        df['%s_out' % i] = (df[str(i)] + '\\n' + df[str(i+1)].str[0]).str.pad(max_line_length+2, 'right', '\\n')\n",
        "    \n",
        "max_line_length += 2\n",
        "\n",
        "inputs = df[['0_in', '1_in', '2_in']].values\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(filters='', char_level=True)\n",
        "tokenizer.fit_on_texts(inputs.flatten())\n",
        "n_tokens = len(tokenizer.word_counts) + 1\n",
        "\n",
        "\n",
        "print(df)\n",
        "\n",
        "# X is the input for each line in sequences of one-hot-encoded values\n",
        "X = np_utils.to_categorical([\n",
        "  tokenizer.texts_to_sequences(inputs[:,i]) for i in range(3)\n",
        "  ], num_classes=n_tokens)\n",
        "\n",
        "outputs = df[['0_out', '1_out', '2_out']].values\n",
        "\n",
        "# Y is the output for each line in sequences of one-hot-encoded values\n",
        "Y = np_utils.to_categorical([\n",
        "    tokenizer.texts_to_sequences(outputs[:,i]) for i in range(3)\n",
        "], num_classes=n_tokens)\n",
        "\n",
        "\n",
        "\n",
        "# X_syllables is the count of syllables for each line\n",
        "X_syllables = df[['0_syllables', '1_syllables', '2_syllables']].values\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Unnamed: 0  ...                                              2_out\n",
            "1043        1043  ...  In quella parte del giovanetto anno\\n\\n\\n\\n\\n\\...\n",
            "2006        2006  ...  perchè iv'era imaginata quella\\n\\n\\n\\n\\n\\n\\n\\n...\n",
            "606          606  ...  nullo martiro, fuor che la tua rabbia,\\n\\n\\n\\n...\n",
            "3867        3867  ...  insieme fui cristiano e Cacciaguida. \\n\\n\\n\\n\\...\n",
            "1069        1069  ...  Noi discendemmo il ponte da la testa\\n\\n\\n\\n\\n...\n",
            "...          ...  ...                                                ...\n",
            "2155        2155  ...  e cusce sì, come a sparvier selvaggio\\n\\n\\n\\n\\...\n",
            "849          849  ...  forte spingava con ambo le piote. \\n\\n\\n\\n\\n\\n...\n",
            "2836        2836  ...  la possa del salir più e 'l diletto. \\n\\n\\n\\n\\...\n",
            "520          520  ...  Chiròn si volse in su la destra poppa,\\n\\n\\n\\n...\n",
            "1105        1105  ...  de' quai nè io nè 'l duca mio s'accorse, \\n\\n\\...\n",
            "\n",
            "[4628 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0MWhOpeJB3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "130634cc-a0c4-4679-b227-227029a16d1a"
      },
      "source": [
        "print(max_line_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbbebPU8U3MZ",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxZSOj7kiwMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicDanteRNN(Model):\n",
        "    def __init__(self, latent_dim, n_tokens, generative=False):\n",
        "        super(BasicDanteRNN, self).__init__()\n",
        "        self.n_tokens = n_tokens\n",
        "        self.latent_dim = latent_dim\n",
        "        self.generative = generative\n",
        "        self.lstm = LSTM(latent_dim, return_state=True, return_sequences=True, name='lstm')\n",
        "        self.tl1 = BasicTrainingLine(self.lstm, self.latent_dim, self.n_tokens)\n",
        "        self.tl2 = BasicTrainingLine(self.lstm, self.latent_dim, self.n_tokens)\n",
        "        self.tl3 = BasicTrainingLine(self.lstm, self.latent_dim, self.n_tokens)\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        #print(inputs) # (<tf.Tensor 'IteratorGetNext:0' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=int64>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=int64>)\n",
        "        outputs = []\n",
        "\n",
        "        if self.generative:\n",
        "          syl1, syl2, syl3 = (11, 11, 11)\n",
        "\n",
        "          # using random start\n",
        "          first_char = chr(int(np.random.randint(ord('a'), ord('z')+1)))\n",
        "          print(tokenizer.texts_to_sequences(first_char))\n",
        "          print(tokenizer.texts_to_sequences(first_char)[0])\n",
        "          # Converting start string to numbers (vectorizing)\n",
        "          input_eval = tokenizer.texts_to_sequences(first_char)[0]\n",
        "          input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "          x1 = self.tl1((first_char, syl1), training=False, previous_line=None)\n",
        "          outputs.append(x1)\n",
        "\n",
        "          x2 = self.tl2((x, syl2), training=False, previous_line=self.tl1)\n",
        "          outputs.append(x2)\n",
        "\n",
        "          x3 = self.tl3((x2, syl3), training=False, previous_line=self.tl2)\n",
        "          outputs.append(x3)\n",
        "        else:\n",
        "\n",
        "          char1, syl1, char2, syl2, char3, syl3 = inputs\n",
        "          outputs.append(self.tl1((char1, syl1), training=training, previous_line=None))\n",
        "          outputs.append(self.tl2((char2, syl2), training=training, previous_line=self.tl1))\n",
        "          outputs.append(self.tl3((char3, syl3), training=training, previous_line=self.tl2))\n",
        "\n",
        "        #input_layer = [self.inp1, self.inp2, self.inp3]\n",
        "\n",
        "\n",
        "        print(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BasicTrainingLine(Model):\n",
        "    def __init__(self, lstm, latent_dim, n_tokens):\n",
        "        super(BasicTrainingLine, self).__init__()\n",
        "        self.lstm = lstm\n",
        "        self.n_tokens = n_tokens\n",
        "        self.dense_in = Dense(latent_dim, activation='relu')\n",
        "        self.dense_out = Dense(self.n_tokens, activation='softmax')\n",
        "        self.lstm_h = None\n",
        "        self.lstm_c = None\n",
        "\n",
        "    def call(self, inputs, training=None, previous_line=None, **kwargs):\n",
        "        #print(\"BasicTrainingLine Start\")\n",
        "        #print(inputs)\n",
        "        # x = self.syllable_input(inputs) NON SI FA PERCHé é UN INPUT LAYER\n",
        "        # INPUTS: ListWrapper([<tf.Tensor 'input_151:0' shape=(None, None, 41) dtype=float32>, UN CARATTERE TOKENIZZATO\n",
        "        #       <tf.Tensor 'input_152:0' shape=(None, 1) dtype=float32>]) NUMERO SILLABE\n",
        "\n",
        "        chars, syllable = inputs\n",
        "        x = self.dense_in(syllable, training=training)\n",
        "        #print(self.n_tokens)\n",
        "\n",
        "        #print(x)\n",
        "\n",
        "        if previous_line:\n",
        "            # WHAT ARE THESE ADD? Simply layer that do an addition maybe\n",
        "            initial_state = [\n",
        "                Add()([\n",
        "                    previous_line.lstm_h,\n",
        "                    x\n",
        "                ]),\n",
        "                Add()([\n",
        "                    previous_line.lstm_c,\n",
        "                    x\n",
        "                ])\n",
        "            ]\n",
        "\n",
        "            #print(previous_line.lstm_c)\n",
        "        else:\n",
        "            initial_state = [x, x]\n",
        "\n",
        "        #print(initial_state)\n",
        "\n",
        "        lstm_out, self.lstm_h, self.lstm_c = self.lstm(chars, initial_state=initial_state, training=training)\n",
        "        outputs = self.dense_out(lstm_out, training=training)\n",
        "        #print(lstm_out)\n",
        "        #print(outputs)\n",
        "\n",
        "        #print(\"BasicTrainingLine End\")\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PZFAaNJU__r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d94e26a-5cac-4361-cb9c-cdc4145e471b"
      },
      "source": [
        "# The latent dimension of the LSTM\n",
        "latent_dim = 2048\n",
        "model = BasicDanteRNN(latent_dim, n_tokens)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "filepath = str(output_dir / (\"%s-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5\" % latent_dim))\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "csv_logger = CSVLogger(str(output_dir / 'training_log.csv'), append=True, separator=',')\n",
        "\n",
        "callbacks_list = [checkpoint, csv_logger]\n",
        "\n",
        "\n",
        "#model.build(((None, 2048)))\n",
        "#model.summary()\n",
        "\n",
        "#print(model.output)\n",
        "\n",
        "# l'input X[0] contiene 46 vettori con ognuno 41 valori\n",
        "# 46 è il numero di caratteri massimo per riga\n",
        "# 41 è il numero di caratteri possibili per ogni carattare\n",
        "\n",
        "model.fit([\n",
        "    X[0], X_syllables[:,0],\n",
        "    X[1], X_syllables[:,1], \n",
        "    X[2], X_syllables[:,2]\n",
        "], [Y[0], Y[1], Y[2]], batch_size=64, epochs=epochs, validation_split=.1, callbacks=callbacks_list)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "True\n",
            "[<tf.Tensor 'basic_dante_rnn_1/basic_training_line_3/dense_7/truediv:0' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'basic_dante_rnn_1/basic_training_line_4/dense_9/truediv:0' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'basic_dante_rnn_1/basic_training_line_5/dense_11/truediv:0' shape=(None, 46, 41) dtype=float32>]\n",
            "True\n",
            "[<tf.Tensor 'basic_dante_rnn_1/basic_training_line_3/dense_7/truediv:0' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'basic_dante_rnn_1/basic_training_line_4/dense_9/truediv:0' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'basic_dante_rnn_1/basic_training_line_5/dense_11/truediv:0' shape=(None, 46, 41) dtype=float32>]\n",
            "66/66 [==============================] - ETA: 0s - loss: 9.3968 - output_1_loss: 3.0871 - output_2_loss: 3.2056 - output_3_loss: 3.1040False\n",
            "[<tf.Tensor 'basic_dante_rnn_1/basic_training_line_3/dense_7/truediv:0' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'basic_dante_rnn_1/basic_training_line_4/dense_9/truediv:0' shape=(None, 46, 41) dtype=float32>, <tf.Tensor 'basic_dante_rnn_1/basic_training_line_5/dense_11/truediv:0' shape=(None, 46, 41) dtype=float32>]\n",
            "\n",
            "Epoch 00001: loss improved from inf to 9.39676, saving model to output_all_data_test_2/2048-01-9.40-8.35.hdf5\n",
            "66/66 [==============================] - 13s 193ms/step - loss: 9.3968 - output_1_loss: 3.0871 - output_2_loss: 3.2056 - output_3_loss: 3.1040 - val_loss: 8.3546 - val_output_1_loss: 2.8347 - val_output_2_loss: 2.7894 - val_output_3_loss: 2.7305\n",
            "Epoch 2/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 8.0360 - output_1_loss: 2.6507 - output_2_loss: 2.7367 - output_3_loss: 2.6487\n",
            "Epoch 00002: loss improved from 9.39676 to 8.03601, saving model to output_all_data_test_2/2048-02-8.04-7.86.hdf5\n",
            "66/66 [==============================] - 12s 178ms/step - loss: 8.0360 - output_1_loss: 2.6507 - output_2_loss: 2.7367 - output_3_loss: 2.6487 - val_loss: 7.8637 - val_output_1_loss: 2.8013 - val_output_2_loss: 2.5234 - val_output_3_loss: 2.5390\n",
            "Epoch 3/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 6.8735 - output_1_loss: 2.3086 - output_2_loss: 2.3278 - output_3_loss: 2.2371\n",
            "Epoch 00003: loss improved from 8.03601 to 6.87345, saving model to output_all_data_test_2/2048-03-6.87-6.70.hdf5\n",
            "66/66 [==============================] - 12s 178ms/step - loss: 6.8735 - output_1_loss: 2.3086 - output_2_loss: 2.3278 - output_3_loss: 2.2371 - val_loss: 6.6969 - val_output_1_loss: 2.2292 - val_output_2_loss: 2.2886 - val_output_3_loss: 2.1791\n",
            "Epoch 4/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 6.1633 - output_1_loss: 2.0749 - output_2_loss: 2.0895 - output_3_loss: 1.9989\n",
            "Epoch 00004: loss improved from 6.87345 to 6.16328, saving model to output_all_data_test_2/2048-04-6.16-6.00.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 6.1633 - output_1_loss: 2.0749 - output_2_loss: 2.0895 - output_3_loss: 1.9989 - val_loss: 5.9982 - val_output_1_loss: 2.0169 - val_output_2_loss: 2.0430 - val_output_3_loss: 1.9383\n",
            "Epoch 5/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 5.8180 - output_1_loss: 1.9537 - output_2_loss: 1.9583 - output_3_loss: 1.9060\n",
            "Epoch 00005: loss improved from 6.16328 to 5.81801, saving model to output_all_data_test_2/2048-05-5.82-5.35.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 5.8180 - output_1_loss: 1.9537 - output_2_loss: 1.9583 - output_3_loss: 1.9060 - val_loss: 5.3514 - val_output_1_loss: 1.7741 - val_output_2_loss: 1.8238 - val_output_3_loss: 1.7534\n",
            "Epoch 6/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 5.1348 - output_1_loss: 1.7142 - output_2_loss: 1.7456 - output_3_loss: 1.6750\n",
            "Epoch 00006: loss improved from 5.81801 to 5.13479, saving model to output_all_data_test_2/2048-06-5.13-5.20.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 5.1348 - output_1_loss: 1.7142 - output_2_loss: 1.7456 - output_3_loss: 1.6750 - val_loss: 5.1952 - val_output_1_loss: 1.7397 - val_output_2_loss: 1.7697 - val_output_3_loss: 1.6858\n",
            "Epoch 7/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.8227 - output_1_loss: 1.6169 - output_2_loss: 1.6364 - output_3_loss: 1.5694\n",
            "Epoch 00007: loss improved from 5.13479 to 4.82273, saving model to output_all_data_test_2/2048-07-4.82-4.97.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 4.8227 - output_1_loss: 1.6169 - output_2_loss: 1.6364 - output_3_loss: 1.5694 - val_loss: 4.9750 - val_output_1_loss: 1.6749 - val_output_2_loss: 1.6869 - val_output_3_loss: 1.6132\n",
            "Epoch 8/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.5156 - output_1_loss: 1.5223 - output_2_loss: 1.5283 - output_3_loss: 1.4649\n",
            "Epoch 00008: loss improved from 4.82273 to 4.51555, saving model to output_all_data_test_2/2048-08-4.52-4.65.hdf5\n",
            "66/66 [==============================] - 12s 178ms/step - loss: 4.5156 - output_1_loss: 1.5223 - output_2_loss: 1.5283 - output_3_loss: 1.4649 - val_loss: 4.6542 - val_output_1_loss: 1.5497 - val_output_2_loss: 1.5697 - val_output_3_loss: 1.5348\n",
            "Epoch 9/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.2539 - output_1_loss: 1.4389 - output_2_loss: 1.4385 - output_3_loss: 1.3764\n",
            "Epoch 00009: loss improved from 4.51555 to 4.25388, saving model to output_all_data_test_2/2048-09-4.25-4.55.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 4.2539 - output_1_loss: 1.4389 - output_2_loss: 1.4385 - output_3_loss: 1.3764 - val_loss: 4.5471 - val_output_1_loss: 1.5368 - val_output_2_loss: 1.5382 - val_output_3_loss: 1.4721\n",
            "Epoch 10/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 4.0151 - output_1_loss: 1.3611 - output_2_loss: 1.3588 - output_3_loss: 1.2953\n",
            "Epoch 00010: loss improved from 4.25388 to 4.01513, saving model to output_all_data_test_2/2048-10-4.02-4.38.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 4.0151 - output_1_loss: 1.3611 - output_2_loss: 1.3588 - output_3_loss: 1.2953 - val_loss: 4.3797 - val_output_1_loss: 1.4781 - val_output_2_loss: 1.4793 - val_output_3_loss: 1.4224\n",
            "Epoch 11/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.8003 - output_1_loss: 1.2907 - output_2_loss: 1.2848 - output_3_loss: 1.2248\n",
            "Epoch 00011: loss improved from 4.01513 to 3.80032, saving model to output_all_data_test_2/2048-11-3.80-4.26.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 3.8003 - output_1_loss: 1.2907 - output_2_loss: 1.2848 - output_3_loss: 1.2248 - val_loss: 4.2642 - val_output_1_loss: 1.4492 - val_output_2_loss: 1.4385 - val_output_3_loss: 1.3766\n",
            "Epoch 12/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.5904 - output_1_loss: 1.2213 - output_2_loss: 1.2131 - output_3_loss: 1.1560\n",
            "Epoch 00012: loss improved from 3.80032 to 3.59038, saving model to output_all_data_test_2/2048-12-3.59-4.17.hdf5\n",
            "66/66 [==============================] - 12s 178ms/step - loss: 3.5904 - output_1_loss: 1.2213 - output_2_loss: 1.2131 - output_3_loss: 1.1560 - val_loss: 4.1719 - val_output_1_loss: 1.4050 - val_output_2_loss: 1.4217 - val_output_3_loss: 1.3452\n",
            "Epoch 13/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.3826 - output_1_loss: 1.1526 - output_2_loss: 1.1441 - output_3_loss: 1.0858\n",
            "Epoch 00013: loss improved from 3.59038 to 3.38256, saving model to output_all_data_test_2/2048-13-3.38-4.08.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 3.3826 - output_1_loss: 1.1526 - output_2_loss: 1.1441 - output_3_loss: 1.0858 - val_loss: 4.0809 - val_output_1_loss: 1.3822 - val_output_2_loss: 1.3894 - val_output_3_loss: 1.3093\n",
            "Epoch 14/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1460 - output_1_loss: 1.0738 - output_2_loss: 1.0647 - output_3_loss: 1.0075\n",
            "Epoch 00014: loss improved from 3.38256 to 3.14598, saving model to output_all_data_test_2/2048-14-3.15-4.03.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 3.1460 - output_1_loss: 1.0738 - output_2_loss: 1.0647 - output_3_loss: 1.0075 - val_loss: 4.0267 - val_output_1_loss: 1.3580 - val_output_2_loss: 1.3669 - val_output_3_loss: 1.3018\n",
            "Epoch 15/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.8785 - output_1_loss: 0.9861 - output_2_loss: 0.9732 - output_3_loss: 0.9192\n",
            "Epoch 00015: loss improved from 3.14598 to 2.87851, saving model to output_all_data_test_2/2048-15-2.88-4.02.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 2.8785 - output_1_loss: 0.9861 - output_2_loss: 0.9732 - output_3_loss: 0.9192 - val_loss: 4.0230 - val_output_1_loss: 1.3553 - val_output_2_loss: 1.3760 - val_output_3_loss: 1.2916\n",
            "Epoch 16/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.5834 - output_1_loss: 0.8875 - output_2_loss: 0.8741 - output_3_loss: 0.8218\n",
            "Epoch 00016: loss improved from 2.87851 to 2.58339, saving model to output_all_data_test_2/2048-16-2.58-4.19.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 2.5834 - output_1_loss: 0.8875 - output_2_loss: 0.8741 - output_3_loss: 0.8218 - val_loss: 4.1948 - val_output_1_loss: 1.4252 - val_output_2_loss: 1.4280 - val_output_3_loss: 1.3416\n",
            "Epoch 17/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.2641 - output_1_loss: 0.7797 - output_2_loss: 0.7652 - output_3_loss: 0.7191\n",
            "Epoch 00017: loss improved from 2.58339 to 2.26408, saving model to output_all_data_test_2/2048-17-2.26-4.32.hdf5\n",
            "66/66 [==============================] - 12s 180ms/step - loss: 2.2641 - output_1_loss: 0.7797 - output_2_loss: 0.7652 - output_3_loss: 0.7191 - val_loss: 4.3195 - val_output_1_loss: 1.4484 - val_output_2_loss: 1.4754 - val_output_3_loss: 1.3957\n",
            "Epoch 18/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.9393 - output_1_loss: 0.6746 - output_2_loss: 0.6516 - output_3_loss: 0.6131\n",
            "Epoch 00018: loss improved from 2.26408 to 1.93929, saving model to output_all_data_test_2/2048-18-1.94-4.48.hdf5\n",
            "66/66 [==============================] - 12s 180ms/step - loss: 1.9393 - output_1_loss: 0.6746 - output_2_loss: 0.6516 - output_3_loss: 0.6131 - val_loss: 4.4823 - val_output_1_loss: 1.5208 - val_output_2_loss: 1.5284 - val_output_3_loss: 1.4331\n",
            "Epoch 19/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.6200 - output_1_loss: 0.5691 - output_2_loss: 0.5407 - output_3_loss: 0.5102\n",
            "Epoch 00019: loss improved from 1.93929 to 1.61997, saving model to output_all_data_test_2/2048-19-1.62-4.71.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 1.6200 - output_1_loss: 0.5691 - output_2_loss: 0.5407 - output_3_loss: 0.5102 - val_loss: 4.7136 - val_output_1_loss: 1.5887 - val_output_2_loss: 1.6073 - val_output_3_loss: 1.5176\n",
            "Epoch 20/20\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.3385 - output_1_loss: 0.4781 - output_2_loss: 0.4428 - output_3_loss: 0.4175\n",
            "Epoch 00020: loss improved from 1.61997 to 1.33846, saving model to output_all_data_test_2/2048-20-1.34-4.87.hdf5\n",
            "66/66 [==============================] - 12s 179ms/step - loss: 1.3385 - output_1_loss: 0.4781 - output_2_loss: 0.4428 - output_3_loss: 0.4175 - val_loss: 4.8656 - val_output_1_loss: 1.6277 - val_output_2_loss: 1.6724 - val_output_3_loss: 1.5655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f19beca6898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwWLEX30U6rG",
        "colab_type": "text"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0RreYqb7ghr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  first_char = chr(int(np.random.randint(ord('a'), ord('z')+1)))\n",
        "  print(tokenizer.texts_to_sequences(first_char))\n",
        "  print(tokenizer.texts_to_sequences(first_char)[0])\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = tokenizer.texts_to_sequences(first_char)[0]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval, training=False)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(tokenizer.sequences_to_texts([\n",
        "                predicted_id\n",
        "            ])[0].strip()[1:].replace(\n",
        "                '   ', '\\n'\n",
        "            ).replace(' ', '').replace('\\n', ' '))\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGdqHkLg7-xk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "2c1882f4-26ec-4f44-ec83-ef43126f3d13"
      },
      "source": [
        "generative_model = BasicDanteRNN(latent_dim, n_tokens, generative=True)\n",
        "\n",
        "generative_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "generative_model.load_weights(output_dir / '2048-20-1.34-4.87.hdf5')\n",
        "\n",
        "\n",
        "print(generate_text(generative_model))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6aa289fb09d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgenerative_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgenerative_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'2048-20-1.34-4.87.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m       raise ValueError(\n\u001b[0;32m-> 2200\u001b[0;31m           \u001b[0;34m'Unable to load weights saved in HDF5 format into a subclassed '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2201\u001b[0m           \u001b[0;34m'Model which has not created its variables yet. Call the Model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m           'first, then load the weights.')\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
          ]
        }
      ]
    }
  ]
}